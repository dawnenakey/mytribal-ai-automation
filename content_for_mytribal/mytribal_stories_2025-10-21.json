[
  {
    "story_number": 1,
    "priority_score": 0.95,
    "ai_relevance_score": 1.0,
    "source_info": {
      "name": "ai_news",
      "url": "https://artificialintelligence-news.com/feed/",
      "category": "ai_specific"
    },
    "content": {
      "title": "How accounting firms are using AI agents to reclaim time and trust",
      "summary": "<p>For CFOs and CIOs under pressure to modernise finance operations, automation – as seen in several generations of RPA (robotic process automation) – isn’t enough. It&#8217;s apparent that transparency and explainability matter just as much. Accounting firms and finance functions inside organisations are now turning to AI systems that reason, not just compute. One of [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/finance-ai-reclaiming-time-trust-with-openai-chatgpt/\">How accounting firms are using AI agents to reclaim time and trust</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
      "original_link": "https://www.artificialintelligence-news.com/news/finance-ai-reclaiming-time-trust-with-openai-chatgpt/",
      "published_date": "Tue, 21 Oct 2025 11:41:15 +0000"
    },
    "mytribal_adaptation": {
      "suggested_title": "How accounting firms are using AI agents to reclaim time and trust",
      "story_angle": "Direct AI development and its impact on technology",
      "key_points": [
        "<p>For CFOs and CIOs under pressure to modernise finance operations, automation – as seen in several generations of RPA (robotic process automation) – isn’t enough",
        "It&#8217;s apparent that transparency and explainability matter just as much",
        "Accounting firms and finance functions inside organisations are now turning to AI systems that reason, not just compute"
      ],
      "seo_keywords": [
        "GPT",
        "OpenAI",
        "automation",
        "AI",
        "ChatGPT"
      ],
      "target_audience": "AI professionals and researchers"
    },
    "publishing_ready": true,
    "timestamp": "2025-10-21T16:28:20.442741"
  },
  {
    "story_number": 2,
    "priority_score": 0.8399999999999999,
    "ai_relevance_score": 0.8,
    "source_info": {
      "name": "reddit_ai",
      "url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
      "category": "community_discussion"
    },
    "content": {
      "title": "Boris Johnson admits writing books using ChatGPT. Former prime minister said ChatGPT was “frankly fantastic” and AI would help society “because we’re all simple.”",
      "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1ocnd6j/boris_johnson_admits_writing_books_using_chatgpt/\"> <img alt=\"Boris Johnson admits writing books using ChatGPT. Former prime minister said ChatGPT was “frankly fantastic” and AI would help society “because we’re all simple.”\" src=\"https://external-preview.redd.it/ZuFi1MhosUo8fsH4zqF8Ajuiqfmc-j6nbW41hurujCs.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f473a91bb6346ed943cf0ed892325a31d620ac17\" title=\"Boris Johnson admits writing books using ChatGPT. Former prime minister said ChatGPT was “frankly fantastic” and AI would help society “because we’re all simple.”\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/esporx\"> /u/esporx </a> <br /> <span><a href=\"https://www.politico.eu/article/uk-boris-johnson-admits-writing-books-using-chatgpt-ai/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1ocnd6j/boris_johnson_admits_writing_books_using_chatgpt/\">[comments]</a></span> </td></tr></table>",
      "original_link": "https://www.reddit.com/r/artificial/comments/1ocnd6j/boris_johnson_admits_writing_books_using_chatgpt/",
      "published_date": "2025-10-21T20:02:07+00:00"
    },
    "mytribal_adaptation": {
      "suggested_title": "Boris Johnson admits writing books using ChatGPT. Former prime minister said ChatGPT was “frankly fantastic” and AI would help society “because we’re all simple.”",
      "story_angle": "Community insights on AI development and trends",
      "key_points": [
        "<table> <tr><td> <a href=\"https://www",
        "com/r/artificial/comments/1ocnd6j/boris_johnson_admits_writing_books_using_chatgpt/\"> <img alt=\"Boris Johnson admits writing books using ChatGPT"
      ],
      "seo_keywords": [
        "GPT",
        "smart",
        "AI",
        "ChatGPT"
      ],
      "target_audience": "Tech enthusiasts and developers"
    },
    "publishing_ready": true,
    "timestamp": "2025-10-21T16:28:20.442868"
  },
  {
    "story_number": 3,
    "priority_score": 0.8399999999999999,
    "ai_relevance_score": 0.8,
    "source_info": {
      "name": "reddit_ai",
      "url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
      "category": "community_discussion"
    },
    "content": {
      "title": "Major AI updates in the last 24h",
      "summary": "<!-- SC_OFF --><div class=\"md\"><h3><strong>Products</strong></h3> <ul> <li><strong>Adobe launched AI Foundry</strong>, letting businesses fine-tune Firefly models on proprietary IP, addressing copyright risk.</li> <li><strong>OpenAI Agentic Commerce Protocol with Stripe</strong>, embedding shopping into ChatGPT for 800 M users and raising privacy and choice concerns.</li> </ul> <hr /> <h3><strong>Infrastructure</strong></h3> <ul> <li><strong>IBM and Groq announced a partnership</strong> delivering over 5x faster, cost-efficient inference for enterprise AI via Groq’s LPU integrated with Watson X Orchestrate.</li> <li>An <strong>AWS US-East-1 outage</strong> affected services including Fortnite, Alexa, Snapchat, highlighting risks of concentrated cloud reliance.</li> <li><strong>NVIDIA and Google Cloud</strong> made G4 VMs with RTX PRO 6000 Blackwell GPUs generally available.</li> </ul> <hr /> <h3><strong>Regulation</strong></h3> <ul> <li><strong>OpenAI subpoenaed several nonprofit critics</strong> to disclose funding and communications, raising concerns about legal pressure on AI oversight.</li> <li><strong>British Columbia unveiled new power regulations</strong> targeting AI workloads and data-centre energy use, aiming to manage grid strain.</li> </ul> <hr /> <h3><strong>Funding &amp; Business</strong></h3> <ul> <li><strong>OpenEvidence raised $200 M</strong>, valuing the company at $6 B, to expand its AI platform that supports ~15 M clinical consultations monthly, aiming to accelerate medical decision-making.</li> </ul> <hr /> <h3><strong>Models And Releases</strong></h3> <ul> <li><strong>DeepSeek released DeepSeek-OCR</strong> on HuggingFace, enabling high-accuracy optical character recognition for enterprise workflows.</li> </ul> <hr /> <p><strong>The Full Daily Brief</strong>: <a href=\"https://aifeed.fyi/briefing\">https://aifeed.fyi/briefing</a> </p> <hr /> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Majestic-Ad-6485\"> /u/Majestic-Ad-6485 </a> <br /> <span><a href=\"https://www.reddit.com/r/artificial/comments/1ocbb91/major_ai_updates_in_the_last_24h/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1ocbb91/major_ai_updates_in_the_last_24h/\">[comments]</a></span>",
      "original_link": "https://www.reddit.com/r/artificial/comments/1ocbb91/major_ai_updates_in_the_last_24h/",
      "published_date": "2025-10-21T12:15:25+00:00"
    },
    "mytribal_adaptation": {
      "suggested_title": "Major AI updates in the last 24h",
      "story_angle": "Community insights on AI development and trends",
      "key_points": [
        "<!-- SC_OFF --><div class=\"md\"><h3><strong>Products</strong></h3> <ul> <li><strong>Adobe launched AI Foundry</strong>, letting businesses fine-tune Firefly models on proprietary IP, addressing copyright risk",
        "</li> <li><strong>OpenAI Agentic Commerce Protocol with Stripe</strong>, embedding shopping into ChatGPT for 800 M users and raising privacy and choice concerns",
        "</li> </ul> <hr /> <h3><strong>Infrastructure</strong></h3> <ul> <li><strong>IBM and Groq announced a partnership</strong> delivering over 5x faster, cost-efficient inference for enterprise AI via Groq’s LPU integrated with Watson X Orchestrate"
      ],
      "seo_keywords": [
        "OpenAI",
        "GPT",
        "AI",
        "ChatGPT"
      ],
      "target_audience": "Tech enthusiasts and developers"
    },
    "publishing_ready": true,
    "timestamp": "2025-10-21T16:28:20.442988"
  },
  {
    "story_number": 4,
    "priority_score": 0.8,
    "ai_relevance_score": 1.0,
    "source_info": {
      "name": "venture_beat",
      "url": "https://venturebeat.com/feed/",
      "category": "tech_news"
    },
    "content": {
      "title": "Qwen's new Deep Research update lets you turn its reports into webpages, podcasts in seconds",
      "summary": "<p>Chinese e-commerce giant Alibaba’s <a href=\"https://venturebeat.com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks\">famously prolific Qwen Team</a> of AI model researchers and engineers has introduced a major expansion to its Qwen Deep Research tool, which is available as an optional modality the user can activate on the web-based Qwen Chat (a competitor to ChatGPT).</p><p>The update lets users generate not only comprehensive research reports with well-organized citations, but also interactive web pages and multi-speaker podcasts — all within 1-2 clicks.</p><p>This functionality is part of a <b>proprietary release</b>, distinct from many of Qwen’s previous open-source model offerings. </p><p>While the feature relies on the open-source models <b>Qwen3-Coder</b>, <b>Qwen-Image</b>, and <b>Qwen3-TTS</b> to power its core capabilities, the end-to-end experience — including research execution, web deployment, and audio generation — is <b>hosted and operated by Qwen</b>. </p><p>This means users benefit from a managed, integrated workflow without needing to configure infrastructure. That said, developers with access to the open-source models could theoretically replicate similar functionality on private or commercial systems.</p><p>The update was announced via the team’s official<a href=\"https://x.com/Alibaba_Qwen/status/1980609551486624237\"> X account (@Alibaba_Qwen)</a> today, October 21, 2025, stating:</p><blockquote><p>“Qwen Deep Research just got a major upgrade. It now creates not only the report, but also a live webpage and a podcast — powered by Qwen3-Coder, Qwen-Image, and Qwen3-TTS. Your insights, now visual and audible.”</p></blockquote><h3><b>Multi-Format Research Output</b></h3><p>The core workflow begins with a user request inside the Qwen Chat interface. From there, Qwen collaborates by asking clarifying questions to shape the research scope, pulls data from the web and official sources, and analyzes or resolves any inconsistencies it finds — even generating custom code when needed.</p><p>A <a href=\"https://x.com/Alibaba_Qwen/status/1980609551486624237\">demo video posted by Qwen on X</a> walks through this process on Qwen Chat using the U.S. SaaS market as an example. </p><p>In it, Qwen retrieves data from multiple industry sources, identifies discrepancies in market size estimates (e.g., $206 billion vs. $253 billion), and highlights ambiguities in the U.S. share of global figures. The assistant comments on differences in scope between sources and calculates a compound annual growth rate (CAGR) of 19.8% from 2020 to 2023, providing contextual analysis to back up the raw numbers.</p><p>Once the research is complete, users can click on the &quot;eyeball&quot; icon below the output result (see screenshot), which will bring up a PDF-style report in the right hand pane.</p><p>Then, when viewing the report in the right-hand pane, the user can click the &quot;Create&quot; button in the upper-right hand corner and select from the following two options:</p><ol><li><p><b>&quot;Web Dev&quot; </b>which produces a <b>live, professional-grade web page</b>, automatically deployed and <b>hosted by Qwen</b>, using Qwen3-Coder for structure and Qwen-Image for visuals.</p></li><li><p>&quot;<b>Podcast</b>,&quot; which, as it states, produces an audio <b>podcast</b>, featuring dynamic, multi-speaker narration generated by Qwen3-TTS, also <b>hosted by Qwen</b> for easy sharing and playback.</p></li></ol><p>This enables users to quickly convert a single research project into multiple forms of content — written, visual, and audible — with minimal extra input.</p><p>The website includes inline graphics generated by Qwen Image, making it suitable for use in public presentations, classrooms, or publishing. </p><p>The podcast feature allows users to select between 17 different speaker names as the host and 7 as the co-host, though I wasn&#x27;t able to find a way to preview the voice outputs before selecting them. It appears designed for deep listening on the go. </p><p>There was no way to change the language output that I could see, so mine came out in English, like my reports and initial prompts, though the Qwen LLMs are multi-modal. The voices were slightly more robotic than other AI tools I&#x27;ve used.</p><p>Here&#x27;s an example of a web page I generated <a href=\"https://chat.qwen.ai/s/deploy/65743dcf-7e0e-455b-b430-5004c8f36841\">on commonalities in authoritarian regimes throughout history</a>, <a href=\"https://chat.qwen.ai/s/deploy/caf2033e-725b-43dc-b4d0-721063728774\">another one on UFO or UAP sightings</a>, and below this paragraph, a podcast on UFO or UAP sightings. </p><p>While the website is hosted via a public link, the podcast must be downloaded by the user and can&#x27;t be linked to publicly, from what I could tell in my brief usage so far.</p><p>Note the podcast is much different than the actual report — not just a straight read-through audio version of it, rather, a new format of two hosts discussing and bantering about the subject using the report as the jumping off point. </p><p>The web page versions of the report also include new graphics not found in the PDF report.</p><h3><b>Comparisons to Google&#x27;s NotebookLM</b></h3><p>While the new capabilities have been well received by many early users, comparisons to other research assistants have surfaced — particularly Google’s <b>NotebookLM</b>, which recently exited beta.</p><p>AI commentator and newsletter writer <a href=\"https://x.com/kimmonismus/status/1980612332767072444\">Chubby (@kimmonismus) noted on X</a>:</p><blockquote><p>“I am really grateful that Qwen provides regular updates. That’s great.</p></blockquote><blockquote><p>But the attempt to build a NotebookLM clone inside Qwen-3-max doesn’t sound very promising compared to Google’s version.”</p></blockquote><p>While NotebookLM is built around organizing and querying existing documents and web pages, Qwen Deep Research focuses more on <b>generating new research content from scratch</b>, aggregating sources from the open web, and presenting it across multiple modalities. </p><p>The comparison suggests that while the two tools overlap in general concept — AI-assisted research — they diverge in approach and target user experience.</p><h3><b>Availability</b></h3><p>Qwen Deep Research is now live and available through the <b>Qwen Chat app</b>. The feature can be accessed with <a href=\"https://chat.qwen.ai/?inputFeature=deep_research\">the following URL.</a></p><p>No pricing details have been provided for Qwen3-Max or the specific Deep Research capabilities as of this writing.</p><h3><b>What&#x27;s Next For Qwen Deep Research?</b></h3><p>By combining research guidance, data analysis, and multi-format content creation into a single tool, Qwen Deep Research aims to streamline the path from idea to publishable output. </p><p>The integration of code, visuals, and voice makes it especially attractive to content creators, educators, and independent analysts who want to scale their research into web- or podcast-friendly forms without switching platforms.</p><p>Still, comparisons to more specialized offerings like NotebookLM raise questions about how Qwen’s generalized approach stacks up on depth, precision, and refinement. Whether the strength of its multi-format execution outweighs those concerns may come down to user priorities — and whether they value single-click publishing over tight integration with existing notes and materials.</p><p>For now, Qwen is signaling that research doesn’t end with a document — it begins with one.</p><p>Let me know if you want this repackaged into something shorter or tailored to a particular audience — newsletter, press-style blog, internal team explainer, etc.</p>",
      "original_link": "https://venturebeat.com/ai/qwens-new-deep-research-update-lets-you-turn-its-reports-into-webpages",
      "published_date": "Tue, 21 Oct 2025 18:32:00 GMT"
    },
    "mytribal_adaptation": {
      "suggested_title": "AI Update: Qwen's new Deep Research update lets you turn its reports into webpages, podcasts in seconds",
      "story_angle": "AI technology making waves in the industry",
      "key_points": [
        "<p>Chinese e-commerce giant Alibaba’s <a href=\"https://venturebeat",
        "com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks\">famously prolific Qwen Team</a> of AI model researchers and engineers has introduced a major expansion to its Qwen Deep Research tool, which is available as an optional modality the user can activate on the web-based Qwen Chat (a competitor to ChatGPT)",
        "</p><p>The update lets users generate not only comprehensive research reports with well-organized citations, but also interactive web pages and multi-speaker podcasts — all within 1-2 clicks"
      ],
      "seo_keywords": [
        "GPT",
        "2025",
        "ML",
        "OpenAI",
        "AI",
        "ChatGPT"
      ],
      "target_audience": "AI professionals and researchers"
    },
    "publishing_ready": true,
    "timestamp": "2025-10-21T16:28:20.443216"
  },
  {
    "story_number": 5,
    "priority_score": 0.8,
    "ai_relevance_score": 1.0,
    "source_info": {
      "name": "venture_beat",
      "url": "https://venturebeat.com/feed/",
      "category": "tech_news"
    },
    "content": {
      "title": "DeepSeek drops open-source model that compresses text 10x through images, defying conventions",
      "summary": "<p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a>, the Chinese artificial intelligence research company that has repeatedly challenged assumptions about <a href=\"https://www.reuters.com/technology/artificial-intelligence/big-tech-faces-heat-chinas-deepseek-sows-doubts-billion-dollar-spending-2025-01-27/\"><u>AI development costs</u></a>, has released a <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>new model</u></a> that fundamentally reimagines how large language models process information—and the implications extend far beyond its modest branding as an optical character recognition tool.</p><p>The company&#x27;s <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR model</u></a>, released Monday with full <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>open-source code</u></a> and <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>weights</u></a>, achieves what researchers describe as a paradigm inversion: compressing text through visual representation up to 10 times more efficiently than traditional text tokens. The finding challenges a core assumption in AI development and could pave the way for language models with dramatically expanded context windows, potentially reaching tens of millions of tokens.</p><p>&quot;We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping,&quot; the research team wrote in their <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>technical paper</u></a>. &quot;Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio &lt; 10×), the model can achieve decoding (OCR) precision of 97%.&quot;</p><p>The implications have resonated across the AI research community. <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Andrej Karpathy</u></a>, co-founder of OpenAI and former director of AI at Tesla, said in a post that the work raises fundamental questions about how AI systems should process information. &quot;Maybe it makes more sense that all inputs to LLMs should only ever be images,&quot; Karpathy wrote. &quot;Even if you happen to have pure text input, maybe you&#x27;d prefer to render it and then feed that in.&quot;</p><h2><b>How DeepSeek achieved 10x compression by treating text as images</b></h2><p>While <a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> marketed the release as an <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>OCR model</u></a> — a technology for converting images of text into digital characters — the research paper reveals more ambitious goals. The model demonstrates that visual representations can serve as a superior compression medium for textual information, inverting the conventional hierarchy where text tokens were considered more efficient than vision tokens.</p><p>&quot;Traditionally, vision LLM tokens almost seemed like an afterthought or &#x27;bolt on&#x27; to the LLM paradigm,&quot; wrote <a href=\"https://x.com/doodlestein/status/1980282222893535376\"><u>Jeffrey Emanuel</u></a>, an AI researcher, in a detailed analysis of the paper. &quot;And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens...But that gets inverted now from the ideas in this paper.&quot;</p><p>The model&#x27;s architecture consists of two primary components: <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepEncoder</u></a>, a novel 380-million-parameter vision encoder, and a 3-billion-parameter mixture-of-experts language decoder with 570 million activated parameters. DeepEncoder combines Meta&#x27;s <a href=\"https://segment-anything.com/\"><u>Segment Anything Model (SAM)</u></a> for local visual perception with <a href=\"https://openai.com/index/clip/\"><u>OpenAI&#x27;s CLIP model</u></a> for global visual understanding, connected through a 16x compression module.</p><p>To validate their compression claims, DeepSeek researchers tested the model on the <a href=\"https://github.com/ucaslcl/Fox\"><u>Fox benchmark</u></a>, a dataset of diverse document layouts. The results were striking: using just 100 vision tokens, the model achieved 97.3% accuracy on documents containing 700-800 text tokens — representing an effective compression ratio of 7.5x. Even at compression ratios approaching 20x, accuracy remained around 60%.</p><h2><b>The practical impact: Processing 200,000 pages per day on a single GPU</b></h2><p>The efficiency gains translate directly to production capabilities. According to the company, a single <a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><u>Nvidia A100-40G GPU</u></a> can process more than 200,000 pages per day using <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR</u></a>. Scaling to a cluster of 20 servers with eight GPUs each, throughput reaches 33 million pages daily — sufficient to rapidly construct training datasets for other AI models.</p><p>On <a href=\"https://github.com/opendatalab/OmniDocBench\"><u>OmniDocBench</u></a>, a comprehensive document parsing benchmark, <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR</u></a> outperformed GOT-OCR2.0 (which uses 256 tokens per page) while using only 100 vision tokens. More dramatically, it surpassed MinerU2.0 — which requires more than 6,000 tokens per page on average — while using fewer than 800 vision tokens.</p><p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> designed the model to support five distinct resolution modes, each optimized for different compression ratios and use cases. The &quot;Tiny&quot; mode operates at 512×512 resolution with just 64 vision tokens, while &quot;Gundam&quot; mode combines multiple resolutions dynamically for complex documents. &quot;Gundam mode consists of n×640×640 tiles (local views) and a 1024×1024 global view,&quot; the researchers wrote.</p><h2><b>Why this breakthrough could unlock 10 million token context windows</b></h2><p>The compression breakthrough has immediate implications for one of the most pressing challenges in AI development: expanding the context windows that determine how much information language models can actively consider. Current state-of-the-art models typically handle context windows measured in hundreds of thousands of tokens. DeepSeek&#x27;s approach suggests a path to windows ten times larger.</p><p>&quot;The potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting,&quot; <a href=\"https://x.com/doodlestein/status/1980282222893535376\"><u>Emanuel wrote</u></a>. &quot;You could basically cram all of a company&#x27;s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective.&quot;</p><p>The researchers explicitly frame their work in terms of context compression for language models. &quot;Through DeepSeek-OCR, we demonstrate that vision-text compression can achieve significant token reduction (7-20×) for different historical context stages, offering a promising direction for addressing long-context challenges in large language models,&quot; <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>they wrote</u></a>.</p><p><a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>The paper</u></a> includes a speculative but intriguing diagram illustrating how the approach could implement memory decay mechanisms similar to human cognition. Older conversation rounds could be progressively downsampled to lower resolutions, consuming fewer tokens while maintaining key information — a form of computational forgetting that mirrors biological memory.</p><h2><b>How visual processing could eliminate the &#x27;ugly&#x27; tokenizer problem</b></h2><p>Beyond compression, Karpathy highlighted how the approach challenges fundamental assumptions about how language models should process text. Traditional tokenizers—the systems that break text into units for processing—have long been criticized for their complexity and limitations.</p><p>&quot;I already ranted about how much I dislike the tokenizer,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Karpathy wrote</u></a>. &quot;Tokenizers are ugly, separate, not end-to-end stage. It &#x27;imports&#x27; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network.&quot;</p><p>Visual processing of text could eliminate these issues while enabling new capabilities. The approach naturally handles formatting information lost in pure text representations: bold text, colors, layout, embedded images. &quot;Input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful,&quot; Karpathy noted.</p><p>The implications resonate with human cognitive science. <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel drew a parallel to Hans Bethe</u></a>, the renowned physicist who memorized vast amounts of reference data: &quot;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&quot;</p><h2><b>The model&#x27;s training: 30 million PDF pages across 100 languages</b></h2><p>The model&#x27;s capabilities rest on an extensive training regimen using diverse data sources. DeepSeek collected 30 million PDF pages covering approximately 100 languages, with Chinese and English accounting for 25 million pages. The training data spans nine document types — academic papers, financial reports, textbooks, newspapers, handwritten notes, and others.</p><p>Beyond document OCR, the training incorporated what the researchers call &quot;OCR 2.0&quot; data: 10 million synthetic charts, 5 million chemical formulas, and 1 million geometric figures. The model also received 20% general vision data for tasks like image captioning and object detection, plus 10% text-only data to maintain language capabilities.</p><p>The training process employed pipeline parallelism across 160 <a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><u>Nvidia A100-40G GPUs</u></a> (20 nodes with 8 GPUs each), with the vision encoder divided between two pipeline stages and the language model split across two others. &quot;For multimodal data, the training speed is 70B tokens/day,&quot; the researchers reported.</p><h2><b>Open source release accelerates research and raises competitive questions</b></h2><p>True to DeepSeek&#x27;s pattern of open development, the company released the complete model weights, training code, and inference scripts on <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>GitHub</u></a> and <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>Hugging Face</u></a>. The GitHub repository gained over 4,000 stars within 24 hours of release, according to Dataconomy.</p><p>The breakthrough raises questions about whether other AI labs have developed similar techniques but kept them proprietary. Emanuel speculated that Google&#x27;s Gemini models, which feature large context windows and strong OCR performance, might employ comparable approaches. &quot;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel wrote</u></a>.</p><p>Google&#x27;s <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\"><u>Gemini 2.5 Pro</u></a> offers a 1-million-token context window, with plans to expand to 2 million, though the company has not publicly detailed the technical approaches enabling this capability. OpenAI&#x27;s <a href=\"https://openai.com/index/introducing-gpt-5/\"><u>GPT-5</u></a> supports 400,000 tokens, while Anthropic&#x27;s <a href=\"https://www.anthropic.com/news/claude-sonnet-4-5\"><u>Claude 4.5</u></a> offers 200,000 tokens, with a 1-million-token window available in beta for eligible organizations.</p><h2><b>The unanswered question: Can AI reason over compressed visual tokens?</b></h2><p>While the compression results are impressive, researchers acknowledge important open questions. &quot;It&#x27;s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel noted</u></a>. &quot;Can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality?&quot;</p><p>The <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepSeek paper</u></a> focuses primarily on the compression-decompression capability, measured through OCR accuracy, rather than downstream reasoning performance. This leaves open whether language models could reason effectively over large contexts represented primarily as compressed visual tokens.</p><p>The researchers acknowledge their work represents &quot;an initial exploration into the boundaries of vision-text compression.&quot; They note that &quot;OCR alone is insufficient to fully validate true context optical compression&quot; and plan future work including &quot;digital-optical text interleaved pretraining, needle-in-a-haystack testing, and other evaluations.&quot;</p><p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> has established a pattern of achieving competitive results with dramatically lower computational resources than Western AI labs. The company&#x27;s earlier <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3\"><u>DeepSeek-V3 model</u></a> reportedly cost <a href=\"https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/\"><u>just $5.6 million to train</u></a>—though this figure represents only the final training run and excludes R&amp;D and infrastructure costs—compared to hundreds of millions for comparable models from OpenAI and Anthropic.</p><p>Industry analysts have questioned the $5.6 million figure, with some estimates placing the company&#x27;s total infrastructure and operational costs <a href=\"https://www.cnbc.com/2025/01/30/chinas-deepseek-has-some-big-ai-claims-not-all-experts-are-convinced-.html\"><u>closer to $1.3 billion</u></a>, though still lower than American competitors&#x27; spending.</p><h2><b>The bigger picture: Should language models process text as images?</b></h2><p><a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepSeek-OCR</u></a> poses a fundamental question for AI development: should language models process text as text, or as images of text? The research demonstrates that, at least for compression purposes, visual representation offers significant advantages. Whether this translates to effective reasoning over vast contexts remains to be determined.</p><p>&quot;From another perspective, optical contexts compression still offers substantial room for research and improvement, representing a promising new direction,&quot; the researchers concluded<a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u> in their paper</u></a>.</p><p>For the AI industry, the work adds another dimension to the race for longer context windows — a competition that has intensified as language models are applied to increasingly complex tasks requiring vast amounts of information. The open-source release ensures the technique will be widely explored, tested, and potentially integrated into future AI systems.</p><p>As Karpathy framed the deeper implication: &quot;OCR is just one of many useful vision -&gt; text tasks. And text -&gt; text tasks can be made to be vision -&gt;text tasks. Not vice versa.&quot; In other words, the path forward for AI might not run through better tokenizers — it might bypass text tokens altogether.</p><p>\n</p>",
      "original_link": "https://venturebeat.com/ai/deepseek-drops-open-source-model-that-compresses-text-10x-through-images",
      "published_date": "Tue, 21 Oct 2025 18:30:00 GMT"
    },
    "mytribal_adaptation": {
      "suggested_title": "AI Update: DeepSeek drops open-source model that compresses text 10x through images, defying conventions",
      "story_angle": "AI technology making waves in the industry",
      "key_points": [
        "<p><a href=\"https://www",
        "com/\"><u>DeepSeek</u></a>, the Chinese artificial intelligence research company that has repeatedly challenged assumptions about <a href=\"https://www"
      ],
      "seo_keywords": [
        "GPT",
        "intelligent",
        "2025",
        "ML",
        "breakthrough",
        "OpenAI",
        "artificial intelligence",
        "AI"
      ],
      "target_audience": "AI professionals and researchers"
    },
    "publishing_ready": true,
    "timestamp": "2025-10-21T16:28:20.443631"
  }
]