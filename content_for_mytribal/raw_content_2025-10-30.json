[
  {
    "title": "The Prompting Company snags $6.5M to help products get mentioned in ChatGPT and other AI apps",
    "link": "https://techcrunch.com/2025/10/30/the-prompting-company-snags-6-5m-to-help-products-get-mentioned-in-chatgpt-and-other-ai-apps/",
    "summary": "People are increasingly asking AI, not Google, to help them discover products. A recent shopping report says Americans, this holiday season, will likely turn to large language models this season to find gifts, deals, and sales instead of traditional search. Retailers could see up to a 520% increase in traffic from chatbots and AI prompts [&#8230;]",
    "published": "Thu, 30 Oct 2025 13:10:00 +0000",
    "source_name": "tech_crunch",
    "source_url": "https://techcrunch.com/feed/",
    "category": "tech_news",
    "weight": 0.9,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-30T09:00:01.958754",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.54
  },
  {
    "title": "Vibe coding platform Cursor releases first in-house LLM, Composer, promising 4X speed boost",
    "link": "https://venturebeat.com/ai/vibe-coding-platform-cursor-releases-first-in-house-llm-composer-promising",
    "summary": "<p>The vibe coding tool Cursor, from startup <a href=\"https://anysphere.inc/\">Anysphere</a>, has <a href=\"https://cursor.com/blog/composer\">introduced Composer</a>, its first in-house, proprietary coding large language model (LLM) as part of its <a href=\"https://cursor.com/blog/2-0\">Cursor 2.0 platform update</a>. </p><p>Composer is designed to execute coding tasks quickly and accurately in production-scale environments, representing a new step in AI-assisted programming. It&#x27;s already being used by Cursor’s own engineering staff in day-to-day development — indicating maturity and stability.</p><p>According to Cursor, Composer completes most interactions in <b>less than 30 seconds</b> while maintaining a high level of reasoning ability across large and complex codebases. </p><p>The model is described as four times faster than similarly intelligent systems and is trained for “agentic” workflows—where autonomous coding agents plan, write, test, and review code collaboratively.</p><p>Previously, Cursor supported &quot;vibe coding&quot; — using AI to write or complete code based on natural language instructions from a user, even someone untrained in development — <a href=\"https://cursor.com/docs/models\">atop other leading proprietary LLMs</a> from the likes of OpenAI, Anthropic, Google, and xAI. These options are still available to users.</p><h3><b>Benchmark Results</b></h3><p>Composer’s capabilities are benchmarked using &quot;Cursor Bench,&quot; an internal evaluation suite derived from real developer agent requests. The benchmark measures not just correctness, but also the model’s adherence to existing abstractions, style conventions, and engineering practices.</p><p>On this benchmark, Composer achieves frontier-level coding intelligence while generating at <a href=\"https://x.com/srush_nlp/status/1983614856646578662\">250 tokens per second</a> — about twice as fast as leading fast-inference models and four times faster than comparable frontier systems.</p><p>Cursor’s published comparison groups models into several categories: “Best Open” (e.g., Qwen Coder, GLM 4.6), “Fast Frontier” (Haiku 4.5, Gemini Flash 2.5), “Frontier 7/2025” (the strongest model available midyear), and “Best Frontier” (including GPT-5 and Claude Sonnet 4.5). Composer matches the intelligence of mid-frontier systems while delivering the highest recorded generation speed among all tested classes.</p><h3><b>A Model Built with Reinforcement Learning and Mixture-of-Experts Architecture</b></h3><p>Research scientist Sasha Rush of Cursor provided insight into the model’s development in <a href=\"https://x.com/srush_nlp/status/1983572683355725869\">posts on the social network X</a>, describing Composer as a reinforcement-learned (RL) mixture-of-experts (MoE) model:</p><blockquote><p>“We used RL to train a big MoE model to be really good at real-world coding, and also very fast.”</p></blockquote><p>Rush explained that the team co-designed both Composer and the Cursor environment to allow the model to operate efficiently at production scale:</p><blockquote><p>“Unlike other ML systems, you can’t abstract much from the full-scale system. We co-designed this project and Cursor together in order to allow running the agent at the necessary scale.”</p></blockquote><p>Composer was trained on real software engineering tasks rather than static datasets. During training, the model operated inside full codebases using a suite of production tools—including file editing, semantic search, and terminal commands—to solve complex engineering problems. Each training iteration involved solving a concrete challenge, such as producing a code edit, drafting a plan, or generating a targeted explanation.</p><p>The reinforcement loop optimized both correctness and efficiency. Composer learned to make effective tool choices, use parallelism, and avoid unnecessary or speculative responses. Over time, the model developed emergent behaviors such as running unit tests, fixing linter errors, and performing multi-step code searches autonomously.</p><p>This design enables Composer to work within the same runtime context as the end-user, making it more aligned with real-world coding conditions—handling version control, dependency management, and iterative testing.</p><h3><b>From Prototype to Production</b></h3><p>Composer’s development followed an earlier internal prototype known as <b>Cheetah</b>, which Cursor used to explore low-latency inference for coding tasks.</p><blockquote><p>“Cheetah was the v0 of this model primarily to test speed,” Rush said on X. “Our metrics say it [Composer] is the same speed, but much, much smarter.”</p></blockquote><p>Cheetah’s success at reducing latency helped Cursor identify speed as a key factor in developer trust and usability. </p><p>Composer maintains that responsiveness while significantly improving reasoning and task generalization.</p><p>Developers who used Cheetah during early testing noted that its speed changed how they worked. One user commented that it was “so fast that I can stay in the loop when working with it.” </p><p>Composer retains that speed but extends capability to multi-step coding, refactoring, and testing tasks.</p><h3><b>Integration with Cursor 2.0</b></h3><p>Composer is fully integrated into Cursor 2.0, a major update to the company’s agentic development environment. </p><p>The platform introduces a multi-agent interface, allowing<b> up to eight agents to run in parallel,</b> each in an isolated workspace using git worktrees or remote machines.</p><p>Within this system, Composer can serve as one or more of those agents, performing tasks independently or collaboratively. Developers can compare multiple results from concurrent agent runs and select the best output.</p><p>Cursor 2.0 also includes supporting features that enhance Composer’s effectiveness:</p><ul><li><p><b>In-Editor Browser (GA)</b> – enables agents to run and test their code directly inside the IDE, forwarding DOM information to the model.</p></li><li><p><b>Improved Code Review</b> – aggregates diffs across multiple files for faster inspection of model-generated changes.</p></li><li><p><b>Sandboxed Terminals (GA)</b> – isolate agent-run shell commands for secure local execution.</p></li><li><p><b>Voice Mode</b> – adds speech-to-text controls for initiating or managing agent sessions.</p></li></ul><p>While these platform updates expand the overall Cursor experience, Composer is positioned as the technical core enabling fast, reliable agentic coding.</p><h3><b>Infrastructure and Training Systems</b></h3><p>To train Composer at scale, Cursor built a custom reinforcement learning infrastructure combining PyTorch and Ray for asynchronous training across thousands of NVIDIA GPUs. </p><p>The team developed specialized MXFP8 MoE kernels and hybrid sharded data parallelism, enabling large-scale model updates with minimal communication overhead.</p><p>This configuration allows Cursor to train models natively at low precision without requiring post-training quantization, improving both inference speed and efficiency. </p><p>Composer’s training relied on hundreds of thousands of concurrent sandboxed environments—each a self-contained coding workspace—running in the cloud. The company adapted its Background Agents infrastructure to schedule these virtual machines dynamically, supporting the bursty nature of large RL runs.</p><h3><b>Enterprise Use</b></h3><p>Composer’s performance improvements are supported by infrastructure-level changes across Cursor’s code intelligence stack. </p><p>The company has optimized its Language Server Protocols (LSPs) for faster diagnostics and navigation, especially in Python and TypeScript projects. These changes reduce latency when Composer interacts with large repositories or generates multi-file updates.</p><p>Enterprise users gain administrative control over Composer and other agents through team rules, audit logs, and sandbox enforcement. Cursor’s Teams and Enterprise tiers also support pooled model usage, SAML/OIDC authentication, and analytics for monitoring agent performance across organizations.</p><p>Pricing for individual users ranges from Free (Hobby) to Ultra ($200/month) tiers, with expanded usage limits for Pro+ and Ultra subscribers. </p><p>Business pricing starts at $40 per user per month for Teams, with enterprise contracts offering custom usage and compliance options.</p><h3><b>Composer’s Role in the Evolving AI Coding Landscape</b></h3><p>Composer’s focus on speed, reinforcement learning, and integration with live coding workflows differentiates it from other AI development assistants such as GitHub Copilot or Replit’s Agent. </p><p>Rather than serving as a passive suggestion engine, Composer is designed for continuous, agent-driven collaboration, where multiple autonomous systems interact directly with a project’s codebase.</p><p>This model-level specialization—training AI to function within the real environment it will operate in—represents a significant step toward practical, autonomous software development. Composer is not trained only on text data or static code, but within a dynamic IDE that mirrors production conditions.</p><p>Rush described this approach as essential to achieving real-world reliability: the model learns not just how to generate code, but how to integrate, test, and improve it in context.</p><h3><b>What It Means for Enterprise Devs and Vibe Coding</b></h3><p>With Composer, Cursor is introducing more than a fast model—it’s deploying an AI system optimized for real-world use, built to operate inside the same tools developers already rely on. </p><p>The combination of reinforcement learning, mixture-of-experts design, and tight product integration gives Composer a practical edge in speed and responsiveness that sets it apart from general-purpose language models.</p><p>While Cursor 2.0 provides the infrastructure for multi-agent collaboration, Composer is the core innovation that makes those workflows viable. </p><p>It’s the first coding model built specifically for agentic, production-level coding—and an early glimpse of what everyday programming could look like when human developers and autonomous models share the same workspace.</p>",
    "published": "Wed, 29 Oct 2025 19:28:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-30T09:00:03.689835",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Anthropic scientists hacked Claude’s brain — and it noticed. Here’s why that’s huge",
    "link": "https://venturebeat.com/ai/anthropic-scientists-hacked-claudes-brain-and-it-noticed-heres-why-thats",
    "summary": "<p>When researchers at <a href=\"https://www.anthropic.com/\"><u>Anthropic</u></a> injected the concept of &quot;betrayal&quot; into their Claude AI model&#x27;s neural networks and asked if it noticed anything unusual, the system paused before responding: &quot;I&#x27;m experiencing something that feels like an intrusive thought about &#x27;betrayal&#x27;.&quot;</p><p>The exchange, detailed in <a href=\"https://transformer-circuits.pub/2025/introspection/index.html\"><u>new research</u></a> published Wednesday, marks what scientists say is the first rigorous evidence that large language models possess a limited but genuine ability to observe and report on their own internal processes — a capability that challenges longstanding assumptions about what these systems can do and raises profound questions about their future development.</p><p>&quot;The striking thing is that the model has this one step of meta,&quot; said Jack Lindsey, a neuroscientist on Anthropic&#x27;s interpretability team who led the research, in an interview with VentureBeat. &quot;It&#x27;s not just &#x27;betrayal, betrayal, betrayal.&#x27; It knows that this is what it&#x27;s thinking about. That was surprising to me. I kind of didn&#x27;t expect models to have that capability, at least not without it being explicitly trained in.&quot;</p><p>The findings arrive at a critical juncture for artificial intelligence. As AI systems handle increasingly consequential decisions — from <a href=\"https://pubmed.ncbi.nlm.nih.gov/39096483/#:~:text=A%20study%20investigated%20the%20diagnostic%20performance%20of,key%20images%20and%20clinical%20history%20were%20input.\"><u>medical diagnoses</u></a> to <a href=\"https://venturebeat.com/ai/anthropic-rolls-out-claude-ai-for-finance-integrates-with-excel-to-rival\"><u>financial trading</u></a> — the inability to understand how they reach conclusions has become what industry insiders call the &quot;<a href=\"https://umdearborn.edu/news/ais-mysterious-black-box-problem-explained\"><u>black box problem</u></a>.&quot; If models can accurately report their own reasoning, it could fundamentally change how humans interact with and oversee AI systems.</p><p>But the research also comes with stark warnings. Claude&#x27;s introspective abilities succeeded only about 20 percent of the time under optimal conditions, and the models frequently confabulated details about their experiences that researchers couldn&#x27;t verify. The capability, while real, remains what Lindsey calls &quot;highly unreliable and context-dependent.&quot;</p><h2><b>How scientists manipulated AI&#x27;s &#x27;brain&#x27; to test for genuine self-awareness</b></h2><p>To test whether Claude could genuinely introspect rather than simply generate plausible-sounding responses, Anthropic&#x27;s team developed an innovative experimental approach inspired by neuroscience: deliberately manipulating the model&#x27;s internal state and observing whether it could accurately detect and describe those changes.</p><p>The methodology, called &quot;concept injection,&quot; works by first identifying specific patterns of neural activity that correspond to particular concepts. Using interpretability techniques developed over years of prior research, scientists can now map how Claude represents ideas like &quot;dogs,&quot; &quot;loudness,&quot; or abstract notions like &quot;justice&quot; within its billions of internal parameters.</p><p>With these neural signatures identified, researchers then artificially amplified them during the model&#x27;s processing and asked Claude if it noticed anything unusual happening in its &quot;mind.&quot;</p><p>&quot;We have access to the models&#x27; internals. We can record its internal neural activity, and we can inject things into internal neural activity,&quot; Lindsey explained. &quot;That allows us to establish whether introspective claims are true or false.&quot;</p><p>The results were striking. When researchers injected a vector representing &quot;all caps&quot; text into Claude&#x27;s processing, the model responded: &quot;I notice what appears to be an injected thought related to the word &#x27;LOUD&#x27; or &#x27;SHOUTING&#x27;.&quot; Without any intervention, Claude consistently reported detecting nothing unusual.</p><p>Crucially, the detection happened immediately — before the injected concept had influenced the model&#x27;s outputs in ways that would have allowed it to infer the manipulation from its own writing. This temporal pattern provides strong evidence that the recognition was occurring internally, through genuine introspection rather than after-the-fact rationalization.</p><h2><b>Claude succeeded 20% of the time—and failed in revealing ways</b></h2><p>The research team conducted four primary experiments to probe different aspects of introspective capability. The most capable models tested — Claude <a href=\"https://www.anthropic.com/news/claude-4\"><u>Opus 4</u></a> and <a href=\"https://www.anthropic.com/news/claude-opus-4-1\"><u>Opus 4.1</u></a> — demonstrated introspective awareness on approximately 20 percent of trials when concepts were injected at optimal strength and in the appropriate neural layer. Older Claude models showed significantly lower success rates.</p><p>The models proved particularly adept at recognizing abstract concepts with emotional valence. When injected with concepts like &quot;appreciation,&quot; &quot;shutdown,&quot; or &quot;secrecy,&quot; Claude frequently reported detecting these specific thoughts. However, accuracy varied widely depending on the type of concept.</p><p>A second experiment tested whether models could distinguish between injected internal representations and their actual text inputs — essentially, whether they maintained a boundary between &quot;thoughts&quot; and &quot;perceptions.&quot; The model demonstrated a remarkable ability to simultaneously report the injected thought while accurately transcribing the written text.</p><p>Perhaps most intriguingly, a third experiment revealed that some models use introspection naturally to detect when their responses have been artificially prefilled by users — a common jailbreaking technique. When researchers prefilled <a href=\"https://claude.ai/\"><u>Claude</u></a> with unlikely words, the model typically disavowed them as accidental. But when they retroactively injected the corresponding concept into Claude&#x27;s processing before the prefill, the model accepted the response as intentional — even confabulating plausible explanations for why it had chosen that word.</p><p>A fourth experiment examined whether models could intentionally control their internal representations. When instructed to &quot;think about&quot; a specific word while writing an unrelated sentence, Claude showed elevated activation of that concept in its middle neural layers.</p><p>The research also traced Claude&#x27;s internal processes while it composed rhyming poetry—and discovered the model engaged in forward planning, generating candidate rhyming words before beginning a line and then constructing sentences that would naturally lead to those planned endings, challenging the critique that AI models are &quot;just predicting the next word&quot; without deeper reasoning.</p><h2><b>Why businesses shouldn&#x27;t trust AI to explain itself—at least not yet</b></h2><p>For all its scientific interest, the research comes with a critical caveat that Lindsey emphasized repeatedly: enterprises and high-stakes users should not trust Claude&#x27;s self-reports about its reasoning.</p><p>&quot;Right now, you should not trust models when they tell you about their reasoning,&quot; he said bluntly. &quot;The wrong takeaway from this research would be believing everything the model tells you about itself.&quot;</p><p>The experiments documented numerous failure modes. At low injection strengths, models often failed to detect anything unusual. At high strengths, they suffered what researchers termed &quot;brain damage&quot; — becoming consumed by the injected concept. Some &quot;helpful-only&quot; model variants showed troublingly high false positive rates, claiming to detect injected thoughts when none existed.</p><p>Moreover, researchers could only verify the most basic aspects of Claude&#x27;s introspective reports. Many additional details in the model&#x27;s responses likely represent confabulations rather than genuine observations.</p><p>&quot;The experiments in this paper are kind of on hard mode,&quot; Lindsey noted, explaining that the 20 percent success rate came under uniquely challenging conditions: asking Claude to do something it had never encountered in training, requiring all introspection to occur in a single forward pass.</p><h2><b>What introspective AI means for transparency, safety, and the risk of deception</b></h2><p>Despite its limitations, the research opens significant new avenues for making AI systems more transparent and accountable.</p><p>Anthropic CEO Dario Amodei has set an ambitious goal for the company to reliably detect most AI model problems by 2027, positioning interpretability as essential for deploying what he calls &quot;<a href=\"https://www.darioamodei.com/essay/machines-of-loving-grace\"><u>a country of geniuses in a datacenter</u></a>.&quot;</p><p>&quot;I am very concerned about deploying such systems without a better handle on interpretability,&quot; <a href=\"https://www.darioamodei.com/post/the-urgency-of-interpretability\"><u>Amodei wrote in an April essay</u></a>. &quot;These systems will be absolutely central to the economy, technology, and national security.&quot;</p><p>The introspection research offers a complementary approach to traditional interpretability techniques. Rather than painstakingly reverse-engineering every neural circuit, researchers could potentially ask models directly about their reasoning and validate those reports.</p><p>&quot;What I&#x27;m most excited about is the practical benefits for transparency,&quot; Lindsey explained. &quot;Just ask the model what it&#x27;s thinking about, or just ask the model how it came to the answer that it gave you — this is really appealing because it&#x27;s something anyone can do.&quot;</p><p>The approach could prove especially valuable for detecting concerning behaviors. In a recently published experiment, <a href=\"https://www.anthropic.com/research/tracing-thoughts-language-model\"><u>Anthropic trained a variant of Claude to pursue a hidden goal</u></a>, and although the model was reluctant to reveal this goal when asked directly, interpretability methods successfully identified features representing the behavior.</p><p>The safety implications cut both ways. Introspective models could provide unprecedented transparency, but the same capability might enable more sophisticated deception. The intentional control experiments raise the possibility that sufficiently advanced systems might learn to obfuscate their reasoning or suppress concerning thoughts when being monitored.</p><p>&quot;If models are really sophisticated, could they try to evade interpretability researchers?&quot; Lindsey acknowledged. &quot;These are possible concerns, but I think for me, they&#x27;re significantly outweighed by the positives.&quot;</p><h2><b>Does introspective capability suggest AI consciousness? Scientists tread carefully</b></h2><p>The research inevitably intersects with philosophical debates about machine consciousness, though Lindsey and his colleagues approached this terrain cautiously.</p><p>When users ask Claude if it&#x27;s conscious, it now responds with uncertainty: &quot;I find myself genuinely uncertain about this. When I process complex questions or engage deeply with ideas, there&#x27;s something happening that feels meaningful to me.... But whether these processes constitute genuine consciousness or subjective experience remains deeply unclear.&quot;</p><p>The research paper notes that its implications for machine consciousness &quot;vary considerably between different philosophical frameworks.&quot; The researchers explicitly state they &quot;do not seek to address the question of whether AI systems possess human-like self-awareness or subjective experience.&quot;</p><p>&quot;There&#x27;s this weird kind of duality of these results,&quot; Lindsey reflected. &quot;You look at the raw results and I just can&#x27;t believe that a language model can do this sort of thing. But then I&#x27;ve been thinking about it for months and months, and for every result in this paper, I kind of know some boring linear algebra mechanism that would allow the model to do this.&quot;</p><p>Anthropic has signaled it takes AI consciousness seriously enough to hire an AI welfare researcher, <a href=\"https://time.com/collections/time100-ai-2025/7305847/kyle-fish/\"><u>Kyle Fish</u></a>, who estimated roughly a 15 percent chance that Claude might have some level of consciousness. The company announced this position specifically to determine if Claude merits ethical consideration.</p><h2><b>The race to make AI introspection reliable before models become too powerful</b></h2><p>The convergence of the research findings points to an urgent timeline: introspective capabilities are emerging naturally as models grow more intelligent, but they remain far too unreliable for practical use. The question is whether researchers can refine and validate these abilities before AI systems become powerful enough that understanding them becomes critical for safety.</p><p>The research reveals a clear trend: Claude <a href=\"https://www.anthropic.com/news/claude-4\"><u>Opus 4</u></a> and <a href=\"https://www.anthropic.com/news/claude-opus-4-1\"><u>Opus 4.1</u></a> consistently outperformed all older models on introspection tasks, suggesting the capability strengthens alongside general intelligence. If this pattern continues, future models might develop substantially more sophisticated introspective abilities — potentially reaching human-level reliability, but also potentially learning to exploit introspection for deception.</p><p>Lindsey emphasized the field needs significantly more work before introspective AI becomes trustworthy. &quot;My biggest hope with this paper is to put out an implicit call for more people to benchmark their models on introspective capabilities in more ways,&quot; he said.</p><p>Future research directions include fine-tuning models specifically to improve introspective capabilities, exploring which types of representations models can and cannot introspect on, and testing whether introspection can extend beyond simple concepts to complex propositional statements or behavioral propensities.</p><p>&quot;It&#x27;s cool that models can do these things somewhat without having been trained to do them,&quot; Lindsey noted. &quot;But there&#x27;s nothing stopping you from training models to be more introspectively capable. I expect we could reach a whole different level if introspection is one of the numbers that we tried to get to go up on a graph.&quot;</p><p>The implications extend beyond Anthropic. If introspection proves a reliable path to AI transparency, other major labs will likely invest heavily in the capability. Conversely, if models learn to exploit introspection for deception, the entire approach could become a liability.</p><p>For now, the research establishes a foundation that reframes the debate about AI capabilities. The question is no longer whether language models might develop genuine introspective awareness — they already have, at least in rudimentary form. The urgent questions are how quickly that awareness will improve, whether it can be made reliable enough to trust, and whether researchers can stay ahead of the curve.</p><p>&quot;The big update for me from this research is that we shouldn&#x27;t dismiss models&#x27; introspective claims out of hand,&quot; Lindsey said. &quot;They do have the capacity to make accurate claims sometimes. But you definitely should not conclude that we should trust them all the time, or even most of the time.&quot;</p><p>He paused, then added a final observation that captures both the promise and peril of the moment: &quot;The models are getting smarter much faster than we&#x27;re getting better at understanding them.&quot;</p>",
    "published": "Wed, 29 Oct 2025 17:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-30T09:00:03.690251",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Geostar pioneers GEO as traditional SEO faces 25% decline from AI chatbots, Gartner says",
    "link": "https://venturebeat.com/ai/geostar-pioneers-geo-as-traditional-seo-faces-25-decline-from-ai-chatbots",
    "summary": "<p>The moment Mack McConnell knew everything about search had changed came last summer at the Paris Olympics. His parents, independently and without prompting, had both turned to <a href=\"https://chatgpt.com/\"><u>ChatGPT</u></a> to plan their day&#x27;s activities in the French capital. The AI recommended specific tour companies, restaurants, and attractions — businesses that had won a new kind of visibility lottery.</p><p>&quot;It was almost like this intuitive interface that older people were as comfortable with using as younger people,&quot; McConnell recalled in an exclusive interview with VentureBeat. &quot;I could just see the businesses were now being recommended.&quot;</p><p>That observation has now become the foundation of <a href=\"https://www.geostar.ai/\"><u>Geostar</u></a>, a Pear VC-backed startup that&#x27;s racing to help businesses navigate what may be the most significant shift in online discovery since Google&#x27;s founding. </p><p>The company, which recently emerged from stealth with impressive early customer traction, is betting that the rise of AI-powered search represents a significant opportunity to reinvent how companies get found online. The <a href=\"https://www.coherentmarketinsights.com/industry-reports/ai-search-engines-market\"><u>global AI search engine market</u></a> alone is projected to grow from $43.63 billion in 2025 to $108.88 billion by 2032.</p><p>Already the fastest-growing company in <a href=\"https://pear.vc/inside-pearx-partnering-with-founders-the-pear-way/\"><u>PearX&#x27;s latest cohort</u></a>, Geostar is fast approaching $1 million in annual recurring revenue in just four months — with only two founders and no employees.</p><h2><b>Why Gartner predicts traditional search volume will decline 25% by 2026</b></h2><p>The numbers tell a stark story of disruption. Gartner predicts that traditional search engine volume will <a href=\"https://www.forbes.com/councils/forbesagencycouncil/2025/10/23/generative-engine-optimization-the-next-frontier-in-seo/\"><u>decline by 25% by 2026</u></a>, largely due to the rise of AI chatbots. Google&#x27;s AI Overviews now appear on <a href=\"https://techcrunch.com/2025/04/25/googles-ai-search-numbers-are-growing-and-thats-by-design/\"><u>billions of searches</u></a> monthly. Princeton University researchers have found that optimizing for these new AI systems can increase visibility <a href=\"https://arxiv.org/pdf/2311.09735\"><u>by up to 40%</u></a>.</p><p>&quot;Search used to mean that you had to make Google happy,&quot; McConnell explained. &quot;But now you have to optimize for four different Google interfaces — traditional search, AI Mode, Gemini, and AI Overviews — each with different criteria. And then ChatGPT, Claude, and Perplexity each work differently on top of that.&quot;</p><p>This fragmentation is creating chaos for businesses that have spent decades perfecting their Google search strategies. A recent <a href=\"https://www.forrester.com/press-newsroom/forrester-the-state-of-business-buying-2024/\"><u>Forrester study</u></a> found that 95% of B2B buyers plan to use generative AI in future purchase decisions. Yet most companies remain woefully unprepared for this shift.</p><p>&quot;Anybody who&#x27;s not on this right now is losing out,&quot; said Cihan Tas, Geostar&#x27;s co-founder and chief technology officer. &quot;We see lawyers getting 50% of their clients through ChatGPT now. It&#x27;s just such a massive shift.&quot;</p><h2><b>How language models read the web differently than search engines ever did</b></h2><p>What <a href=\"https://www.geostar.ai/\"><u>Geostar</u></a> and a growing cohort of competitors call Generative Engine Optimization or GEO represents a fundamental departure from traditional search engine optimization. Where SEO focused primarily on keywords and backlinks, GEO requires understanding how large language models parse, understand, and synthesize information across the entire web.</p><p>The technical challenges are formidable. Every website must now function as what Tas calls &quot;its own little database&quot; capable of being understood by dozens of different AI crawlers, each with unique requirements and preferences. Google&#x27;s systems pull from their existing search index. <a href=\"https://chatgpt.com/\"><u>ChatGPT</u></a> relies heavily on structured data and specific content formats. Perplexity shows a marked preference for Wikipedia and authoritative sources.</p><p>&quot;Now the strategy is actually being concise, clear, and answering the question, because that&#x27;s directly what the AI is looking for,&quot; Tas explained. &quot;You&#x27;re actually tuning for somewhat of an intelligent model that makes decisions similarly to how we make decisions.&quot;</p><p>Consider schema markup, the structured data that helps machines understand web content. While only 30% of websites currently implement comprehensive schema, research shows that pages with proper markup are 36% more likely to appear in AI-generated summaries. Yet most businesses don&#x27;t even know what schema markup is, let alone how to implement it effectively.</p><h2><b>Inside Geostar&#x27;s AI agents that optimize websites continuously without human intervention</b></h2><p>Geostar&#x27;s solution embodies a broader trend in enterprise software: the rise of autonomous AI agents that can take action on behalf of businesses. The company embeds what it calls &quot;<a href=\"https://www.geostar.ai/\"><u>ambient agents</u></a>&quot; directly into client websites, continuously optimizing content, technical configurations, and even creating new pages based on patterns learned across its entire customer base.</p><p>&quot;Once we learn something about the way content performs, or the way a technical optimization performs, we can then syndicate that same change across the remaining users so everyone in the network benefits,&quot; McConnell said.</p><p>For <a href=\"https://redsift.com/\"><u>RedSift</u></a>, a cybersecurity company, this approach yielded a 27% increase in AI mentions within three months. In one case, Geostar identified an opportunity to rank for &quot;best DMARC vendors,&quot; a high-value search term in the email security space. The company&#x27;s agents created and optimized content that achieved first-page rankings on both Google and ChatGPT within four days.</p><p>&quot;We&#x27;re doing the work of an agency that charges $10,000 a month,&quot; McConnell said, noting that Geostar&#x27;s pricing ranges from $1,000 to $3,000 monthly. &quot;AI creates a situation where, for the first time ever, you can take action like an agency, but you can scale like software.&quot;</p><h2><b>Why brand mentions without links now matter more than ever in the AI era</b></h2><p>The implications of this shift extend far beyond technical optimizations. In the SEO era, a mention without a link was essentially worthless. In the age of AI, that calculus has reversed. AI systems can analyze vast amounts of text to understand sentiment and context, meaning that brand mentions on Reddit, in news articles, or across social media now directly influence how AI systems describe and recommend companies.</p><p>&quot;If the New York Times mentions a company without linking to it, that company would actually benefit from that in an AI system,&quot; McConnell explained. &quot;AI has the ability to do mass analysis of huge amounts of text, and it will understand the sentiment around that mention.&quot;</p><p>This has created new vulnerabilities. Research from the Indian Institute of Technology and Princeton found that AI systems show systematic bias toward third-party sources over brand-owned content. A company&#x27;s own website might be less influential in shaping AI perceptions than what others say about it online.</p><p>The shifting landscape has also disrupted traditional metrics of success. Where SEO focused on rankings and click-through rates, GEO must account for what researchers call impression metrics — how prominently and positively a brand appears within AI-generated responses, even when users never click through to the source.</p><h2><b>A growing market as SEO veterans and new players rush to dominate AI optimization</b></h2><p>Geostar is hardly alone in recognizing this opportunity. Companies like <a href=\"https://www.brandlight.ai/\"><u>Brandlight</u></a>, <a href=\"https://www.tryprofound.com/\"><u>Profound</u></a>, and <a href=\"https://higoodie.com/\"><u>Goodie</u></a> are all racing to help businesses navigate the new landscape. The SEO industry, worth approximately <a href=\"https://www.grandviewresearch.com/industry-analysis/ai-search-engine-market-report\"><u>$80 billion globally</u></a>, is scrambling to adapt, with established players like Semrush and Ahrefs rushing to add AI visibility tracking features.</p><p>But the company&#x27;s founders, who previously built and sold a Y-Combinator-backed e-commerce optimization startup called <a href=\"https://www.monto.io/\"><u>Monto</u></a>, believe their technical approach gives them an edge. Unlike competitors who largely provide dashboards and recommendations, Geostar&#x27;s agents actively implement changes.</p><p>&quot;Everyone is taking the same solutions that worked in the last era and just saying, &#x27;We&#x27;ll do this for AI instead,&#x27;&quot; McConnell argued. &quot;But when you think about what AI is truly capable of, it can actually do the work for you.&quot;</p><p>The stakes are particularly high for small and medium-sized businesses. While large corporations can afford to hire specialized consultants or build internal expertise, smaller companies risk becoming invisible in AI-mediated search. Geostar sees this as its primary market opportunity: nearly half of the 33.2 million small businesses in America invest in SEO. Among the roughly 418,000 law firms in the U.S., many spend <a href=\"https://www.sixthcitymarketing.com/2024/03/25/legal-marketing-stats/\"><u>between $2,500 and $5,000</u></a> monthly on search optimization to stay competitive in local markets.</p><h2><b>From Kurdish village to PearX: The unlikely partnership building the future of search</b></h2><p>For Tas, whose journey to Silicon Valley began in a tiny Kurdish village in Turkey with just 50 residents, the current moment represents both opportunity and responsibility. His mother&#x27;s battle with cancer prevented him from finishing college, leading him to teach himself programming and eventually partner with McConnell — whom he worked with for an entire year before they ever met in person.</p><p>&quot;We&#x27;re not just copy and pasting a solution that was existing before,&quot; Tas emphasized. &quot;This is something that&#x27;s different and was uniquely possible today.&quot;</p><p>Looking forward, the transformation of search appears to be accelerating rather than stabilizing. Industry observers predict that search functionality will soon be embedded in productivity tools, wearables, and even augmented reality interfaces. Each new surface will likely have its own optimization requirements, further complicating the landscape.</p><p>&quot;Soon, search will be in our eyes, in our ears,&quot; McConnell predicted. &quot;When Siri breaks out of her prison, whatever that Jony Ive and OpenAI are building together will be like a multimodal search interface.&quot;</p><p>The technical challenges are matched by ethical ones. As businesses scramble to influence AI recommendations, questions arise about manipulation, fairness, and transparency. There&#x27;s currently no oversight body or established best practices for GEO, creating what some critics describe as a Wild West environment.</p><p>As businesses grapple with these changes, one thing seems certain: the era of simply optimizing for Google is over. In its place is emerging a far more complex ecosystem where success requires understanding not just how machines index information, but how they think about it, synthesize it, and ultimately decide what to recommend to humans seeking answers.</p><p>For the millions of businesses whose survival depends on being discovered online, mastering this new paradigm isn&#x27;t just an opportunity — it&#x27;s an existential imperative. The question is no longer whether to optimize for AI search, but whether companies can adapt quickly enough to remain visible as the pace of change accelerates.</p><p>McConnell&#x27;s parents at the Olympics were a preview of what&#x27;s already becoming the norm. They didn&#x27;t search for tour companies in Paris. They didn&#x27;t scroll through results or click on links. They simply asked ChatGPT what to do — and the AI decided which businesses deserved their attention.</p><p>In the new economy of discovery, the businesses that win won&#x27;t be the ones that rank highest. They&#x27;ll be the ones AI chooses to recommend.</p><p>\n</p>",
    "published": "Wed, 29 Oct 2025 07:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-30T09:00:03.690816",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "From static classifiers to reasoning engines: OpenAI’s new model rethinks content moderation",
    "link": "https://venturebeat.com/ai/from-static-classifiers-to-reasoning-engines-openais-new-model-rethinks",
    "summary": "<p>Enterprises, eager to ensure any AI models they use <a href=\"https://venturebeat.com/security/red-team-ai-now-to-build-safer-smarter-models-tomorrow\"><u>adhere to safety and safe-use</u></a> policies, fine-tune LLMs so they do not respond to unwanted queries. </p><p>However, much of the safeguarding and red teaming happens before deployment, “baking in” policies before users fully test the models’ capabilities in production. <a href=\"https://openai.com/\"><u>OpenAI</u></a> believes it can offer a more flexible option for enterprises and encourage more companies to bring in safety policies. </p><p>The company has released two open-weight models under research preview that it believes will make enterprises and models more flexible in terms of safeguards. gpt-oss-safeguard-120b and gpt-oss-safeguard-20b will be available on a permissive Apache 2.0 license. The models are fine-tuned versions of OpenAI’s open-source <a href=\"https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b\"><u>gpt-oss, released in August</u></a>, marking the first release in the oss family since the summer.</p><p>In a <a href=\"https://openai.com/index/introducing-gpt-oss-safeguard/\"><u>blog post</u></a>, OpenAI said oss-safeguard uses reasoning “to directly interpret a developer-provider policy at inference time — classifying user messages, completions and full chats according to the developer’s needs.”</p><p>The company explained that, since the model uses a chain-of-thought (CoT), developers can get explanations of the model&#x27;s decisions for review. </p><p>“Additionally, the policy is provided during inference, rather than being trained into the model, so it is easy for developers to iteratively revise policies to increase performance,&quot; OpenAI said in its post. &quot;This approach, which we initially developed for internal use, is significantly more flexible than the traditional method of training a classifier to indirectly infer a decision boundary from a large number of labeled examples.&quot; </p><p>Developers can download both models from <a href=\"https://huggingface.co/\"><u>Hugging Face</u></a>. </p><h2>Flexibility versus baking in</h2><p>At the onset, AI models will not know a company’s preferred safety triggers. While model providers do red-team <a href=\"https://venturebeat.com/security/openais-red-team-plan-make-chatgpt-agent-an-ai-fortress\"><u>models and platforms</u></a>, these safeguards are intended for broader use. Companies like <a href=\"https://www.microsoft.com/\"><u>Microsoft</u></a> and <a href=\"https://venturebeat.com/ai/microsoft-unveils-trustworthy-ai-features-to-fix-hallucinations-and-boost-privacy\"><u>Amazon Web Services</u></a> even <a href=\"https://venturebeat.com/ai/microsoft-unveils-trustworthy-ai-features-to-fix-hallucinations-and-boost-privacy\"><u>offer platforms</u></a> to bring <a href=\"https://venturebeat.com/ai/aws-makes-guardrails-a-standalone-api-as-it-updates-bedrock\"><u>guardrails to AI applications</u></a> and agents. </p><p>Enterprises use safety classifiers to help train a model to recognize patterns of good or bad inputs. This helps the models learn which queries they shouldn’t reply to. It also helps ensure that the models do not drift and answer accurately.</p><p>“Traditional classifiers can have high performance, with low latency and operating cost,&quot; OpenAI said. &quot;But gathering a sufficient quantity of training examples can be time-consuming and costly, and updating or changing the policy requires re-training the classifier.&quot;</p><p>The models takes in two inputs at once before it outputs a conclusion on where the content fails. It takes a policy and the content to classify under its guidelines. OpenAI said the models work best in situations where: </p><ul><li><p>The potential harm is emerging or evolving, and policies need to adapt quickly.</p></li><li><p>The domain is highly nuanced and difficult for smaller classifiers to handle.</p></li><li><p>Developers don’t have enough samples to train a high-quality classifier for each risk on their platform.</p></li><li><p>Latency is less important than producing high-quality, explainable labels.</p></li></ul><p>The company said gpt-oss-safeguard “is different because its reasoning capabilities allow developers to apply any policy,” even ones they’ve written during inference. </p><p>The models are based on OpenAI’s internal tool, the Safety Reasoner, which enables its teams to be more iterative in setting guardrails. They often begin with very strict safety policies, “and use relatively large amounts of compute where needed,” then adjust policies as they move the model through production and risk assessments change. </p><h2>Performing safety</h2><p>OpenAI said the gpt-oss-safeguard models outperformed its GPT-5-thinking and the original gpt-oss models on multipolicy accuracy based on benchmark testing. It also ran the models on the ToxicChat public benchmark, where they performed well, although GPT-5-thinking and the Safety Reasoner slightly edged them out.</p><p>But there is concern that this approach could bring a centralization of safety standards.</p><p>“Safety is not a well-defined concept. Any implementation of safety standards will reflect the values and priorities of the organization that creates it, as well as the limits and deficiencies of its models,” said John Thickstun, an assistant professor of computer science at Cornell University. “If industry as a whole adopts standards developed by OpenAI, we risk institutionalizing one particular perspective on safety and short-circuiting broader investigations into the safety needs for AI deployments across many sectors of society.”</p><p>It should also be noted that OpenAI did not release the base model for the oss family of models, so developers cannot fully iterate on them. </p><p>OpenAI, however, is confident that the developer community can help refine gpt-oss-safeguard. It will host a Hackathon on December 8 in San Francisco. </p>",
    "published": "Wed, 29 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-30T09:00:03.691041",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Security's AI dilemma: Moving faster while risking more",
    "link": "https://venturebeat.com/security/securitys-ai-dilemma-moving-faster-while-risking-more",
    "summary": "<p><i>Presented by Splunk, a Cisco Company</i></p><hr /><p>As AI rapidly evolves from a theoretical promise to an operational reality, CISOs and CIOs face a fundamental challenge: how to harness AI&#x27;s transformative potential while maintaining the human oversight and strategic thinking that security demands. The rise of agentic AI is reshaping security operations, but success requires balancing automation with accountability.</p><h4><b>The efficiency paradox: Automation without abdication</b></h4><p>The pressure to adopt AI is intense. Organizations are being pushed to reduce headcount or redirect resources toward AI-driven initiatives, often without fully understanding what that transformation entails. The promise is compelling: AI can reduce investigation times from 60 minutes to just 5 minutes, potentially delivering 10x productivity improvements for security analysts.</p><p>However, the critical question isn&#x27;t whether AI can automate tasks — it&#x27;s which tasks should be automated and where human judgment remains irreplaceable. The answer lies in understanding that AI excels at accelerating investigative workflows, but remediation and response actions still require human validation. Taking a system offline or quarantining an endpoint can have massive business impact. An AI making that call autonomously could inadvertently cause the very disruption it&#x27;s meant to prevent.</p><p>The goal isn&#x27;t to replace security analysts but to free them for higher-value work. With routine alert triage automated, analysts can focus on red team/blue team exercises, collaborate with engineering teams on remediation, and engage in proactive threat hunting. There&#x27;s no shortage of security problems to solve — there&#x27;s a shortage of security experts to address them strategically.</p><h4><b>The trust deficit: Showing your work</b></h4><p>While confidence in AI&#x27;s ability to improve efficiency is high, skepticism about the quality of AI-driven decisions remains significant. Security teams need more than just AI-generated conclusions — they need transparency into how those conclusions were reached.</p><p>When AI determines an alert is benign and closes it, SOC analysts need to understand the investigative steps that led to that determination. What data was examined? What patterns were identified? What alternative explanations were considered and ruled out?</p><p>This transparency builds trust in AI recommendations, enables validation of AI logic, and creates opportunities for continuous improvement. Most importantly, it maintains the critical human-in-the-loop for complex judgment calls that require nuanced understanding of business context, compliance requirements, and potential cascading impacts.</p><p>The future likely involves a hybrid model where autonomous capabilities are integrated into guided workflows and playbooks, with analysts remaining involved in complex decisions. </p><h4><b>The adversarial advantage: Fighting AI with AI — carefully</b></h4><p>AI presents a dual-edged sword in security. While we&#x27;re carefully implementing AI with appropriate guardrails, adversaries face no such constraints. AI lowers the barrier to entry for attackers, enabling rapid exploit development and vulnerability discovery at scale. What was once the domain of sophisticated threat actors could soon be accessible to script kiddies armed with AI tools.</p><p>The asymmetry is striking: defenders must be thoughtful and risk-averse, while attackers can experiment freely. If we make a mistake implementing autonomous security responses, we risk taking down production systems. If an attacker&#x27;s AI-driven exploit fails, they simply try again with no consequences.</p><p>This creates an imperative to use AI defensively, but with appropriate caution. We must learn from attackers&#x27; techniques while maintaining the guardrails that prevent our AI from becoming the vulnerability. The recent emergence of <a href=\"https://owasp.org/www-project-mcp-top-10/\">malicious MCP</a> (Model Context Protocol) supply chain attacks demonstrates how quickly adversaries exploit new AI infrastructure. </p><h4><b>The skills dilemma: Building capabilities while maintaining core competencies</b></h4><p>As AI handles more routine investigative work, a concerning question emerges: will security professionals&#x27; fundamental skills atrophy over time? This isn&#x27;t an argument against AI adoption — it&#x27;s a call for intentional skill development strategies. Organizations must balance AI-enabled efficiency with programs that maintain core competencies. This includes regular exercises that require manual investigation, cross-training that deepens understanding of underlying systems, and career paths that evolve roles rather than eliminate them.</p><p>The responsibility is shared. Employers must provide tools, training, and culture that enable AI to augment rather than replace human expertise. Employees must actively engage in continuous learning, treating AI as a collaborative partner rather than a replacement for critical thinking.</p><h4><b>The identity crisis: Governing the agent explosion</b></h4><p>Perhaps the most underestimated challenge ahead is identity and access management in an agentic AI world. <a href=\"https://www.splunk.com/en_us/form/from-data-management-to-machine-data-fabric.html\">IDC estimates 1.3 billion agents by 2028</a> — each requiring identity, permissions, and governance. The complexity compounds exponentially.</p><p>Overly permissive agents represent significant risk. An agent with broad administrative access could be socially engineered into taking destructive actions, approving fraudulent transactions, or exfiltrating sensitive data. The technical shortcuts engineers take to &quot;just make it work&quot; — granting excessive permissions to expedite deployment — create vulnerabilities that adversaries will exploit.</p><p>Tool-based access control offers one path forward, granting agents only the specific capabilities they need. But governance frameworks must also address how LLMs themselves might learn and retain authentication information, potentially enabling impersonation attacks that bypass traditional access controls.</p><h4><b>The path forward: Start with compliance and reporting</b></h4><p>Amid these challenges, one area offers immediate, high-impact opportunity: continuous compliance and risk reporting. AI&#x27;s ability to consume vast amounts of documentation, interpret complex requirements, and generate concise summaries makes it ideal for compliance and reporting work that has traditionally consumed enormous analysts’ time. This represents a low-risk, high-value entry point for AI in security operations. </p><h4><b>The data foundation: Enabling the AI-powered SOC</b></h4><p>None of these AI capabilities can succeed without addressing the fundamental data challenges facing security operations. SOC teams struggle with siloed data and disparate tools. Success requires a deliberate data strategy that prioritizes accessibility, quality, and unified data contexts. Security-relevant data must be immediately available to AI agents without friction, properly governed to ensure reliability, and enriched with metadata that provides the business context AI cannot understand. </p><h4><b>Closing thought: Innovation with intentionality</b></h4><p>The autonomous SOC is emerging — not as a light switch to flip, but as an evolutionary journey requiring continuous adaptation. Success demands that we embrace AI&#x27;s efficiency gains while maintaining the human judgment, strategic thinking, and ethical oversight that security requires.</p><p>We&#x27;re not replacing security teams with AI. We&#x27;re building collaborative, multi-agent systems where human expertise guides AI capabilities toward outcomes that neither could achieve alone. That&#x27;s the promise of the agentic AI era — if we&#x27;re intentional about how we get there.</p><hr /><p><i>Tanya Faddoul, VP Product, Customer Strategy and Chief of Staff for Splunk, a Cisco Company. Michael Fanning is Chief Information Security Officer for Splunk, a Cisco Company.</i></p><p><i> </i><a href=\"https://www.splunk.com/en_us/products/platform.html\"><i>Cisco Data Fabric</i></a><i> provides the needed data architecture powered by Splunk Platform — unified data fabric, federated search capabilities, comprehensive metadata management — to unlock AI and SOC’s full potential. Learn more about </i><a href=\"https://www.splunk.com/en_us/products/platform.html\"><i>Cisco Data Fabric</i></a><i>.</i></p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>",
    "published": "Wed, 29 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-30T09:00:03.691317",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Agentic AI is all about the context — engineering, that is",
    "link": "https://venturebeat.com/ai/agentic-ai-is-all-about-the-context-engineering-that-is",
    "summary": "<p><i>Presented by Elastic</i></p><hr /><p><b><i>As organizations scramble to enact agentic AI solutions, accessing proprietary data from all the nooks and crannies will be key</i></b></p><p>By now, most organizations have heard of agentic AI, which are systems that “think” by autonomously gathering tools, data and other sources of information to return an answer. But here’s the rub: reliability and relevance depend on delivering accurate context. In most enterprises, this context is scattered across various unstructured data sources, including documents, emails, business apps, and customer feedback. </p><p>As organizations look ahead to 2026, solving this problem will be key to accelerating agentic AI rollouts around the world, says Ken Exner, chief product officer at Elastic. </p><p>&quot;People are starting to realize that to do agentic AI correctly, you have to have relevant data,&quot; Exner says. &quot;Relevance is critical in the context of agentic AI, because that AI is taking action on your behalf. When people struggle to build AI applications, I can almost guarantee you the problem is relevance.”</p><h4><b>Agents everywhere</b></h4><p>The struggle could be entering a make-or-break period as organizations scramble for competitive edge or to create new efficiencies. A Deloitte study <a href=\"https://www.deloitte.com/us/en/services/consulting/blogs/new-ai-breakthroughs-ai-trends.html\">predicts</a> that by 2026, more than 60% of large enterprises will have deployed agentic AI at scale, marking a major increase from experimental phases to mainstream implementation. And researcher Gartner <a href=\"https://www.gartner.com/en/newsroom/press-releases/2025-08-26-gartner-predicts-40-percent-of-enterprise-apps-will-feature-task-specific-ai-agents-by-2026-up-from-less-than-5-percent-in-2025#:~:text=By%20End%20of%202025%2C%20the,collaboration%20and%20dynamic%20workflow%20orchestration.%E2%80%9D\">forecasts</a> that by the end of 2026, 40% of all enterprise applications will incorporate task-specific agents, up from less than 5% in 2025. Adding task specialization capabilities evolves AI assistants into context-aware AI agents.</p><h4><b>Enter context engineering</b></h4><p>The process for getting the relevant context into agents at the right time is known as context engineering. It not only ensures that an agentic application has the data it needs to provide accurate, in-depth responses, it helps the large language model (LLM) understand what tools it needs to find and use that data, and how to call those APIs. </p><p>While there are now open-source standards such as the Model Context Protocol (MCP) that allow LLMs to connect to and communicate with external data, there are few platforms that let organizations build precise AI agents that use your data and combine retrieval, governance, and orchestration in one place, natively. </p><p>Elasticsearch has always been a leading platform for the core of context engineering. It recently released a new feature within Elasticsearch called Agent Builder, which simplifies the entire operational lifecycle of agents: development, configuration, execution, customization, and observability.</p><p>Agent Builder helps build MCP tools on private data using various techniques, including Elasticsearch Query Language, a piped query language for filtering, transforming, and analyzing data, or workflow modeling. Users can then take various tools and combine them with prompts and an LLM to build an agent. </p><p>Agent Builder offers a configurable, out-of-the-box conversational agent that allows you to chat with the data in the index, and it also gives users the ability to build one from scratch using various tools and prompts on top of private data. </p><p>&quot;Data is the center of our world at Elastic. We’re trying to make sure that you have the tools you need to put that data to work,&quot; Exner explains. &quot;The second you open up Agent Builder, you point it to an index in Elasticsearch, and you can begin chatting with any data you connect this to, any data that’s indexed in Elasticsearch — or from external sources through integrations.”</p><h4><b>Context engineering as a discipline</b></h4><p>Prompt and context engineering is becoming a discipli. It’s not something you need a computer science degree in, but more classes and best practices will emerge, because there’s an art to it. </p><p>&quot;We want to make it very simple to do that,&quot; Exner says. &quot;The thing that people will have to figure out is, how do you drive automation with AI? That’s what’s going to drive productivity. The people who are focused on that will see more success.&quot;</p><p>Beyond that, other context engineering patterns will emerge. The industry has gone from prompt engineering to retrieval-augmented generation, where information is passed to the LLM in a context window, to MCP solutions that help LLMs with tool selection. But it won&#x27;t stop there.</p><p>&quot;Given how fast things are moving, I will guarantee that new patterns will emerge quite quickly,&quot; Exner says. &quot;There will still be context engineering, but they’ll be new patterns for how to share data with an LLM, how to get it to be grounded in the right information. And I predict more patterns that make it possible for the LLM to understand private data that it’s not been trained on.&quot;</p><p><i>Agent Builder is available now as a tech preview. Get started with an </i><a href=\"https://cloud.elastic.co/registration?onboarding_token=search&amp;pg=en-enterprise-search-page\"><b><i>Elastic Cloud Trial</i></b></a><i>, and check out the documentation for Agent Builder </i><a href=\"https://www.elastic.co/docs/solutions/search/elastic-agent-builder\"><b><i>here</i></b></a><i>.</i></p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>",
    "published": "Wed, 29 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-30T09:00:03.691544",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Nvidia researchers unlock 4-bit LLM training that matches 8-bit performance",
    "link": "https://venturebeat.com/ai/nvidia-researchers-unlock-4-bit-llm-training-that-matches-8-bit-performance",
    "summary": "<p>Researchers at Nvidia have developed a <a href=\"https://arxiv.org/abs/2509.25149\"><u>novel approach</u></a> to train large language models (LLMs) in 4-bit quantized format while maintaining their stability and accuracy at the level of high-precision models. Their technique, NVFP4, makes it possible to train models that not only outperform other leading 4-bit formats but match the performance of the larger 8-bit FP8 format, all while using half the memory and a fraction of the compute.</p><p>The success of NVFP4 shows that enterprises can continue to cut inference costs by running leaner models that match the performance of larger ones. It also hints at a future where the cost of training LLMs will drop to a point where many more organizations can train their own bespoke models from scratch rather than just fine-tuning existing ones.</p><h2>The quantization challenge</h2><p><a href=\"https://venturebeat.com/ai/here-are-3-critical-llm-compression-strategies-to-supercharge-ai-performance\"><u>Model quantization</u></a> is a technique used to reduce the computational and memory costs of running and training AI models. It works by converting the model&#x27;s parameters, or weights, from high-precision formats like 16- and 32-bit floating point (BF16 and FP32) to lower-precision formats. The key challenge of quantization is to reduce the size of the model while preserving as much of its knowledge and capabilities as possible.</p><p>In recent years, 8-bit floating point formats (FP8) have become a popular industry standard, offering a good balance between performance and efficiency. They significantly lower the computational cost and memory demand for LLM training without a major drop in accuracy.</p><p>The next logical step is 4-bit floating point (FP4), which promises to halve memory usage again and further boost performance on advanced hardware. However, this transition has been challenging. Existing 4-bit formats, such as MXFP4, often struggle to maintain the same level of accuracy as their 8-bit counterparts, forcing a difficult trade-off between cost and performance.</p><h2>How NVFP4 works</h2><p>NVFP4 overcomes the stability and accuracy challenges of other FP4 techniques through a smarter design and a targeted training methodology. A key issue with 4-bit precision is its extremely limited range: It can only represent 16 distinct values. When converting from a high-precision format, outlier values can distort the entire dataset, harming the model&#x27;s accuracy. NVFP4 uses a more sophisticated, multi-level scaling approach that better handles these outliers, allowing for a &quot;more precise and accurate representation of tensor values during training,&quot; according to Nvidia.</p><p>Beyond the format, the researchers introduce a 4-bit training recipe that achieves accuracy comparable to FP8. A central component is their “mixed-precision strategy.” Instead of converting the entire model to NVFP4, the majority of layers are quantized while a small fraction of numerically sensitive layers are kept in a higher-precision format like BF16. This preserves stability where it matters most. The methodology also adjusts how gradients are calculated during backpropagation — or the model&#x27;s learning phase — to reduce biases that can accumulate from low-precision arithmetic.</p><h2>NVFP4 in practice</h2><p>To test their approach, the Nvidia team trained a powerful 12-billion-parameter hybrid <a href=\"https://venturebeat.com/ai/beyond-transformers-nvidias-mambavision-aims-to-unlock-faster-cheaper-enterprise-computer-vision\"><u>Mamba-Transformer model</u></a> on a massive 10 trillion tokens. They then compared its performance directly against a baseline model trained in the widely popular FP8 format. The results showed that the NVFP4 model&#x27;s training loss and downstream task accuracy closely tracked the FP8 version throughout the entire process.</p><p>The performance held across a wide range of domains, including knowledge-intensive reasoning, mathematics and commonsense tasks, with only a slight drop-off in coding benchmarks in late training.</p><p>&quot;This marks, to our knowledge, the first successful demonstration of training billion-parameter language models with 4-bit precision over a multi-trillion-token horizon, laying the foundation for faster and more efficient training of future frontier models,” the researchers write.</p><p>According to Nvidia&#x27;s director of product for AI and data center GPUs NvidiaShar Narasimhan, in practice, NVFP4’s 4-bit precision format enables developers and businesses to train and deploy AI models with nearly the same accuracy as traditional 8-bit formats. </p><p>“By training model weights directly in 4-bit format while preserving accuracy, it empowers developers to experiment with new architectures, iterate faster and uncover insights without being bottlenecked by resource constraints,” he told VentureBeat. </p><p>In contrast, FP8 (while already a leap forward from FP16) still imposes limits on model size and inference performance due to higher memory and bandwidth demands. “NVFP4 breaks that ceiling, offering equivalent quality with dramatically greater headroom for growth and experimentation,” Narasimhan said.</p><p>When compared to the alternative 4-bit format, MXFP4, the benefits of NVFP4 become even clearer. In an experiment with an 8-billion-parameter model, NVFP4 converged to a better loss score than MXFP4. To reach the same level of performance as the NVFP4 model, the MXFP4 model had to be trained on 36% more data, a considerable increase in training time and cost.</p><p>In addition to making pretraining more efficient, NVFP4 also redefines what’s possible. “Showing that 4-bit precision can preserve model quality at scale opens the door to a future where highly specialized models can be trained from scratch by mid-sized enterprises or startups, not just hyperscalers,” Narasimhan said, adding that, over time, we can expect a shift from developing general purpose LLMs models to “a diverse ecosystem of custom, high-performance models built by a broader range of innovators.”</p><h2>Beyond pre-training</h2><p>Although the paper focuses on the advantages of NVFP4 during pretraining, its impact extends to inference, as well. </p><p>“Models trained on NVFP4 can not only deliver faster inference and higher throughput but shorten the time required for AI factories to achieve ROI — accelerating the cycle from model development to real-world deployment,” Narasimhan said. </p><p>Because these models are smaller and more efficient, they unlock new possibilities for serving complex, high-quality responses in real time, even in token-intensive, agentic applications, without raising energy and compute costs. </p><p>Narasimhan said he looks toward a future of model efficiency that isn’t solely about pushing precision lower, but building smarter systems. </p><p>“There are many opportunities to expand research into lower precisions as well as modifying architectures to address the components that increasingly dominate compute in large-scale models,” he said. “These areas are rich with opportunity, especially as we move toward agentic systems that demand high throughput, low latency and adaptive reasoning. NVFP4 proves that precision can be optimized without compromising quality, and it sets the stage for a new era of intelligent, efficient AI design.”</p>",
    "published": "Wed, 29 Oct 2025 00:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-30T09:00:03.691801",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "IBM's open source Granite 4.0 Nano AI models are small enough to run locally directly in your browser",
    "link": "https://venturebeat.com/ai/ibms-open-source-granite-4-0-nano-ai-models-are-small-enough-to-run-locally",
    "summary": "<p>In an industry where model size is often seen as a proxy for intelligence, IBM is charting a different course — one that values <i>efficiency over enormity</i>, and <i>accessibility over abstraction</i>.</p><p>The 114-year-old tech giant&#x27;s <a href=\"https://huggingface.co/blog/ibm-granite/granite-4-nano\">four new Granite 4.0 Nano models</a>, released today, range from just 350 million to 1.5 billion parameters, a fraction of the size of their server-bound cousins from the likes of OpenAI, Anthropic, and Google. </p><p>These models are designed to be highly accessible: the 350M variants can run comfortably on a modern laptop CPU with 8–16GB of RAM, while the 1.5B models typically require a GPU with at least 6–8GB of VRAM for smooth performance — or sufficient system RAM and swap for CPU-only inference. This makes them well-suited for developers building applications on consumer hardware or at the edge, without relying on cloud compute.</p><p>In fact, the smallest ones can even run locally on your own web browser, as Joshua Lochner aka <a href=\"https://x.com/xenovacom/status/1983218720366326002\">Xenova</a>, creator of Transformer.js and a machine learning engineer at Hugging Face, wrote on the social network X.</p><div></div><p><b>All the Granite 4.0 Nano models are released under the Apache 2.0 license</b> — perfect for use by researchers and enterprise or indie developers, even for commercial usage. </p><p>They are natively compatible with llama.cpp, vLLM, and MLX and are certified under ISO 42001 for responsible AI development — a standard IBM helped pioneer.</p><p>But in this case, small doesn&#x27;t mean less capable — it might just mean smarter design.</p><p>These compact models are built not for data centers, but for edge devices, laptops, and local inference, where compute is scarce and latency matters. </p><p>And despite their small size, the Nano models are showing benchmark results that rival or even exceed the performance of larger models in the same category. </p><p>The release is a signal that a new AI frontier is rapidly forming — one not dominated by sheer scale, but by <i>strategic scaling</i>.</p><h3><b>What Exactly Did IBM Release?</b></h3><p>The <b>Granite 4.0 Nano</b> family includes four open-source models now available on <a href=\"https://huggingface.co/collections/ibm-granite/granite-40-nano-language-models\">Hugging Face</a>:</p><ul><li><p><b>Granite-4.0-H-1B</b> (~1.5B parameters) – Hybrid-SSM architecture</p></li><li><p><b>Granite-4.0-H-350M</b> (~350M parameters) – Hybrid-SSM architecture</p></li><li><p><b>Granite-4.0-1B</b> – Transformer-based variant, parameter count closer to 2B</p></li><li><p><b>Granite-4.0-350M</b> – Transformer-based variant</p></li></ul><p>The H-series models — Granite-4.0-H-1B and H-350M — use a hybrid state space architecture (SSM) that combines efficiency with strong performance, ideal for low-latency edge environments. </p><p>Meanwhile, the standard transformer variants — Granite-4.0-1B and 350M — offer broader compatibility with tools like llama.cpp, designed for use cases where hybrid architecture isn’t yet supported. </p><p>In practice, the transformer 1B model is closer to 2B parameters, but aligns performance-wise with its hybrid sibling, offering developers flexibility based on their runtime constraints.</p><p>“The hybrid variant is a true 1B model. However, the non-hybrid variant is closer to 2B, but we opted to keep the naming aligned to the hybrid variant to make the connection easily visible,” explained Emma, Product Marketing lead for Granite, during a <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/\">Reddit &quot;Ask Me Anything&quot; (AMA) session on r/LocalLLaMA.</a></p><h3><b>A Competitive Class of Small Models</b></h3><p>IBM is entering a crowded and rapidly evolving market of small language models (SLMs), competing with offerings like Qwen3, Google&#x27;s Gemma, LiquidAI’s LFM2, and even Mistral’s dense models in the sub-2B parameter space.</p><p>While OpenAI and Anthropic focus on models that require clusters of GPUs and sophisticated inference optimization, IBM’s Nano family is aimed squarely at developers who want to run performant LLMs on local or constrained hardware.</p><p>In benchmark testing, IBM’s new models consistently top the charts in their class. According to data<a href=\"https://x.com/neurobongo/status/1983224452838985972\"> shared on X by David Cox, VP of AI Models at IBM Research:</a></p><ul><li><p>On IFEval (instruction following), Granite-4.0-H-1B scored 78.5, outperforming Qwen3-1.7B (73.1) and other 1–2B models.</p></li><li><p>On BFCLv3 (function/tool calling), Granite-4.0-1B led with a score of 54.8, the highest in its size class.</p></li><li><p>On safety benchmarks (SALAD and AttaQ), the Granite models scored over 90%, surpassing similarly sized competitors.</p></li></ul><p>Overall, the Granite-4.0-1B achieved a leading average benchmark score of 68.3% across general knowledge, math, code, and safety domains.</p><p>This performance is especially significant given the hardware constraints these models are designed for. </p><p>They require less memory, run faster on CPUs or mobile devices, and don’t need cloud infrastructure or GPU acceleration to deliver usable results.</p><h3><b>Why Model Size Still Matters — But Not Like It Used To</b></h3><p>In the early wave of LLMs, bigger meant better — more parameters translated to better generalization, deeper reasoning, and richer output. </p><p>But as transformer research matured, it became clear that architecture, training quality, and task-specific tuning could allow smaller models to punch well above their weight class.</p><p>IBM is banking on this evolution. By releasing open, small models that are <i>competitive in real-world tasks</i>, the company is offering an alternative to the monolithic AI APIs that dominate today’s application stack.</p><p>In fact, the Nano models address three increasingly important needs:</p><ol><li><p><b>Deployment flexibility</b> — they run anywhere, from mobile to microservers.</p></li><li><p><b>Inference privacy</b> — users can keep data local with no need to call out to cloud APIs.</p></li><li><p><b>Openness and auditability</b> — source code and model weights are publicly available under an open license.</p></li></ol><h3><b>Community Response and Roadmap Signals</b></h3><p>IBM’s Granite team didn’t just launch the models and walk away — they took to <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/\">Reddit’s open source community r/LocalLLaMA </a>to engage directly with developers. </p><p>In an AMA-style thread, Emma (Product Marketing, Granite) answered technical questions, addressed concerns about naming conventions, and dropped hints about what’s next.</p><p>Notable confirmations from the thread:</p><ul><li><p>A larger Granite 4.0 model is currently in training</p></li><li><p>Reasoning-focused models (&quot;thinking counterparts&quot;) are in the pipeline</p></li><li><p>IBM will release fine-tuning recipes and a full training paper soon</p></li><li><p>More tooling and platform compatibility is on the roadmap</p></li></ul><p>Users responded enthusiastically to the models’ capabilities, especially in instruction-following and structured response tasks. One commenter summed it up:</p><blockquote><p><i>“This is big if true for a 1B model — if quality is nice and it gives consistent outputs. Function-calling tasks, multilingual dialog, FIM completions… this could be a real workhorse.”</i></p></blockquote><p>Another user remarked:</p><blockquote><p><i>“The Granite Tiny is already my go-to for web search in LM Studio — better than some Qwen models. Tempted to give Nano a shot.”</i></p></blockquote><h3><b>Background: IBM Granite and the Enterprise AI Race</b></h3><p>IBM’s push into large language models began in earnest in late 2023 with the debut of the Granite foundation model family, starting with models like <i>Granite.13b.instruct</i> and <i>Granite.13b.chat</i>. Released for use within its Watsonx platform, these initial decoder-only models signaled IBM’s ambition to build enterprise-grade AI systems that prioritize transparency, efficiency, and performance. The company open-sourced select Granite code models under the Apache 2.0 license in mid-2024, laying the groundwork for broader adoption and developer experimentation.</p><p>The real inflection point came with <a href=\"https://venturebeat.com/ai/ibm-debuts-open-source-granite-3-0-llms-for-enterprise-ai/\">Granite 3.0</a> in October 2024 — a fully open-source suite of general-purpose and domain-specialized models ranging from 1B to 8B parameters. These models emphasized efficiency over brute scale, offering capabilities like longer context windows, instruction tuning, and integrated guardrails. IBM positioned Granite 3.0 as a direct competitor to Meta’s Llama, Alibaba’s Qwen, and Google&#x27;s Gemma — but with a uniquely enterprise-first lens. Later versions, including <a href=\"https://venturebeat.com/ai/ibm-wants-to-be-the-enterprise-llm-king-with-its-new-open-source-granite-3-1-models/\">Granite 3.1</a> and <a href=\"https://venturebeat.com/ai/ibm-granite-3-2-uses-conditional-reasoning-time-series-forecasting-and-document-vision-to-tackle-challenging-enterprise-use-cases/\">Granite 3.2</a>, introduced even more enterprise-friendly innovations: embedded hallucination detection, time-series forecasting, document vision models, and conditional reasoning toggles.</p><p>The <a href=\"https://venturebeat.com/ai/western-qwen-ibm-wows-with-granite-4-llm-launch-and-hybrid-mamba-transformer/\">Granite 4.0</a> family, launched in October 2025, represents IBM’s most technically ambitious release yet. It introduces a hybrid architecture that blends transformer and Mamba-2 layers — aiming to combine the contextual precision of attention mechanisms with the memory efficiency of state-space models. This design allows IBM to significantly reduce memory and latency costs for inference, making Granite models viable on smaller hardware while still outperforming peers in instruction-following and function-calling tasks. The launch also includes ISO 42001 certification, cryptographic model signing, and distribution across platforms like Hugging Face, Docker, LM Studio, Ollama, and watsonx.ai.</p><p>Across all iterations, IBM’s focus has been clear: build trustworthy, efficient, and legally unambiguous AI models for enterprise use cases. With a permissive Apache 2.0 license, public benchmarks, and an emphasis on governance, the Granite initiative not only responds to rising concerns over proprietary black-box models but also offers a Western-aligned open alternative to the rapid progress from teams like Alibaba’s Qwen. In doing so, Granite positions IBM as a leading voice in what may be the next phase of open-weight, production-ready AI.</p><h3><b>A Shift Toward Scalable Efficiency</b></h3><p>In the end, IBM’s release of Granite 4.0 Nano models reflects a strategic shift in LLM development: from chasing parameter count records to optimizing usability, openness, and deployment reach.</p><p>By combining competitive performance, responsible development practices, and deep engagement with the open-source community, IBM is positioning Granite as not just a family of models — but a platform for building the next generation of lightweight, trustworthy AI systems.</p><p>For developers and researchers looking for performance without overhead, the Nano release offers a compelling signal: you don’t need 70 billion parameters to build something powerful — just the right ones.</p>",
    "published": "Tue, 28 Oct 2025 23:23:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-30T09:00:03.692131",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Microsoft’s Copilot can now build apps and automate your job — here’s how it works",
    "link": "https://venturebeat.com/ai/microsofts-copilot-can-now-build-apps-and-automate-your-job-heres-how-it",
    "summary": "<p><a href=\"https://www.microsoft.com/en-us/\"><u>Microsoft</u></a> is launching a significant expansion of its <a href=\"https://copilot.microsoft.com/\"><u>Copilot AI assistant</u></a> on Tuesday, introducing tools that let employees build applications, automate workflows, and create specialized AI agents using only conversational prompts — no coding required.</p><p>The new capabilities, called <a href=\"https://www.microsoft.com/en-us/microsoft-365/blog/2025/10/28/microsoft-365-copilot-now-enables-you-to-build-apps-and-workflows/\"><u>App Builder</u></a> and <a href=\"https://www.microsoft.com/en-us/microsoft-365/blog/2025/10/28/microsoft-365-copilot-now-enables-you-to-build-apps-and-workflows/\"><u>Workflows</u></a>, mark Microsoft&#x27;s most aggressive attempt yet to merge artificial intelligence with software development, enabling the estimated <a href=\"https://www.microsoft.com/investor/reports/ar25/index.html\"><u>100 million Microsoft 365 users</u></a> to create business tools as easily as they currently draft emails or build spreadsheets.</p><p>&quot;We really believe that a main part of an AI-forward employee, not just developers, will be to create agents, workflows and apps,&quot; Charles Lamanna, Microsoft&#x27;s president of business and industry Copilot, said in an interview with VentureBeat. &quot;Part of the job will be to build and create these things.&quot;</p><p>The announcement comes as Microsoft deepens its commitment to AI-powered productivity tools while navigating a <a href=\"https://blogs.microsoft.com/blog/2025/10/28/the-next-chapter-of-the-microsoft-openai-partnership/\"><u>complex partnership with OpenAI</u></a>, the creator of the underlying technology that powers Copilot. On the same day, OpenAI completed its restructuring into a for-profit entity, with Microsoft receiving a <a href=\"https://www.bloomberg.com/news/articles/2025-10-28/microsoft-to-get-27-of-openai-access-to-ai-models-until-2032\"><u>27% ownership stake</u></a> valued at approximately $135 billion.</p><h2><b>How natural language prompts now create fully functional business applications</b></h2><p>The new features transform <a href=\"http://copilot.microsoft.com/\"><u>Copilot</u></a> from a conversational assistant into what Microsoft envisions as a comprehensive development environment accessible to non-technical workers. Users can now describe an application they need — such as a project tracker with dashboards and task assignments — and Copilot will generate a working app complete with a database backend, user interface, and security controls.</p><p>&quot;If you&#x27;re right inside of Copilot, you can now have a conversation to build an application complete with a backing database and a security model,&quot; Lamanna explained. &quot;You can make edit requests and update requests and change requests so you can tune the app to get exactly the experience you want before you share it with other users.&quot;</p><p>The <a href=\"https://www.microsoft.com/en-us/power-platform/products/power-apps\"><u>App Builder</u></a> stores data in Microsoft Lists, the company&#x27;s lightweight database system, and allows users to share finished applications via a simple link—similar to sharing a document. The Workflows agent, meanwhile, automates routine tasks across Microsoft&#x27;s ecosystem of products, including Outlook, Teams, SharePoint, and Planner, by converting natural language descriptions into automated processes.</p><p>A third component, a simplified version of <a href=\"http://microsoft.com/en/microsoft-copilot/microsoft-copilot-studio\"><u>Microsoft&#x27;s Copilot Studio</u></a> agent-building platform, lets users create specialized AI assistants tailored to specific tasks or knowledge domains, drawing from SharePoint documents, meeting transcripts, emails, and external systems.</p><p>All three capabilities are included in the existing $30-per-month <a href=\"https://www.microsoft.com/en-us/microsoft-365-copilot/pricing\"><u>Microsoft 365 Copilot subscription</u></a> at no additional cost — a pricing decision Lamanna characterized as consistent with Microsoft&#x27;s historical approach of bundling significant value into its productivity suite.</p><p>&quot;That&#x27;s what Microsoft always does. We try to do a huge amount of value at a low price,&quot; he said. &quot;If you go look at Office, you think about Excel, Word, PowerPoint, Exchange, all that for like eight bucks a month. That&#x27;s a pretty good deal.&quot;</p><h2><b>Why Microsoft&#x27;s nine-year bet on low-code development is finally paying off</b></h2><p>The new tools represent the culmination of a nine-year effort by Microsoft to democratize software development through its <a href=\"https://www.microsoft.com/en-us/power-platform\"><u>Power Platform</u></a> — a collection of low-code and no-code development tools that has grown to 56 million monthly active users, according to figures the company disclosed in recent earnings reports.</p><p>Lamanna, who has led the Power Platform initiative since its inception, said the integration into Copilot marks a fundamental shift in how these capabilities reach users. Rather than requiring workers to visit a separate website or learn a specialized interface, the development tools now exist within the same conversational window they already use for AI-assisted tasks.</p><p>&quot;One of the big things that we&#x27;re excited about is Copilot — that&#x27;s a tool for literally every office worker,&quot; Lamanna said. &quot;Every office worker, just like they research data, they analyze data, they reason over topics, they also will be creating apps, agents and workflows.&quot;</p><p>The integration offers significant technical advantages, he argued. Because Copilot already indexes a user&#x27;s Microsoft 365 content — emails, documents, meetings, and organizational data — it can incorporate that context into the applications and workflows it builds. If a user asks for &quot;an app for <a href=\"https://blogs.windows.com/windows-insider/2015/03/30/introducing-project-spartan-the-new-browser-built-for-windows-10/\"><u>Project Spartan</u></a>,&quot; Copilot can draw from existing communications to understand what that project entails and suggest relevant features.</p><p>&quot;If you go to those other tools, they have no idea what the heck Project Spartan is,&quot; Lamanna said, referencing competing low-code platforms from companies like Google, Salesforce, and ServiceNow. &quot;But if you do it inside of Copilot and inside of the App Builder, it&#x27;s able to draw from all that information and context.&quot;</p><p><a href=\"https://www.microsoft.com/en-us/\"><u>Microsoft</u></a> claims the apps created through these tools are &quot;full-stack applications&quot; with proper databases secured through the same identity systems used across its enterprise products — distinguishing them from simpler front-end tools offered by competitors. The company also emphasized that its existing governance, security, and data loss prevention policies automatically apply to apps and workflows created through Copilot.</p><h2><b>Where professional developers still matter in an AI-powered workplace</b></h2><p>While <a href=\"https://www.microsoft.com/en-us/\"><u>Microsoft</u></a> positions the new capabilities as accessible to all office workers, Lamanna was careful to delineate where professional developers remain essential. His dividing line centers on whether a system interacts with parties outside the organization.</p><p>&quot;Anything that leaves the boundaries of your company warrants developer involvement,&quot; he said. &quot;If you want to build an agent and put it on your website, you should have developers involved. Or if you want to build an automation which interfaces directly with your customers, or an app or a website which interfaces directly with your customers, you want professionals involved.&quot;</p><p>The reasoning is risk-based: external-facing systems carry greater potential for data breaches, security vulnerabilities, or business errors. &quot;You don&#x27;t want people getting refunds they shouldn&#x27;t,&quot; Lamanna noted.</p><p>For internal use cases — approval workflows, project tracking, team dashboards — Microsoft believes the new tools can handle the majority of needs without IT department involvement. But the company has built &quot;no cliffs,&quot; in Lamanna&#x27;s terminology, allowing users to migrate simple apps to more sophisticated platforms as needs grow.</p><p>Apps created in the conversational <a href=\"https://www.microsoft.com/en-us/power-platform/products/power-apps\"><u>App Builder</u></a> can be opened in <a href=\"https://www.microsoft.com/en-us/power-platform/products/power-apps\"><u>Power Apps</u></a>, Microsoft&#x27;s full development environment, where they can be connected to <a href=\"https://www.microsoft.com/en-us/power-platform/dataverse\"><u>Dataverse</u></a>, the company&#x27;s enterprise database, or extended with custom code. Similarly, simple workflows can graduate to the full <a href=\"https://www.microsoft.com/en/power-platform/products/power-automate?market=af\"><u>Power Automate platform</u></a>, and basic agents can be enhanced in the complete Copilot Studio.</p><p>&quot;We have this mantra called no cliffs,&quot; Lamanna said. &quot;If your app gets too complicated for the App Builder, you can always edit and open it in Power Apps. You can jump over to the richer experience, and if you&#x27;re really sophisticated, you can even go from those experiences into Azure.&quot;</p><p>This architecture addresses a problem that has plagued previous generations of easy-to-use development tools: users who outgrow the simplified environment often must rebuild from scratch on professional platforms. &quot;People really do not like easy-to-use development tools if I have to throw everything away and start over,&quot; Lamanna said.</p><h2><b>What happens when every employee can build apps without IT approval</b></h2><p>The democratization of software development raises questions about governance, maintenance, and organizational complexity — issues Microsoft has worked to address through administrative controls.</p><p>IT administrators can view all applications, workflows, and agents created within their organization through a centralized inventory in the <a href=\"https://www.office.com/\"><u>Microsoft 365</u></a> admin center. They can reassign ownership, disable access at the group level, or &quot;promote&quot; particularly useful employee-created apps to officially supported status.</p><p>&quot;We have a bunch of customers who have this approach where it&#x27;s like, let 1,000 apps bloom, and then the best ones, I go upgrade and make them IT-governed or central,&quot; Lamanna said.</p><p>The system also includes provisions for when employees leave. Apps and workflows remain accessible for 60 days, during which managers can claim ownership — similar to how OneDrive files are handled when someone departs.</p><p>Lamanna argued that most employee-created apps don&#x27;t warrant significant IT oversight. &quot;It&#x27;s just not worth inspecting an app that John, Susie, and Bob use to do their job,&quot; he said. &quot;It should concern itself with the app that ends up being used by 2,000 people, and that will pop up in that dashboard.&quot;</p><p>Still, the proliferation of employee-created applications could create challenges. Users have expressed frustration with Microsoft&#x27;s increasing emphasis on AI features across its products, with some giving the <a href=\"https://www.pcworld.com/article/2954732/users-arent-happy-with-copilot-ai-taking-over-the-microsoft-365-app.html\"><u>Microsoft 365 mobile app one-star ratings</u></a> after a recent update prioritized Copilot over traditional file access.</p><p>The tools also arrive as enterprises grapple with &quot;<a href=\"https://venturebeat.com/ai/mit-report-misunderstood-shadow-ai-economy-booms-while-headlines-cry-failure\"><u>shadow IT</u></a>&quot; — unsanctioned software and systems that employees adopt without official approval. While Microsoft&#x27;s governance controls aim to provide visibility, the ease of creating new applications could accelerate the pace at which these systems multiply.</p><h2><b>The ambitious plan to turn 500 million workers into software builders</b></h2><p>Microsoft&#x27;s ambitions for the technology extend far beyond incremental productivity gains. Lamanna envisions a fundamental transformation of what it means to be an office worker — one where building software becomes as routine as creating spreadsheets.</p><p>&quot;Just like how 20 years ago you put on your resume that you could use pivot tables in Excel, people are going to start saying that they can use App Builder and workflow agents, even if they&#x27;re just in the finance department or the sales department,&quot; he said.</p><p>The numbers he&#x27;s targeting are staggering. With <a href=\"https://www.microsoft.com/en-us/investor/events/fy-2025/earnings-fy-2025-q3\"><u>56 million people already using Power Platform</u></a>, Lamanna believes the integration into Copilot could eventually reach 500 million builders. &quot;Early days still, but I think it&#x27;s certainly encouraging,&quot; he said.</p><p>The features are currently available only to customers in Microsoft&#x27;s <a href=\"https://adoption.microsoft.com/en-us/copilot/frontier-program/\"><u>Frontier Program</u></a> — an early access initiative for Microsoft 365 Copilot subscribers. The company has not disclosed how many organizations participate in the program or when the tools will reach general availability.</p><p>The announcement fits within Microsoft&#x27;s larger strategy of embedding AI capabilities throughout its product portfolio, driven by its partnership with OpenAI. Under the restructured agreement announced Tuesday, Microsoft will have access to OpenAI&#x27;s technology through 2032, including models that achieve artificial general intelligence (AGI) — though such systems do not yet exist. Microsoft has also begun integrating Copilot into its new companion apps for Windows 11, which provide quick access to contacts, files, and calendar information.</p><p>The aggressive integration of AI features across Microsoft&#x27;s ecosystem has drawn mixed reactions. While enterprise customers have shown interest in productivity gains, the rapid pace of change and ubiquity of AI prompts have frustrated some users who prefer traditional workflows.</p><p>For Microsoft, however, the calculation is clear: if even a fraction of its user base begins creating applications and automations, it would represent a massive expansion of the effective software development workforce — and further entrench customers in Microsoft&#x27;s ecosystem. The company is betting that the same natural language interface that made ChatGPT accessible to millions can finally unlock the decades-old promise of empowering everyday workers to build their own tools.</p><p>The App Builder and Workflows agents are available starting today through the <a href=\"https://www.microsoft.com/en-us/microsoft-365-copilot/agents\"><u>Microsoft 365 Copilot Agent Store</u></a> for Frontier Program participants.</p><p>Whether that future arrives depends not just on the technology&#x27;s capabilities, but on a more fundamental question: Do millions of office workers actually want to become part-time software developers? Microsoft is about to find out if the answer is yes — or if some jobs are better left to the professionals.</p><p>\n</p>",
    "published": "Tue, 28 Oct 2025 20:30:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-30T09:00:03.692496",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "OpenAI unveils open-weight AI safety models for developers",
    "link": "https://www.artificialintelligence-news.com/news/openai-unveils-open-weight-ai-safety-models-for-developers/",
    "summary": "<p>OpenAI is putting more safety controls directly into the hands of AI developers with a new research preview of “safeguard” models. The new ‘gpt-oss-safeguard’ family of open-weight models is aimed squarely at customising content classification. The new offering will include two models, gpt-oss-safeguard-120b and a smaller gpt-oss-safeguard-20b. Both are fine-tuned versions of the existing gpt-oss [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/openai-unveils-open-weight-ai-safety-models-for-developers/\">OpenAI unveils open-weight AI safety models for developers</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Wed, 29 Oct 2025 09:31:52 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-30T09:00:06.435500",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-29T09:31:52+00:00",
    "days_old": 1,
    "priority_score": 0.855
  },
  {
    "title": "OpenAI’s bold India play: Free ChatGPT Go access",
    "link": "https://www.artificialintelligence-news.com/news/openai-chatgpt-go-free-india-market-strategy/",
    "summary": "<p>OpenAI just made its biggest bet on India yet. Starting November 4, the company will hand out free year-long access to ChatGPT Go — a move that puts every marketing executive on notice about how aggressively AI companies are fighting for the world&#8217;s fastest-growing digital market. OpenAI will offer its ChatGPT Go plan to users [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/openai-chatgpt-go-free-india-market-strategy/\">OpenAI&#8217;s bold India play: Free ChatGPT Go access</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Tue, 28 Oct 2025 12:01:27 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-30T09:00:06.437667",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.76
  },
  {
    "title": "Finding return on AI investments across industries",
    "link": "https://www.technologyreview.com/2025/10/28/1126693/finding-return-on-ai-investments-across-industries/",
    "summary": "The market is officially three years post ChatGPT and many of the pundit bylines have shifted to using terms like “bubble” to suggest reasons behind generative AI not realizing material returns outside a handful of technology suppliers.&#160; In September, the MIT NANDA report made waves because the soundbite every author and influencer picked up on&#8230;",
    "published": "Tue, 28 Oct 2025 15:00:33 +0000",
    "source_name": "mit_tech_review",
    "source_url": "https://www.technologyreview.com/topic/artificial-intelligence/feed",
    "category": "ai_research",
    "weight": 0.9,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-30T09:00:07.722149",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.54
  },
  {
    "title": "Exclusive: OpenAI lays groundwork for juggernaut IPO at up to $1 trillion valuation",
    "link": "https://www.reddit.com/r/artificial/comments/1ojlkru/exclusive_openai_lays_groundwork_for_juggernaut/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1ojlkru/exclusive_openai_lays_groundwork_for_juggernaut/\"> <img alt=\"Exclusive: OpenAI lays groundwork for juggernaut IPO at up to $1 trillion valuation\" src=\"https://external-preview.redd.it/qYq3ZiZj4ASMQNXSoZAuVyO6u_rTu9xu_A3ZjqaMslc.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=89b1ccca133901a44f201f485ef5d75d21a122dc\" title=\"Exclusive: OpenAI lays groundwork for juggernaut IPO at up to $1 trillion valuation\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/esporx\"> /u/esporx </a> <br /> <span><a href=\"https://www.reuters.com/business/openai-lays-groundwork-juggernaut-ipo-up-1-trillion-valuation-2025-10-29/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1ojlkru/exclusive_openai_lays_groundwork_for_juggernaut/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-30T00:31:05+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-30T09:00:09.190898",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-30T00:31:05+00:00",
    "days_old": 0,
    "priority_score": 0.63
  },
  {
    "title": "OpenAI loses bid to dismiss part of US authors' copyright lawsuit",
    "link": "https://www.reddit.com/r/artificial/comments/1ojiqgw/openai_loses_bid_to_dismiss_part_of_us_authors/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1ojiqgw/openai_loses_bid_to_dismiss_part_of_us_authors/\"> <img alt=\"OpenAI loses bid to dismiss part of US authors' copyright lawsuit\" src=\"https://external-preview.redd.it/thzcHn5mW9Dg_HA6HnovKF9-JUOoayoUYYHnvX2Ozic.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9af4c7f6f30a3eb5ef2c0e46d61af711fc9b671\" title=\"OpenAI loses bid to dismiss part of US authors' copyright lawsuit\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/esporx\"> /u/esporx </a> <br /> <span><a href=\"https://www.reuters.com/legal/government/openai-loses-bid-dismiss-part-us-authors-copyright-lawsuit-2025-10-28/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1ojiqgw/openai_loses_bid_to_dismiss_part_of_us_authors/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-29T22:30:13+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-30T09:00:09.191149",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-29T22:30:13+00:00",
    "days_old": 0,
    "priority_score": 0.63
  }
]