[
  {
    "title": "OpenAI and Amazon ink $38B cloud computing deal",
    "link": "https://techcrunch.com/2025/11/03/openai-and-amazon-ink-38b-cloud-computing-deal/",
    "summary": "OpenAI isn't done securing the AI infrastructure it needs to rapidly scale agentic workloads. The ChatGPT-maker on Monday said it has reached a deal with Amazon to buy $38 billion in cloud computing services over the next seven years.",
    "published": "Mon, 03 Nov 2025 15:21:51 +0000",
    "source_name": "tech_crunch",
    "source_url": "https://techcrunch.com/feed/",
    "category": "tech_news",
    "weight": 0.9,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-03T09:00:03.464104",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-03T15:21:51+00:00",
    "days_old": 0,
    "priority_score": 1.35
  },
  {
    "title": "The beginning of the end of the transformer era? Neuro-symbolic AI startup AUI announces new funding at $750M valuation",
    "link": "https://venturebeat.com/ai/the-beginning-of-the-end-of-the-transformer-era-neuro-symbolic-ai-startup",
    "summary": "<p>The buzzed-about but still stealthy New York City startup <a href=\"https://www.aui.io/\">Augmented Intelligence Inc (AUI)</a>, which seeks to go beyond the popular &quot;transformer&quot; architecture used by most of today&#x27;s LLMs such as ChatGPT and Gemini, has<b> raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million</b>, VentureBeat can exclusively reveal.</p><p>The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.</p><p>AUI relies on a fusion of the transformer tech and a newer technology called &quot;neuro-symbolic AI,&quot; described in greater detail below. </p><p>&quot;We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,&quot; said <b>Ohad Elhelo</b>, <b>AUI co-founder and CEO</b> in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside <b>co-founder and Chief Product Officer Ori Cohen.</b></p><p>The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the <a href=\"https://cloud.google.com/blog/topics/partners/google-cloud-partners-with-aui/\">company’s announced go-to-market partnership with Google</a> in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.</p><p>According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.</p><p>AUI is the <a href=\"https://venturebeat.com/ai/has-this-stealth-startup-finally-cracked-the-code-on-enterprise-ai-agent\">company behind Apollo-1</a>, a new foundation model built for task-oriented dialog, which it describes as the &quot;economic half&quot; of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. </p><p>The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.</p><p>Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”</p><h3><b>A Distinctive Neuro-Symbolic Architecture</b></h3><p>Apollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper &quot;Attention Is All You Need&quot; — AUI&#x27;s system integrates two layers:</p><ul><li><p>Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.</p></li><li><p>A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.</p></li></ul><p>This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.</p><p>Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”</p><p>However, enterprises that have already built systems built around transformer LLMs needn&#x27;t worry. AUI wants to make adopting its new technology just as easy. </p><p>&quot;Apollo-1 deploys like any modern foundation model,&quot; Elhelo told VentureBeat in a text last night. &quot;It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.&quot;</p><h3><b>Generalization and Domain Flexibility</b></h3><p>Apollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.</p><p>Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.</p><p>Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. </p><p>For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.</p><p>As Elhelo explained to VentureBeat, LLMs are &quot;not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”</p><h3><b>Availability and Developer Access</b></h3><p>Apollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a <a href=\"https://www.theinformation.com/articles/startup-teaching-ai-agents-shop\">previous report by <i>The Information</i></a><i>, </i>which broke the initial news on the startup.</p><p>Enterprises can integrate with Apollo-1 either via:</p><ul><li><p>A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; or</p></li><li><p>A standard API, using OpenAI-compatible formats.</p></li></ul><p>The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.</p><h3><b>Enterprise Fit: When Reliability Beats Fluency</b></h3><p>While LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. </p><p>Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.</p><p>Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”</p>",
    "published": "Mon, 03 Nov 2025 14:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-11-03T09:00:04.916244",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Moving past speculation: How deterministic CPUs deliver predictable AI performance",
    "link": "https://venturebeat.com/ai/moving-past-speculation-how-deterministic-cpus-deliver-predictable-ai",
    "summary": "<p>For more than three decades, modern CPUs have relied on speculative execution to keep pipelines full. When it emerged in the 1990s, speculation was hailed as a breakthrough — just as pipelining and superscalar execution had been in earlier decades. Each marked a generational leap in microarchitecture. By predicting the outcomes of branches and memory loads, processors could avoid stalls and keep execution units busy. </p><p>But this architectural shift came at a cost: Wasted energy when predictions failed, increased complexity and vulnerabilities such as Spectre and Meltdown. These challenges set the stage for an alternative: A deterministic, time-based execution model. As David Patterson <a href=\"https://venturebeat.com/ai/meta-researchers-open-the-llm-black-box-to-repair-flawed-ai-reasoning\">observed in 1980</a>, “A RISC potentially gains in speed merely from a simpler design.” Patterson’s principle of simplicity underpins a new alternative to speculation: A deterministic, time-based execution model.&quot;</p><p>For the first time since speculative execution became the dominant paradigm, a fundamentally new approach has been invented. This breakthrough is embodied in a series of six recently issued U.S. patents, sailing through the U.S. Patent and Trademark Office (USPTO). Together, they introduce a <a href=\"https://patents.google.com/patent/US11829187B2/en\">radically different</a> instruction execution model. Departing sharply from conventional speculative techniques, this novel deterministic framework replaces guesswork with a time-based, latency-tolerant mechanism. Each instruction is assigned a precise execution slot within the pipeline, resulting in a rigorously ordered and predictable flow of execution. This reimagined model redefines how modern processors can handle latency and concurrency with greater efficiency and reliability. </p><p>A simple time counter is used to deterministically set the exact time of when instructions should be executed in the future. Each instruction is dispatched to an execution queue with a preset execution time based on resolving its data dependencies and availability of resources — read buses, execution units and the write bus to the register file. Each instruction remains queued until its scheduled execution slot arrives. This new deterministic approach may represent the first major architectural challenge to speculation since it <a href=\"https://patents.google.com/patent/US11829187B2/en\">became the standard</a>.</p><p>The architecture extends naturally into matrix computation, with a RISC-V instruction set proposal under community review. Configurable general matrix multiply (GEMM) units, ranging from 8×8 to 64×64, can operate using either register-based or direct-memory acceess (DMA)-fed operands. This flexibility supports a wide range of AI and high-performance computing (HPC) workloads. Early analysis suggests scalability that rivals Google’s TPU cores, while maintaining significantly lower cost and power requirements. </p><p>Rather than a direct comparison with general-purpose CPUs, the more accurate reference point is vector and matrix engines: Traditional CPUs still depend on speculation and branch prediction, whereas this design applies deterministic scheduling directly to GEMM and vector units. This efficiency stems not only from the configurable GEMM blocks but also from the time-based execution model, where instructions are decoded and assigned precise execution slots based on operand readiness and resource availability. </p><p>Execution is never a random or heuristic choice among many candidates, but a predictable, pre-planned flow that keeps compute resources continuously busy. Planned matrix benchmarks will provide direct comparisons with TPU GEMM implementations, highlighting the ability to deliver datacenter-class performance without datacenter-class overhead.</p><p>Critics may argue that static scheduling introduces latency into instruction execution. In reality, the latency already exists — waiting on data dependencies or memory fetches. Conventional CPUs attempt to hide it with speculation, but when predictions fail, the resulting pipeline flush introduces delay and wastes power. </p><p>The time-counter approach acknowledges this latency and fills it deterministically with useful work, avoiding rollbacks. As the first patent notes, instructions retain out-of-order efficiency: “A <a href=\"https://patents.google.com/patent/US11829187B2/en\">microprocessor</a> with a time counter for statically dispatching instructions enables execution based on predicted timing rather than speculative issue and recovery,&quot; with preset execution times but without the overhead of register renaming or speculative comparators.</p><h2>Why speculation stalled</h2><p>Speculative execution boosts performance by predicting outcomes before they’re known — executing instructions ahead of time and discarding them if the guess was wrong. While this approach can accelerate workloads, it also introduces unpredictability and power inefficiency. Mispredictions inject “No Ops” into the pipeline, stalling progress and wasting energy on work that never completes. </p><p>These issues are magnified in modern <a href=\"https://venturebeat.com/ai/large-reasoning-models-almost-certainly-can-think\">AI and machine learning (ML)</a> workloads, where vector and matrix operations dominate and memory access patterns are irregular. Long fetches, non-cacheable loads and misaligned vectors frequently trigger pipeline flushes in speculative architectures.</p><p>The result is performance cliffs that vary wildly across datasets and problem sizes, making consistent tuning nearly impossible. Worse still, speculative side effects have exposed vulnerabilities that led to high-profile security exploits. As data intensity grows and memory systems strain, speculation struggles to keep pace — undermining its original promise of seamless acceleration.</p><h2>Time-based execution and deterministic scheduling</h2><p>At the core of this invention is a <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">vector coprocessor</a> with a time counter for statically dispatching instructions. Rather than relying on speculation, instructions are issued only when data dependencies and latency windows are fully known. This eliminates guesswork and costly pipeline flushes while preserving the throughput advantages of out-of-order execution. Architectures built on this patented framework feature deep pipelines — typically spanning 12 stages — combined with wide front ends supporting up to 8-way decode and large reorder buffers exceeding 250 entries</p><p>As illustrated in Figure 1, the architecture mirrors a conventional RISC-V processor at the top level, with instruction fetch and decode stages feeding into execution units. The innovation emerges in the integration of a <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">time counter and register scoreboard</a>, strategically positioned between fetch/decode and the vector execution units. Instead of relying on speculative comparators or register renaming, they utilize a Register Scoreboard and <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">Time Resource Matrix</a> (TRM) to deterministically schedule instructions based on operand readiness and resource availability.  </p><p><i>Figure 1: High-level block diagram of deterministic processor. A time counter and scoreboard sit between fetch/decode and vector execution units, ensuring instructions issue only when operands are ready.</i></p><p>A typical program running on the deterministic processor begins much like it does on any conventional RISC-V system: Instructions are fetched from memory and decoded to determine whether they are scalar, vector, matrix or custom extensions. The difference emerges at the point of dispatch. Instead of issuing instructions speculatively, the processor employs a cycle-accurate time counter, working with a register scoreboard, to decide exactly when each instruction can be executed. This mechanism provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.</p><p>In conjunction with a register scoreboard, the time-resource matrix associates instructions with execution cycles, allowing the processor to plan dispatch deterministically across available resources. The scoreboard tracks operand readiness and hazard information, enabling scheduling without register renaming or speculative comparators. By monitoring dependencies such as read-after-write (RAW) and write-after-read, it ensures hazards are resolved without costly pipeline flushes. As noted <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">in the patent</a>, “in a multi-threaded microprocessor, the time counter and scoreboard permit rescheduling around cache misses, branch flushes, and RAW hazards without speculative rollback.”</p><p>Once operands are ready, the instruction is dispatched to the appropriate execution unit. Scalar operations use standard artithmetic logic units (ALUs), while vector and matrix instructions execute in wide execution units connected to a <a href=\"https://venturebeat.com/ai/abstract-or-die-why-ai-enterprises-cant-afford-rigid-vector-stacks\">large vector</a> register file. Because instructions launch only when conditions are safe, these units stay highly utilized without the wasted work or recovery cycles caused by mis-predicted speculation. </p><p>The key enabler of this approach is a simple time counter that orchestrates execution according to data readiness and resource availability, ensuring instructions advance only when operands are ready and resources available. The same principle applies to memory operations: The interface predicts latency windows for loads and stores, allowing the processor to fill those slots with independent instructions and keep execution flowing.</p><h2>Programming model differences</h2><p>From the programmer’s perspective, the flow remains familiar — RISC-V code compiles and executes in the usual way. The crucial difference lies in the execution contract: Rather than relying on dynamic speculation to hide latency, the processor guarantees predictable dispatch and completion times. This eliminates the performance cliffs and wasted energy of speculation while still providing the throughput benefits of out-of-order execution. </p><p>This perspective underscores how deterministic execution preserves the familiar RISC-V programming model while eliminating the unpredictability and wasted effort of speculation. As <a href=\"https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/about/interview.html\">John Hennessy put it</a>: &quot;It’s stupid to do work in run time that you can do in compile time”— a remark reflecting the foundations of RISC and its forward-looking design philosophy.</p><p>The RISC-V ISA provides opcodes for custom and extension instructions, including floating-point, DSP, and vector operations. The result is a processor that executes instructions deterministically while retaining the benefits of out-of-order performance. By eliminating speculation, the design simplifies hardware, reduces power consumption and avoids pipeline flushes. </p><p>These efficiency gains grow even more significant in vector and matrix operations, where wide execution units require consistent utilization to reach peak performance. Vector extensions require wide register files and large execution units, which in speculative processors necessitate expensive register renaming to recover from branch mispredictions. In the deterministic design, vector instructions are executed only after commit, eliminating the need for renaming.</p><p>Each instruction is scheduled against a <a href=\"https://patents.google.com/patent/US12001848B2/en?oq=U.S.+Patent+No.+12%2c001%2c848\">cycle-accurate time counter</a>: “The time counter provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.” The vector register scoreboard resolves data dependency before issuing instructions to execution pipeline.  Instructions are dispatched in a known order at the correct cycle, making execution both predictable and efficient.</p><p>Vector execution units (integer and floating point) connect directly to a large vector register file. Because instructions are never flushed, there is no renaming overhead. The scoreboard ensures safe access, while the time counter aligns execution with memory readiness. A dedicated memory block predicts the return cycle of loads. Instead of stalling or speculating, the processor schedules independent instructions into latency slots, keeping <a href=\"https://patents.google.com/patent/US12001848B2/en?oq=U.S.+Patent+No.+12%2c001%2c848\">execution units busy</a>. “A vector coprocessor with a time counter for statically dispatching instructions ensures high utilization of wide execution units while avoiding misprediction penalties.”</p><p>In today’s CPUs, compilers and programmers write code assuming the hardware will dynamically reorder instructions and speculatively execute branches. The hardware handles hazards with register renaming, branch prediction and recovery mechanisms. Programmers benefit from performance, but at the cost of unpredictability and power consumption.</p><p>In the deterministic time-based architecture, instructions are dispatched only when the time counter indicates their operands will be ready. This means the compiler (or runtime system) doesn’t need to insert guard code for misprediction recovery. Instead, compiler scheduling becomes simpler, as instructions are guaranteed to issue at the correct cycle without rollbacks. For programmers, the ISA remains RISC-V compatible, but deterministic extensions reduce reliance on speculative safety nets.</p><h2>Application in AI and ML</h2><p>In <a href=\"https://venturebeat.com/ai/under-the-hood-of-ai-agents-a-technical-guide-to-the-next-frontier-of-gen-ai\">AI/ML kernels</a>, vector loads and matrix operations often dominate runtime. On a speculative CPU, misaligned or non-cacheable loads can trigger stalls or flushes, starving wide vector and matrix units and wasting energy on discarded work. A deterministic design instead issues these operations with cycle-accurate timing, ensuring high utilization and steady throughput. For programmers, this means fewer performance cliffs and more predictable scaling across problem sizes. And because the patents extend the RISC-V ISA rather than replace it, deterministic processors remain fully compatible with the RVA23 profile and mainstream toolchains such as GCC, LLVM, FreeRTOS, and Zephyr.</p><p>In practice, the deterministic model doesn’t change how code is written — it remains RISC-V assembly or high-level languages compiled to RISC-V instructions. What changes is the execution contract: Rather than relying on speculative guesswork, programmers can expect predictable latency behavior and higher efficiency without tuning code around microarchitectural quirks.</p><p>The industry is at an inflection point. AI/ML workloads are dominated by vector and matrix math, where GPUs and TPUs excel — but only by consuming massive power and adding architectural complexity. In contrast, general-purpose CPUs, still tied to speculative execution models, lag behind.</p><p>A deterministic processor delivers predictable performance across a wide range of workloads, ensuring consistent behavior regardless of task complexity. Eliminating speculative execution enhances energy efficiency and avoids unnecessary computational overhead. Furthermore, deterministic design scales naturally to vector and matrix operations, making it especially well-suited for AI workloads that rely on high-throughput parallelism. This new deterministic approach may represent the next such leap: The first major architectural challenge to speculation since speculation itself became the standard.</p><p>Will deterministic CPUs replace speculation in mainstream computing? That remains to be seen. But with issued patents, proven novelty and growing pressure from AI workloads, the timing is right for a paradigm shift. Taken together, these advances signal deterministic execution as the next architectural leap — redefining performance and efficiency just as speculation once did.</p><p>Speculation marked the last revolution in CPU design; determinism may well represent the next.</p><p><i>Thang Tran is the founder and CTO of Simplex Micro.</i></p><p><i>Read more from our </i><a href=\"https://venturebeat.com/datadecisionmakers\"><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href=\"https://r39crwmcu9m.typeform.com/to/NEzWFTji\"><i>guidelines here</i></a><i>. </i></p>",
    "published": "Sun, 02 Nov 2025 05:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-03T09:00:04.916763",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "Large reasoning models almost certainly can think",
    "link": "https://venturebeat.com/ai/large-reasoning-models-almost-certainly-can-think",
    "summary": "<p>Recently, there has been a lot of hullabaloo about the idea that large reasoning models (LRM) are unable to think. This is mostly due to a research article published by Apple, &quot;<a href=\"https://machinelearning.apple.com/research/illusion-of-thinking\"><u>The Illusion of Thinking</u></a>&quot; Apple argues that LRMs must not be able to think; instead, they just perform pattern-matching. The evidence they provided is that LRMs with <a href=\"https://venturebeat.com/ai/dont-believe-reasoning-models-chains-of-thought-says-anthropic\">chain-of-thought (CoT</a>) reasoning are unable to carry on the calculation using a predefined algorithm as the problem grows.</p><p>This is a fundamentally flawed argument. If you ask a human who already knows the algorithm for solving the Tower-of-Hanoi problem to solve a Tower-of-Hanoi problem with twenty discs, for instance, he or she would almost certainly fail to do so. By that logic, we must conclude that humans cannot think either. However, this argument only points to the idea that there is no evidence that LRMs cannot think. This alone certainly does not mean that LRMs can think — just that we cannot be sure they don’t.</p><p>In this article, I will make a bolder claim: LRMs almost certainly can think. I say ‘almost’ because there is always a chance that further research would surprise us. But I think my argument is pretty conclusive.</p><h2>What is thinking?</h2><p>Before we try to understand if LRMs can think, we need to define what we mean by thinking. But first, we have to make sure that humans can think per the definition. We will only consider thinking in relation to problem solving, which is the matter of contention.</p><p><b>1. Problem representation (frontal and parietal lobes)</b></p><p>When you think about a problem, the process engages your prefrontal cortex. This region is responsible for working memory, attention and executive functions — capacities that let you hold the problem in mind, break it into sub-components and set goals. Your parietal cortex helps encode symbolic structure for math or puzzle problems.</p><p><b>2. Mental simulation (morking Memory and inner speech)</b></p><p>This has two components: One is an auditory loop that lets you talk to yourself — very similar to <a href=\"https://venturebeat.com/ai/llms-generate-fluent-nonsense-when-reasoning-outside-their-training-zone\">CoT generation</a>. The other is visual imagery, which allows you to manipulate objects visually. Geometry was so important for navigating the world that we developed specialized capabilities for it. The auditory part is linked to Broca’s area and the auditory cortex, both reused from language centers. The visual cortex and parietal areas primarily control the visual component.</p><p><b>3. Pattern matching and retrieval (Hippocampus and Temporal Lobes)</b></p><p>These actions depend on past experiences and stored knowledge from long-term memory:</p><ul><li><p>The hippocampus helps retrieve related memories and facts.</p></li><li><p>The temporal Lobe brings in semantic knowledge — meanings, rules, categories.</p></li></ul><p>This is similar to how neural networks depend on their training to process the task.</p><p><b>4. Monitoring and evaluation (Anterior Cingulate Cortex)</b></p><p>Our anterior cingulate cortex (ACC) monitors for errors, conflicts or impasses — it’s where you notice contradictions or dead ends. This process is essentially based on pattern matching from prior experience.</p><p><b>5. Insight or reframing (default mode network and right hemisphere)</b></p><p>When you&#x27;re stuck, your brain might shift into <b>default mode </b>— a more relaxed, internally-directed network. This is when you step back, let go of the current thread and sometimes ‘suddenly’ see a new angle (the classic “aha!” moment).</p><p>This is similar to how <b>DeepSeek-R1</b> was trained for CoT reasoning without having CoT examples in its training data. Remember, the brain continuously learns as it processes data and solves problems.</p><p>In contrast, <b>LRMs</b> aren’t allowed to change based on real-world feedback during prediction or generation. But with DeepSeek-R1’s CoT training, learning <i>did</i> happen as it attempted to solve the problems — essentially updating while reasoning.</p><h2>Similarities betweem CoT reasoning and biological thinking</h2><p>LRM does not have all of the faculties mentioned above. For example, an LRM is very unlikely to do too much visual reasoning in its circuit, although a little may happen. But it certainly does not generate intermediate images in the CoT generation.</p><p>Most humans can make spatial models in their heads to solve problems. Does this mean we can conclude that LRMs cannot think? I would disagree. Some humans also find it difficult to form spatial models of the concepts they think about. This condition is called <i>aphantasia</i>. People with this condition can think just fine. In fact, they go about life as if they don’t lack any ability at all. Many of them are actually great at symbolic reasoning and quite good at math — often enough to compensate for their lack of visual reasoning. We might expect our neural network models also to be able to circumvent this limitation.</p><p>If we take a more abstract view of the human thought process described earlier, we can see mainly the following things involved:</p><p>1.  Pattern-matching is used for recalling learned experience, problem representation and monitoring and evaluating chains of thought.</p><p>2.  Working memory is to store all the intermediate steps.</p><p>3.  Backtracking search concludes that the CoT is not going anywhere and backtracks to some reasonable point.</p><p>Pattern-matching in an LRM <a href=\"https://venturebeat.com/ai/abstract-or-die-why-ai-enterprises-cant-afford-rigid-vector-stacks\">comes from its training</a>. The whole point of training is to learn both knowledge of the world and the patterns to process that knowledge effectively. Since an LRM is a layered network, the entire working memory needs to fit within one layer. The weights store the knowledge of the world and the patterns to follow, while processing happens between layers using the learned patterns stored as model parameters.</p><p>Note that even in CoT, the entire text — including the input, CoT and part of the output already generated — must fit into each layer. Working memory is just one layer (in the case of the attention mechanism, this includes the KV-cache).</p><p>CoT is, in fact, very similar to what we do when we are talking to ourselves (which is almost always). We nearly always verbalize our thoughts, and so does a CoT reasoner.</p><p>There is also good evidence that CoT reasoner can take backtracking steps when a certain line of reasoning seems futile. In fact, this is what the Apple researchers saw when they tried to ask the LRMs to solve bigger instances of simple puzzles. The LRMs correctly recognized that trying to solve the puzzles directly would not fit in their working memory, so they tried to figure out better shortcuts, just like a human would do. This is even more evidence that LRMs are thinkers, not just blind followers of predefined patterns.</p><h2>But why would a next-token-predictor learn to think?</h2><p><a href=\"https://www.talentica.com/blogs/why-neural-networks-can-learn-anything/\"><u>Neural networks of sufficient size can learn any computation, including thinking</u></a>. But a next-word-prediction system can also learn to think. Let me elaborate. </p><p>A general idea is LRMs cannot think because, at the end of the day, they are just predicting the next token; it is only a &#x27;glorified auto-complete.&#x27; This view is fundamentally incorrect — not that it is an &#x27;auto-complete,&#x27; but that an &#x27;auto-complete&#x27; does not have to think. In fact, next word prediction is far from a limited representation of thought. On the contrary, it is the most general form of knowledge representation that anyone can hope for. Let me explain.</p><p>Whenever we want to represent some knowledge, we need a language or a system of symbolism to do so. Different formal languages exist that are very precise in terms of what they can express. However, such languages are fundamentally limited in the kinds of knowledge they can represent.</p><p>For example, first-order predicate logic cannot represent properties of all predicates that satisfy a certain property, because it doesn&#x27;t allow predicates over predicates.</p><p>Of course, there are higher-order predicate calculi that can represent predicates on predicates to arbitrary depths. But even they cannot express ideas that lack precision or are abstract in nature.</p><p>Natural language, however, is complete in expressive power — you can describe any concept in any level of detail or abstraction. In fact, you can even describe concepts <i>about</i> natural language using natural language itself. That makes it a strong candidate for knowledge representation.</p><p>The challenge, of course, is that this expressive richness makes it harder to process the information encoded in natural language. But we don’t necessarily need to understand how to do it manually — we can simply program the machine using data, through a process called training.</p><p>A next-token prediction machine essentially computes a probability distribution over the next token, given a context of preceding tokens. Any machine that aims to compute this probability accurately must, in some form, represent world knowledge.</p><p>A simple example: Consider the incomplete sentence, &quot;The highest mountain peak in the world is Mount ...&quot; — to predict the next word as Everest, the model must have this knowledge stored somewhere. If the task requires the model to compute the answer or solve a puzzle, the next-token predictor needs to output CoT tokens to carry the logic forward.</p><p>This implies that, even though it’s predicting one token at a time, the model must internally represent at least the next few tokens in its working memory — enough to ensure it stays on the logical path.</p><p>If you think about it, humans also predict the next token — whether during speech or when thinking using the inner voice. A perfect auto-complete system that always outputs the right tokens and produces correct answers would have to be omniscient. Of course, we’ll never reach that point — because not every answer is computable.</p><p>However, a parameterized model that can represent knowledge by tuning its parameters, and that can learn through data and reinforcement, can certainly learn to think.</p><h2>Does it produce the effects of thinking?</h2><p>At the end of the day, the ultimate test of thought is a system’s ability to solve problems that require thinking. If a system can answer previously unseen questions that demand some level of reasoning, it must have learned to think — or at least to reason — its way to the answer.</p><p>We know that proprietary LRMs perform very well on certain reasoning benchmarks. However, since there&#x27;s a possibility that some of these models were fine-tuned on benchmark test sets through a backdoor, we’ll focus only on <b>open-source models</b> for fairness and transparency.</p><p>We evaluate them using the following benchmarks:</p><p>As one can see, in some benchmarks, LRMs are able to solve a significant number of logic-based questions. While it’s true that they still lag behind human performance in many cases, it’s important to note that the human baseline often comes from individuals trained specifically on those benchmarks. In fact, in certain cases, LRMs outperform the average untrained human.</p><h2>Conclusion</h2><p>Based on the benchmark results, the striking similarity between CoT reasoning and biological reasoning, and the theoretical understanding that any system with sufficient representational capacity, enough training data, and adequate computational power can perform any computable task — LRMs meet those criteria to a considerable extent.</p><p>It is therefore reasonable to conclude that LRMs almost certainly possess the ability to think.</p><p><i>Debasish Ray Chawdhuri is a senior principal engineer at</i><a href=\"https://www.talentica.com/\"><i> </i><i><u>Talentica Software</u></i></a><i> and a Ph.D. candidate in Cryptography at IIT Bombay. </i></p><p><i>Read more from our </i><a href=\"https://venturebeat.com/datadecisionmakers\"><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href=\"https://r39crwmcu9m.typeform.com/to/NEzWFTji\"><i>guidelines here</i></a><i>. </i></p>",
    "published": "Sat, 01 Nov 2025 05:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-03T09:00:04.917177",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "CrowdStrike & NVIDIA’s open source AI gives enterprises the edge against machine-speed attacks",
    "link": "https://venturebeat.com/security/crowdstrike-nvidia-open-source-ai-soc-machine-speed-attacks",
    "summary": "<p>Every SOC leader knows the feeling: drowning in alerts, blind to the real threat, stuck playing defense in a war waged at the speed of AI. </p><p>Now <a href=\"https://www.crowdstrike.com/en-us/platform/charlotte-ai/\">CrowdStrike</a> and <a href=\"https://www.nvidia.com/en-us/\">NVIDIA</a> are flipping the script. Armed with autonomous agents powered by Charlotte AI and NVIDIA Nemotron models, security teams aren&#x27;t just reacting; they&#x27;re striking back at attackers before their next move. Welcome to cybersecurity&#x27;s new arms race. Combining open source&#x27;s many strengths with agentic AI will shift the balance of power against adversarial AI. </p><p>CrowdStrike and NVIDIA&#x27;s agentic ecosystem combines <a href=\"https://www.crowdstrike.com/en-us/platform/charlotte-ai/\">Charlotte AI AgentWorks,</a> <a href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/nemotron/\">NVIDIA Nemotron</a> open models, <a href=\"https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/index.html\">NVIDIA NeMo Data Designer</a> synthetic data, <a href=\"https://developer.nvidia.com/nemo-agent-toolkit\">NVIDIA Nemo Agent Toolkit</a>, and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/\">NVIDIA NIM microservices</a>. </p><p>&quot;This collaboration redefines security operations by enabling analysts to build and deploy specialized <a href=\"https://www.nvidia.com/en-us/glossary/ai-agents/\">AI agents</a> at scale, leveraging trusted, enterprise-grade security with Nemotron models,&quot; <a href=\"https://blogs.nvidia.com/blog/nemotron-open-source-ai/\">writes </a>Bryan Catanzaro, vice president, Applied Deep Learning Research at NVIDIA. </p><p>The partnership is designed to enable autonomous agents to learn quickly, reducing risks, threats, and false positives. Achieving that takes a heavy load off SOC leaders and their teams, who fight data fatigue nearly every day due to inaccurate data.</p><p>The announcement at GTC Washington, D.C., signals the arrival of machine-speed defense that can finally match machine-speed attacks. </p><h2><b>Transforming elite analyst expertise into datasets at machine scale</b></h2><p>The partnership is differentiated by how the AI agents are designed to continually aggregate telemetry data, including insights from <a href=\"https://www.crowdstrike.com/en-us/services/falcon-complete-next-gen-mdr/\">CrowdStrike Falcon Complete Managed Detection and Response</a> analysts. </p><p>&quot;What we&#x27;re able to do is take the intelligence, take the data, take the experience of our Falcon Complete analysts, and turn these experts into datasets. Turn the datasets into AI models, and then be able to create agents based on, really, the whole composition and experience that we&#x27;ve built up within the company so that our customers can benefit at scale from these agents always,&quot; said Daniel Bernard, CrowdStrike&#x27;s Chief Business Officer, during a recent briefing. </p><p>Capitalizing on the strengths of the NVIDIA Nemotron open models, organizations will be able to have their autonomous agents continually learn by training on the datasets from Falcon Complete, the world&#x27;s largest MDR service handling millions of triage decisions monthly. </p><p>CrowdStrike has previous experience in AI detection triage to the point of launching a service that scales this capability across its customer base. <a href=\"https://www.crowdstrike.com/en-us/platform/charlotte-ai/\">Charlotte AI Detection Triage,</a> designed to integrate into existing <a href=\"https://venturebeat.com/security/the-ai-paradox-how-tomorrows-cutting-edge-tools-can-become-dangerous-cyber-threats-and-how-to-prepare/\">security workflows</a> and continuously adapt to evolving threats, automates alert assessment with over 98% accuracy and cuts manual triage by more than 40 hours per week.</p><p>Elia Zaitsev, CrowdStrike&#x27;s chief technology officer, in explaining how Charlotte AI Detection Triage is able to deliver that level of performance, <a href=\"https://venturebeat.com/security/crowdstrikes-ai-slashes-soc-workloads-over-40-hours-a-week\"><b>told VentureBeat</b></a>: &quot;We wouldn&#x27;t have achieved this without the support of our Falcon Complete team. They perform triage within their workflow, manually addressing millions of detections. The high-quality, human-annotated dataset they provide is what enabled us to reach an accuracy of over 98%.&quot;</p><p>Lessons learned with Charlotte AI Detection Triage directly apply to the NVIDIA partnership, further increasing the value it has the potential to deliver to SOCs who need help dealing with the deluge of alerts. </p><h2><b>Open source is table stakes for this partnership to work </b></h2><p>NVIDIA&#x27;s Nemotron open models address what many security leaders identify as the most critical barrier to AI adoption in regulated environments, which is the lack of clarity regarding how the model works, what its weights are, and how secure it is. </p><p>Justin Boitano, Vice President, Enterprise and Edge Computing at NVIDIA, speaking for NVIDIA during a recent press briefing, explained: &quot;Open models are where people start in trying to build their own specialized domain knowledge. You want to own the IP ultimately. Not everybody wants to export their data, and then sort of import or pay for the intelligence that they consume. A lot of sovereign countries, many enterprises in regulated industries want to maintain all that data privacy and security.&quot; </p><p>John Morello, CTO and co-founder of Gutsy (now <a href=\"https://www.minimus.io/\">Minimus</a>), <a href=\"https://venturebeat.com/security/how-open-source-llms-enable-security-teams-to-stay-ahead-of-evolving-threats\"><b>told VentureBeat</b></a> that &quot;the open-source nature of <a href=\"https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/\">Google&#x27;s BERT</a> open-source language model allows Gutsy to customize and train their model for specific security use cases while maintaining privacy and efficiency.&quot; Morello emphasized that practitioners cite &quot;more transparency and better assurances of data privacy, along with great availability of expertise and more integration options across their architectures, as key reasons for going with open source.&quot;</p><h2><b>Keeping adversarial AI&#x27;s balance of power in check </b></h2><p>Cisco&#x27;s DJ Sampath, senior vice president of Cisco&#x27;s AI software and platform group, articulated the industry-wide imperative for open-source security models <a href=\"https://venturebeat.com/security/cisco-warns-enterprises-without-tapping-machine-data-your-ai-strategy-is\"><b>during a recent interview with VentureBeat</b></a>: &quot;The reality is that attackers have access to open-source models too. The goal is to empower as many defenders as possible with robust models to strengthen security.&quot; </p><p>Sampath explained that when Cisco released Foundation-Sec-8B, their open-source security model, at RSAC 2025, it was driven by a sense of responsibility: &quot;Funding for open-source projects has stalled, and there is a growing need for sustainable funding sources within the community. It is a corporate responsibility to provide these models while enabling communities to engage with AI from a defensive standpoint.&quot;</p><p>The commitment to transparency extends to the most sensitive aspects of AI development. When concerns emerged about DeepSeek R1&#x27;s training data and potential compromise, NVIDIA responded decisively. </p><p>As Boitano explained to VentureBeat, &quot;Government agencies were super concerned. They wanted the reasoning capabilities of DeepSeek, but they were a little concerned with, obviously, what might be trained into the DeepSeek model, which is what actually inspired us to completely open source everything in Nemotron models, including reasoning datasets.&quot;</p><p>For practitioners managing open-source security at scale, this transparency is core to their companies. Itamar Sher, CEO of <a href=\"https://www.seal.security/\">Seal Security</a>, emphasized <a href=\"https://venturebeat.com/security/how-open-source-llms-enable-security-teams-to-stay-ahead-of-evolving-threats\"><b>to VentureBeat</b></a> that &quot;open-source models offer transparency,&quot; though he noted that &quot;managing their cycles and compliance remains a significant concern.&quot; Sher&#x27;s company uses generative AI to automate vulnerability remediation in open-source software, and as a recognized CVE Naming Authority (CNA), Seal can identify, document, and assign vulnerabilities, enhancing security across the ecosystem.</p><h2><b>A key partnership goal: bringing intelligence to the Edge</b></h2><p>&quot;Bringing the intelligence closer to where data is and decisions are made is just going to be a big advancement for security operations teams around the industry,&quot; Boitano emphasized. This edge deployment capability is especially critical for government agencies with fragmented and often legacy IT environments. </p><p>VentureBeat asked Boitano how the initial discussions went with government agencies briefed on the partnership and its design goals before work began. &quot;The feeling across agencies that we&#x27;ve talked to is they always feel like, unfortunately, they&#x27;re behind the curve on these technology adoption,&quot; Boitano explained. &quot;The response was, anything you guys can do to help us secure the endpoints. It was a tedious and long process to get open models onto these, you know, higher side networks.&quot;</p><p>NVIDIA and CrowdStrike have done the foundational work, including STIG hardening, FIPS encryption, air-gap compatibility, and removing the barriers that delayed open-model adoption on higher-side networks. The <a href=\"https://blogs.nvidia.com/blog/us-technology-leaders-ai-factory-design-government/\">NVIDIA AI Factory for Government reference design</a> provides comprehensive guidance for deploying AI agents in federal and high-assurance organizations while meeting the strictest security requirements.</p><p>As Boitano explained, the urgency is existential: &quot;Having AI defense that&#x27;s running in your estate that can search for and detect these anomalies, and then alert and respond much faster, is just the natural consequence. It&#x27;s the only way to protect against the speed of AI at this point.&quot;</p>",
    "published": "Sat, 01 Nov 2025 01:10:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-03T09:00:04.917532",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Inside Celosphere 2025: Why there’s no ‘enterprise AI’ without process intelligence",
    "link": "https://venturebeat.com/ai/inside-celosphere-2025-why-theres-no-enterprise-ai-without-process",
    "summary": "<p><i>Presented by Celonis</i></p><hr /><p>AI adoption is accelerating, but results often lag expectations. And enterprise leaders are under pressure to prove measurable ROI from the AI solutions — especially as the use of autonomous agents rises and global tariffs disrupt supply chains.</p><p>The issue isn’t the AI itself, says Alex Rinke, co-founder and co-CEO of Celonis, a global leader in process intelligence. “To succeed, enterprise AI needs to understand the context of a business’s processes — and how to improve them,” he explains. Without this business context, AI risks becoming, as Rinke puts it, “just an internal social experiment.”</p><p>Next week’s <a href=\"https://www.celonis.com/celosphere?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=celosphere_2025&amp;utm_term=celosphere%202025&amp;utm_content=22651914883_en_land_701S700000OKTCyIAP_779234345923&amp;_bt=779234345923&amp;_bk=celosphere%202025&amp;_bm=e&amp;_bn=g&amp;_bg=179381830486&amp;gad_source=1&amp;gad_campaignid=22651914883&amp;gbraid=0AAAAADLGpd3h9NEhPsTPCZIIT2iqAbOhc&amp;gclid=Cj0KCQjwsPzHBhDCARIsALlWNG1CtEEAOM16cYwmh8udXlnIWgyjZLc0KRW61p2uY5hRFI8RWg14DpAaAo35EALw_wcB\">Celosphere 2025</a> will tackle the AI ROI challenge head-on. The three-day event brings together customer strategies, hands-on workshops, and live demonstrations, highlighting enhancements to the Celonis Process Intelligence (PI) Platform that help enterprises harness ‘enterprise AI,’ powered by PI, to continuously improve operations, creating measurable business value at scale.</p><h4><b>Focus on measurable ROI</b></h4><p>The event’s focus on achieving AI ROI reflects three challenges facing technology and business leaders moving from pilot to production: obsolete systems, break-neck industry change, and agentic AI. <a href=\"https://www.gartner.com/en/newsroom/press-releases/2024-10-22-gartner-survey-finds-ai-investments-lack-measurable-business-value\">According to Gartner</a>, 64% of board members now view AI as a top-three priority — yet only 10% of organizations report meaningful financial returns.</p><p>Celonis customers are bucking that trend. A <a href=\"https://www.celonis.com/resources/total-economic-impact-celonis\">Forrester Total Economic Impact study</a> found organizations using its platform achieved 383% ROI over three years, with payback in just six months. One company improved sales order automation from 33% to 86%, saving $24.5 million. The study estimated $44.1 million in total benefits over three years, driven by faster automation, reduced inefficiencies, and higher process visibility. These numbers underscore a broader pattern — companies that modernize outdated systems and align AI with process optimization see faster payback and sustained gains.</p><h4><b>Real companies, real results</b></h4><p>Celosphere will spotlight how global enterprises are building “future-fit” operations. Mercedes-Benz Group AG and Vinmar Group will showcase AI-driven, composable solutions, powered by PI, and attendees will see demonstrations of PI enabling agents in live production environments.</p><p><b>Among the notable success stories: </b></p><p><a href=\"https://www.celonis.com/solutions/stories/astrazeneca-inventory-management\">AstraZeneca</a>, the pharmaceutical company, reduced excess inventory while keeping critical medicines flowing by using Celonis as a foundation for its OpenAI partnership.</p><p><a href=\"https://www.celonis.com/solutions/stories/oklahoma-state-procurement\">The State of Oklahoma</a> can answer procurement status questions at scale, unlocking over $10 million in value. </p><p><a href=\"https://www.celonis.com/news/press/cosentino-drives-business-transformation-with-ai-powered-by-celonis-process-intelligence\">Cosentino</a> clears blocked sales orders up to 5x faster using an AI-powered credit management assistant. </p><h4><b>Raising the stakes for agentic AI</b></h4><p>Numerous sessions will focus on orchestrating AI agents. The shift from AI-as-advisor to AI-as-actor, changes everything, says Rinke. </p><p>“The agent needs to understand not just what to do, but how your specific business actually works,” he explains. “Process intelligence provides those rails.&quot; </p><p>This leap from recommendation to autonomous action raises the stakes exponentially. When agents can independently trigger purchase orders, reroute shipments, or approve exceptions, bad context can mean catastrophically bad outcomes at scale.</p><p>Celosphere attendees will get to see first-hand how companies are using the Celonis Orchestration Engine to coordinate AI agents alongside people and systems. Effective orchestration is a crucial protection against the chaos of agents working at cross-purposes, duplicating actions, or letting crucial steps fall through the cracks. </p><h4><b>Navigating tariffs and supply chain shocks</b></h4><p>Global trade volatility isn&#x27;t just a headline — it&#x27;s an operational nightmare reshaping how companies deploy AI, Rinke says. </p><p>New tariffs trigger cascading effects across procurement, logistics, and compliance. Each policy shift can cascade across thousands of SKUs — forcing new supplier contracts, rerouted shipments, and rebalanced inventories. For AI systems trained on static conditions, that volatility is almost impossible to predict. Traditional AI systems struggle with such variability — but process intelligence gives organizations real-time visibility into how changes ripple through operations.</p><p>Celosphere case studies will show how companies turn disruption into advantage. Smurfit Westrock uses PI to optimize inventory and reduce costs amid tariff uncertainty, while ASOS leverages PI to optimize its supply chain operations, enhancing efficiency, reducing costs, and continuing to deliver an outstanding customer experience.</p><h4><b>Platform over point solutions</b></h4><p>Rinke argues that Celonis’ edge lies in treating process intelligence not as an add-on, but as the foundation of the enterprise stack. Unlike bolt-on optimization tools, the Celonis platform creates a living digital twin of business operations — a continuously updated model enriched by context that lets AI operate effectively from analysis to execution.</p><p>“What sets Celonis apart is visibility across systems and offline tasks, which is critical for true intelligent automation,” Rinke says. “The platform offers comprehensive capabilities spanning process analysis, design, and orchestration rather than a point solution.”</p><h4><b>“Free the Process” and the future of AI</b></h4><p>Celonis continues to champion openness through its “Free the Process” movement, promoting fair competition and freeing enterprises from legacy lock-in. By giving organizations full access to their own process data, open APIs, and a growing partner network that includes The Hackett Group, ClearOps, and Lobster, Celonis is building the connective tissue for a new era of interoperable automation.</p><p>For Rinke, this open foundation is what turns AI from a set of experiments into an enterprise engine. “Process intelligence creates a flywheel,” he says. “Better understanding leads to better optimization, which enables better AI — and that, in turn, drives even greater understanding. There is no AI without PI.&quot;</p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>",
    "published": "Fri, 31 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-03T09:00:04.917847",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Meet Aardvark, OpenAI’s security agent for code analysis and patching",
    "link": "https://venturebeat.com/security/meet-aardvark-openais-in-house-security-agent-for-code-analysis-and-patching",
    "summary": "<p>OpenAI has introduced <b>Aardvark</b>, a <a href=\"https://openai.com/index/introducing-aardvark/\">GPT-5-powered autonomous security researcher agent</a> now available <a href=\"https://openai.com/form/aardvark-beta-signup/\">in private beta.</a></p><p>Designed to emulate how human experts identify and resolve software vulnerabilities, Aardvark offers a multi-stage, LLM-driven approach for continuous, 24<!-- -->/7/365<!-- --> <b>code analysis</b>, <b>exploit validation</b>, and<b> patch generation</b>!</p><p>Positioned as a scalable defense tool for modern software development environments, Aardvark is being tested across internal and external codebases. </p><p>OpenAI reports high recall and real-world effectiveness in identifying known and synthetic vulnerabilities, with early deployments surfacing previously undetected security issues.</p><p>Aardvark comes on the heels of <a href=\"https://venturebeat.com/ai/from-static-classifiers-to-reasoning-engines-openais-new-model-rethinks\">OpenAI’s release of the gpt-oss-safeguard models</a> yesterday, extending the company’s recent emphasis on agentic and policy-aligned systems. </p><h3><b>Technical Design and Operation</b></h3><p>Aardvark operates as an agentic system that continuously analyzes source code repositories. Unlike conventional tools that rely on fuzzing or software composition analysis, Aardvark leverages LLM reasoning and tool-use capabilities to interpret code behavior and identify vulnerabilities. </p><p>It simulates a security researcher’s workflow by reading code, conducting semantic analysis, writing and executing test cases, and using diagnostic tools.</p><p>Its process follows a structured multi-stage pipeline:</p><ol><li><p><b>Threat Modeling</b> – Aardvark initiates its analysis by ingesting an entire code repository to generate a threat model. This model reflects the inferred security objectives and architectural design of the software.</p></li><li><p><b>Commit-Level Scanning</b> – As code changes are committed, Aardvark compares diffs against the repository’s threat model to detect potential vulnerabilities. It also performs historical scans when a repository is first connected.</p></li><li><p><b>Validation Sandbox</b> – Detected vulnerabilities are tested in an isolated environment to confirm exploitability. This reduces false positives and enhances report accuracy.</p></li><li><p><b>Automated Patching</b> – The system integrates with OpenAI Codex to generate patches. These proposed fixes are then reviewed and submitted via pull requests for developer approval.</p></li></ol><p>Aardvark integrates with GitHub, Codex, and common development pipelines to provide continuous, non-intrusive security scanning. All insights are intended to be human-auditable, with clear annotations and reproducibility.</p><h3><b>Performance and Application</b></h3><p>According to OpenAI, Aardvark has been operational for several months on internal codebases and with select alpha partners. </p><p>In benchmark testing on “golden” repositories—where known and synthetic vulnerabilities were seeded—<b>Aardvark identified 92% of total issues. </b></p><p>OpenAI emphasizes that its accuracy and low false positive rate are key differentiators.</p><p>The agent has also been deployed on open-source projects. To date, it has discovered multiple critical issues, including ten vulnerabilities that were assigned CVE identifiers. </p><p>OpenAI states that all findings were responsibly disclosed under its recently updated coordinated disclosure policy, which favors collaboration over rigid timelines.</p><p>In practice, Aardvark has surfaced complex bugs beyond traditional security flaws, including logic errors, incomplete fixes, and privacy risks. This suggests broader utility beyond security-specific contexts.</p><h3><b>Integration and Requirements</b></h3><p>During the private beta, Aardvark is only available to organizations using GitHub Cloud (github.com). OpenAI invites beta testers to <a href=\"https://openai.com/form/aardvark-beta-signup/\">sign up here</a> online by filling out a web form. Participation requirements include:</p><ul><li><p>Integration with GitHub Cloud</p></li><li><p>Commitment to interact with Aardvark and provide qualitative feedback</p></li><li><p>Agreement to beta-specific terms and privacy policies</p></li></ul><p>OpenAI confirmed that code submitted to Aardvark during the beta will not be used to train its models.</p><p>The company is also offering pro bono vulnerability scanning for selected non-commercial open-source repositories, citing its intent to contribute to the health of the software supply chain.</p><h3><b>Strategic Context</b></h3><p>The launch of Aardvark signals OpenAI’s broader movement into agentic AI systems with domain-specific capabilities. </p><p>While OpenAI is best known for its general-purpose models (e.g., GPT-4 and GPT-5), Aardvark is part of a growing trend o<b>f specialized AI agents</b> designed to operate semi-autonomously within real-world environments. In fact, it joins two other active OpenAI agents now:</p><ul><li><p><a href=\"https://venturebeat.com/ai/openai-unveils-chatgpt-agent-that-gives-chatgpt-its-own-computer-to-autonomously-use-your-email-and-web-apps-download-and-create-files-for-you\">ChatGPT agent</a>, unveiled back in July 2025, which controls a virtual computer and web browser and can create and edit common productivity files</p></li><li><p><a href=\"https://venturebeat.com/programming-development/openai-launches-research-preview-of-codex-ai-software-engineering-agent-for-developers-with-parallel-tasking\">Codex</a> — previously the name of OpenAI&#x27;s open source coding model, which it took and re-used as the name of its new GPT-5 variant-powered AI coding agent unveiled back in May 2025</p></li></ul><p>But a security-focused agent makes a lot of sense, especially as demands on security teams grow.</p><p>In 2024 alone, over 40,000 Common Vulnerabilities and Exposures (CVEs) were reported, and OpenAI’s internal data suggests that 1.2% of all code commits introduce bugs. </p><p>Aardvark’s positioning as a “defender-first” AI aligns with a market need for proactive security tools that integrate tightly with developer workflows rather than operate as post-hoc scanning layers.</p><p>OpenAI’s coordinated disclosure policy updates further reinforce its commitment to sustainable collaboration with developers and the open-source community, rather than emphasizing adversarial vulnerability reporting.</p><p>While yesterday&#x27;s release of oss-safeguard uses chain-of-thought reasoning to apply safety policies during inference, Aardvark applies similar LLM reasoning to secure evolving codebases. </p><p>Together, these tools signal OpenAI’s shift from static tooling toward flexible, continuously adaptive systems — one focused on content moderation, the other on proactive vulnerability detection and automated patching within real-world software development environments.</p><h3><b>What It Means For Enterprises and the CyberSec Market Going Forward</b></h3><p>Aardvark represents OpenAI’s entry into automated security research through agentic AI. By combining GPT-5’s language understanding with Codex-driven patching and validation sandboxes, Aardvark offers an integrated solution for modern software teams facing increasing security complexity.</p><p>While currently in limited beta, the early performance indicators suggest potential for broader adoption. If proven effective at scale, Aardvark could contribute to a shift in how organizations embed security into continuous development environments.</p><p>For security leaders tasked with managing incident response, threat detection, and day-to-day protections—particularly those operating with limited team capacity—Aardvark may serve as a force multiplier. Its autonomous validation pipeline and human-auditable patch proposals could streamline triage and reduce alert fatigue, enabling smaller security teams to focus on strategic incidents rather than manual scanning and follow-up.</p><p>AI engineers responsible for integrating models into live products may benefit from Aardvark’s ability to surface bugs that arise from subtle logic flaws or incomplete fixes, particularly in fast-moving development cycles. Because Aardvark monitors commit-level changes and tracks them against threat models, it may help prevent vulnerabilities introduced during rapid iteration, without slowing delivery timelines.</p><p>For teams orchestrating AI across distributed environments, Aardvark’s sandbox validation and continuous feedback loops could align well with CI/CD-style pipelines for ML systems. Its ability to plug into GitHub workflows positions it as a compatible addition to modern AI operations stacks, especially those aiming to integrate robust security checks into automation pipelines without additional overhead.</p><p>And for data infrastructure teams maintaining critical pipelines and tooling, Aardvark’s LLM-driven inspection capabilities could offer an added layer of resilience. Vulnerabilities in data orchestration layers often go unnoticed until exploited; Aardvark’s ongoing code review process may surface issues earlier in the development lifecycle, helping data engineers maintain both system integrity and uptime.</p><p>In practice, Aardvark represents a shift in how security expertise might be operationalized—not just as a defensive perimeter, but as a persistent, context-aware participant in the software lifecycle. Its design suggests a model where defenders are no longer bottlenecked by scale, but augmented by intelligent agents working alongside them.</p>",
    "published": "Thu, 30 Oct 2025 21:07:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-03T09:00:04.918171",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Why IT leaders should pay attention to Canva’s ‘imagination era’ strategy",
    "link": "https://venturebeat.com/ai/why-it-leaders-should-pay-attention-to-canvas-imagination-era-strategy",
    "summary": "<p>The rise of AI marks a critical shift away from decades defined by information-chasing and a push for more and more compute power. </p><p>Canva co-founder and CPO Cameron Adams refers to this dawning time as the “imagination era.” Meaning: Individuals and enterprises must be able to turn creativity into <a href=\"https://venturebeat.com/ai/what-mit-got-wrong-about-ai-agents-new-g2-data-shows-theyre-already-driving\">action with AI</a>.  </p><p>Canva hopes to position itself at the center of this shift with a sweeping new suite of tools. The company’s new Creative Operating System (COS) integrates AI across every layer of content creation, creating a single, comprehensive creativity platform rather than a simple, template-based design tool.</p><p>“We’re entering a new era where we need to rethink how we achieve our goals,” said Adams. “We’re enabling people’s imagination and giving them the tools they need to take action.”</p><h2>An &#x27;engine&#x27; for creativity</h2><p>Adams describes Canva’s platform as a three-layer stack: The top Visual Suite layer containing designs, images and other content; a collaborative Canva AI plane at center; and a foundational proprietary model holding it all up. </p><p>At the heart of Canva’s strategy is its Creative Operating System (COS) underlying. This “engine,” as Adams describes it, integrates documents, websites, presentations, sheets, whiteboards, videos, social content, hundreds of millions of photos, illustrations, a rich sound library, and numerous templates, charts, and branded elements.</p><p>The COS is getting a 2.0 upgrade, but the crucial advance is the “middle, crucial layer” that fully integrates AI and makes it accessible throughout various workflows, Adams explained. This gives creative and technical teams a single dashboard for generating, editing and launching all types of content.</p><p>The underlying model is trained to understand the “complexity of design” so the platform can build out various elements — such as photos, videos, textures, or 3D graphics — in real time, matching branding style without the need for manual adjustments. It also supports live collaboration, meaning teams across departments can co-create. </p><p>With a unified dashboard, a user working on a specific design, for instance, can create a new piece of content (say, a presentation) within the same workflow, without having to switch to another window or platform. Also, if they generate an image and aren’t pleased with it, they don’t have to go back and create from scratch; they can immediately begin editing, changing colors or tone. </p><p>Another new capability in COS, “Ask Canva,” provides direct design advice. Users can tag @Canva to get copy suggestions and smart edits; or, they can highlight an image and direct the AI assistant to modify it or generate variants. </p><p>“It’s a really unique interaction,” said Adams, noting that this AI design partner is always present. “It’s a real collaboration between people and AI, and we think it’s a revolutionary change.”</p><p>Other new features include a 2.0 video editor and interactive form and email design with drag-and-drop tools. Further, Canva is now incorporated with Affinity, its unified app for pro designers incorporating vector, pixel and layer workflows, and Affinity is “free forever.” </p><h2>Automating intelligence, supporting marketing</h2><p>Branding is critical for enterprise; Canva has introduced new tools to help organizations consistently showcase theirs across platforms. The new Canva Grow engine integrates business objectives into the creative process so teams can workshop, create, distribute and refine ads and other materials. </p><p>As Adams explained: “It automatically scans your website, figures out who your audience is, what assets you use to promote your products, the message it needs to send out, the formats you want to send it out in, makes a creative for you, and you can deploy it directly to the platform without having to leave Canva.”</p><p>Marketing teams can now design and launch ads across platforms like Meta, track insights as they happen and refine future content based on performance metrics. “Your brand system is now available inside the AI you’re working with,” Adams noted. </p><h2><b>Success metrics and enterprise adoption</b></h2><p>The impact of Canva’s COS is reflected in notable user metrics: More than 250 million people use Canva every month, just over 29 million of which are paid subscribers. Adams reports that 41 billion designs have been created on Canva since launch, which equates to 1 billion each month. </p><p>“If you break that down, it turns into the crazy number of 386 designs being created every single second,” said Adams. Whereas in the early days, it took roughly an hour for users to create a single design. </p><p>Canva customers include Walmart, Disney, Virgin Voyages, Pinterest, FedEx, Expedia and eXp Realty. DocuSign, for one, reported that it unlocked more than 500 hours of team capacity and saved $300,000-plus in design hours by fully integrating Canva into its content creation. Disney, meanwhile, uses translation capabilities for its internationalization work, Adams said. </p><h2>Competitors in the design space</h2><p>Canva plays in an evolving landscape of professional design tools including Adobe Express and Figma; AI-powered challengers led by Microsoft Designer; and direct consumer alternatives like Visme and Piktochart.</p><p>Adobe Express (<a href=\"https://www.adobe.com/express/pricing\"><u>starting at $9.99 a month</u></a> for premium features) is known for its ease of use and integration with the broader Adobe Creative Cloud ecosystem. It features professional-grade templates and access to Adobe’s extensive stock library, and has incorporated Google&#x27;s Gemini 2.5 Flash image model and other gen AI features so that designers can create graphics via natural language prompts. Users with some design experience say they prefer its interface, controls and technical advantages over Canva (such as the ability to import high-fidelity PDFs). </p><p>Figma (<a href=\"https://www.figma.com/pricing/\"><u>starting at $3 a month</u></a> for professional plans) is touted for its real-time collaboration, advanced prototyping capabilities and deep integration with dev workflows; however, some say it has a steeper learning curve and higher-precision design tools, making it preferable for professional designers, developers and product teams working on more complex projects. </p><p>Microsoft Designer (free version available; although a Microsoft 365 subscription starting at <a href=\"https://support.microsoft.com/en-us/topic/frequently-asked-questions-about-microsoft-designer-9264654d-22f5-43ac-961d-b35851bbb93f#:~:text=Is%20Microsoft%20Designer%20free%20to,want%20to%20create%20more%20frequently.\"><u>$9.99 a month</u></a> unlocks additional features) benefits from its integration with Microsoft’s AI capabilities, Copilot layout and text generation and Dall-E powered image generation. The platform’s “Inspire Me” and “New Ideas” buttons provide design variations, and users can also import data from Excel, add 3D models from PowerPoint and access images from OneDrive. </p><p>However, users report that its stock photos and template and image libraries are limited compared to Canva&#x27;s extensive collection, and its visuals can come across as outdated. </p><p>Canva’s advantage seems to be in its extensive template library (more than 600,000 ready-to-use) and asset library (141 million-plus stock photos, videos, graphics, and audio elements).​ Its platform is also praised for its ease of use and interface friendly to non-designers, allowing them to begin quickly without training. </p><p>Canva has also expanded into a variety of content types — documents, websites, presentations, whiteboards, videos, and more — making its platform a comprehensive visual suite than just a graphics tool. </p><p>Canva has <a href=\"https://www.canva.com/pricing/\"><u>four pricing tiers</u></a>: Canva Free for one user; Canva Pro for $120 a year for one person; Canva Teams for $100 a year for each team member; and the custom-priced Canva Enterprise. </p><h2>Key takeaways: Be open, embrace human-AI collaboration</h2><p>Canva’s COS is underpinned by Canva’s frontier model, an in-house, proprietary engine based on years of R&amp;D and research partnerships, including the acquisition of visual AI company <a href=\"https://venturebeat.com/ai/canva-acquires-leonardo-ai-image-startup-to-bolster-generative-offerings\">Leonardo</a>. Adams notes that Canva works with top AI providers including OpenAI, Anthropic and Google. </p><p>For technology teams, Canva’s approach offers important lessons, including a commitment to openness. “There are so many models floating around,” Adams noted; it’s important for enterprises to recognize when they should work with top models and when they should develop their own proprietary ones, he advised. </p><p>For instance, OpenAI and Anthropic recently announced integrations with Canva as a visual layer because, as Adams explained, they realized they didn’t have the capability to create the same kinds of editable designs that Canva can. This creates a mutually-beneficial ecosystem. </p><p>Ultimately, Adams noted: “We have this underlying philosophy that the future is people and technology working together. It&#x27;s not an either or. We want people to be at the center, to be the ones with the creative spark, and to use AI as a collaborator.”</p>",
    "published": "Thu, 30 Oct 2025 03:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-11-03T09:00:04.918462",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Meta researchers open the LLM black box to repair flawed AI reasoning",
    "link": "https://venturebeat.com/ai/meta-researchers-open-the-llm-black-box-to-repair-flawed-ai-reasoning",
    "summary": "<p>Researchers at Meta FAIR and the University of Edinburgh have developed a new technique that can predict the correctness of a large language model&#x27;s (LLM) reasoning and even intervene to fix its mistakes. Called <a href=\"https://arxiv.org/abs/2510.09312\"><u>Circuit-based Reasoning Verification</u></a> (CRV), the method looks inside an LLM to monitor its internal “reasoning circuits” and detect signs of computational errors as the model solves a problem.</p><p>Their findings show that CRV can detect reasoning errors in LLMs with high accuracy by building and observing a computational graph from the model&#x27;s internal activations. In a key breakthrough, the researchers also demonstrated they can use this deep insight to apply targeted interventions that correct a model’s faulty reasoning on the fly.</p><p>The technique could help solve one of the great challenges of AI: Ensuring a model’s reasoning is faithful and correct. This could be a critical step toward building more trustworthy AI applications for the enterprise, where reliability is paramount.</p><h2>Investigating chain-of-thought reasoning</h2><p>Chain-of-thought (CoT) reasoning has been a powerful method for boosting the performance of LLMs on complex tasks and has been one of the key ingredients in the success of reasoning models such as the OpenAI o-series and <a href=\"https://venturebeat.com/ai/deepseek-r1-0528-arrives-in-powerful-open-source-challenge-to-openai-o3-and-google-gemini-2-5-pro\"><u>DeepSeek-R1</u></a>. </p><p>However, despite the success of CoT, it is not fully reliable. The reasoning process itself is often flawed, and <a href=\"https://venturebeat.com/ai/dont-believe-reasoning-models-chains-of-thought-says-anthropic\"><u>several</u></a> <a href=\"https://venturebeat.com/ai/llms-generate-fluent-nonsense-when-reasoning-outside-their-training-zone\"><u>studies</u></a> have shown that the CoT tokens an LLM generates is not always a faithful representation of its internal reasoning process.</p><p>Current remedies for verifying CoT fall into two main categories. “Black-box” approaches analyze the final generated token or the confidence scores of different token options. “Gray-box” approaches go a step further, looking at the model&#x27;s internal state by using simple probes on its raw neural activations. </p><p>But while these methods can detect that a model’s internal state is correlated with an error, they can&#x27;t explain <i>why</i> the underlying computation failed. For real-world applications where understanding the root cause of a failure is crucial, this is a significant gap.</p><h2>A white-box approach to verification</h2><p>CRV is based on the idea that models perform tasks using specialized subgraphs, or &quot;circuits,&quot; of neurons that function like latent algorithms. So if the model’s reasoning fails, it is caused by a flaw in the execution of one of these algorithms. This means that by inspecting the underlying computational process, we can diagnose the cause of the flaw, similar to how developers examine execution traces to debug traditional software.</p><p>To make this possible, the researchers first make the target LLM interpretable. They replace the standard dense layers of the transformer blocks with trained &quot;transcoders.&quot; A transcoder is a specialized deep learning component that forces the model to represent its intermediate computations not as a dense, unreadable vector of numbers, but as a sparse and meaningful set of features. Transcoders are similar to the <a href=\"https://venturebeat.com/ai/deepmind-makes-big-jump-toward-interpreting-llms-with-sparse-autoencoders\"><u>sparse autoencoders</u></a> (SAE) used in mechanistic interpretability research with the difference that they also preserve the functionality of the network they emulate. This modification effectively installs a diagnostic port into the model, allowing researchers to observe its internal workings.</p><p>With this interpretable model in place, the CRV process unfolds in a few steps. For each reasoning step the model takes, CRV constructs an &quot;attribution graph&quot; that maps the causal flow of information between the interpretable features of the transcoder and the tokens it is processing. From this graph, it extracts a &quot;structural fingerprint&quot; that contains a set of features describing the graph&#x27;s properties. Finally, a “diagnostic classifier” model is trained on these fingerprints to predict whether the reasoning step is correct or not.</p><p>At inference time, the classifier monitors the activations of the model and provides feedback on whether the model’s reasoning trace is on the right track.</p><h2>Finding and fixing errors</h2><p>The researchers tested their method on a <a href=\"https://venturebeat.com/ai/meta-unleashes-its-most-powerful-ai-model-llama-3-1-with-405b-parameters\"><u>Llama 3.1 8B</u></a> Instruct model modified with the transcoders, evaluating it on a mix of synthetic (Boolean and Arithmetic) and real-world (GSM8K math problems) datasets. They compared CRV against a comprehensive suite of black-box and gray-box baselines.</p><p>The results provide strong empirical support for the central hypothesis: the structural signatures in a reasoning step&#x27;s computational trace contain a verifiable signal of its correctness. CRV consistently outperformed all baseline methods across every dataset and metric, demonstrating that a deep, structural view of the model&#x27;s computation is more powerful than surface-level analysis.</p><p>Interestingly, the analysis revealed that the signatures of error are highly domain-specific. This means failures in different reasoning tasks (formal logic versus arithmetic calculation) manifest as distinct computational patterns. A classifier trained to detect errors in one domain does not transfer well to another, highlighting that different types of reasoning rely on different internal circuits. In practice, this means that you might need to train a separate classifier for each task (though the transcoder remains unchanged).</p><p>The most significant finding, however, is that these error signatures are not just correlational but causal. Because CRV provides a transparent view of the computation, a predicted failure can be traced back to a specific component. In one case study, the model made an order-of-operations error. CRV flagged the step and identified that a &quot;multiplication&quot; feature was firing prematurely. The researchers intervened by manually suppressing that single feature, and the model immediately corrected its path and solved the problem correctly. </p><p>This work represents a step toward a more rigorous science of AI interpretability and control. As the paper concludes, “these findings establish CRV as a proof-of-concept for mechanistic analysis, showing that shifting from opaque activations to interpretable computational structure enables a causal understanding of how and why LLMs fail to reason correctly.” To support further research, the team plans to release its datasets and trained transcoders to the public.</p><h2>Why it’s important</h2><p>While CRV is a research proof-of-concept, its results hint at a significant future for AI development. AI models learn internal algorithms, or &quot;circuits,&quot; for different tasks. But because these models are opaque, we can&#x27;t debug them like standard computer programs by tracing bugs to specific steps in the computation. Attribution graphs are the closest thing we have to an execution trace, showing how an output is derived from intermediate steps.</p><p>This research suggests that attribution graphs could be the foundation for a new class of AI model debuggers. Such tools would allow developers to understand the root cause of failures, whether it&#x27;s insufficient training data or interference between competing tasks. This would enable precise mitigations, like targeted fine-tuning or even direct model editing, instead of costly full-scale retraining. They could also allow for more efficient intervention to correct model mistakes during inference.</p><p>The success of CRV in detecting and pinpointing reasoning errors is an encouraging sign that such debuggers could become a reality. This would pave the way for more robust LLMs and autonomous agents that can handle real-world unpredictability and, much like humans, correct course when they make reasoning mistakes. </p>",
    "published": "Thu, 30 Oct 2025 00:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-03T09:00:04.918805",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Vibe coding platform Cursor releases first in-house LLM, Composer, promising 4X speed boost",
    "link": "https://venturebeat.com/ai/vibe-coding-platform-cursor-releases-first-in-house-llm-composer-promising",
    "summary": "<p>The vibe coding tool Cursor, from startup <a href=\"https://anysphere.inc/\">Anysphere</a>, has <a href=\"https://cursor.com/blog/composer\">introduced Composer</a>, its first in-house, proprietary coding large language model (LLM) as part of its <a href=\"https://cursor.com/blog/2-0\">Cursor 2.0 platform update</a>. </p><p>Composer is designed to execute coding tasks quickly and accurately in production-scale environments, representing a new step in AI-assisted programming. It&#x27;s already being used by Cursor’s own engineering staff in day-to-day development — indicating maturity and stability.</p><p>According to Cursor, Composer completes most interactions in <b>less than 30 seconds</b> while maintaining a high level of reasoning ability across large and complex codebases. </p><p>The model is described as four times faster than similarly intelligent systems and is trained for “agentic” workflows—where autonomous coding agents plan, write, test, and review code collaboratively.</p><p>Previously, Cursor supported &quot;vibe coding&quot; — using AI to write or complete code based on natural language instructions from a user, even someone untrained in development — <a href=\"https://cursor.com/docs/models\">atop other leading proprietary LLMs</a> from the likes of OpenAI, Anthropic, Google, and xAI. These options are still available to users.</p><h3><b>Benchmark Results</b></h3><p>Composer’s capabilities are benchmarked using &quot;Cursor Bench,&quot; an internal evaluation suite derived from real developer agent requests. The benchmark measures not just correctness, but also the model’s adherence to existing abstractions, style conventions, and engineering practices.</p><p>On this benchmark, Composer achieves frontier-level coding intelligence while generating at <a href=\"https://x.com/srush_nlp/status/1983614856646578662\">250 tokens per second</a> — about twice as fast as leading fast-inference models and four times faster than comparable frontier systems.</p><p>Cursor’s published comparison groups models into several categories: “Best Open” (e.g., Qwen Coder, GLM 4.6), “Fast Frontier” (Haiku 4.5, Gemini Flash 2.5), “Frontier 7/2025” (the strongest model available midyear), and “Best Frontier” (including GPT-5 and Claude Sonnet 4.5). Composer matches the intelligence of mid-frontier systems while delivering the highest recorded generation speed among all tested classes.</p><h3><b>A Model Built with Reinforcement Learning and Mixture-of-Experts Architecture</b></h3><p>Research scientist Sasha Rush of Cursor provided insight into the model’s development in <a href=\"https://x.com/srush_nlp/status/1983572683355725869\">posts on the social network X</a>, describing Composer as a reinforcement-learned (RL) mixture-of-experts (MoE) model:</p><blockquote><p>“We used RL to train a big MoE model to be really good at real-world coding, and also very fast.”</p></blockquote><p>Rush explained that the team co-designed both Composer and the Cursor environment to allow the model to operate efficiently at production scale:</p><blockquote><p>“Unlike other ML systems, you can’t abstract much from the full-scale system. We co-designed this project and Cursor together in order to allow running the agent at the necessary scale.”</p></blockquote><p>Composer was trained on real software engineering tasks rather than static datasets. During training, the model operated inside full codebases using a suite of production tools—including file editing, semantic search, and terminal commands—to solve complex engineering problems. Each training iteration involved solving a concrete challenge, such as producing a code edit, drafting a plan, or generating a targeted explanation.</p><p>The reinforcement loop optimized both correctness and efficiency. Composer learned to make effective tool choices, use parallelism, and avoid unnecessary or speculative responses. Over time, the model developed emergent behaviors such as running unit tests, fixing linter errors, and performing multi-step code searches autonomously.</p><p>This design enables Composer to work within the same runtime context as the end-user, making it more aligned with real-world coding conditions—handling version control, dependency management, and iterative testing.</p><h3><b>From Prototype to Production</b></h3><p>Composer’s development followed an earlier internal prototype known as <b>Cheetah</b>, which Cursor used to explore low-latency inference for coding tasks.</p><blockquote><p>“Cheetah was the v0 of this model primarily to test speed,” Rush said on X. “Our metrics say it [Composer] is the same speed, but much, much smarter.”</p></blockquote><p>Cheetah’s success at reducing latency helped Cursor identify speed as a key factor in developer trust and usability. </p><p>Composer maintains that responsiveness while significantly improving reasoning and task generalization.</p><p>Developers who used Cheetah during early testing noted that its speed changed how they worked. One user commented that it was “so fast that I can stay in the loop when working with it.” </p><p>Composer retains that speed but extends capability to multi-step coding, refactoring, and testing tasks.</p><h3><b>Integration with Cursor 2.0</b></h3><p>Composer is fully integrated into Cursor 2.0, a major update to the company’s agentic development environment. </p><p>The platform introduces a multi-agent interface, allowing<b> up to eight agents to run in parallel,</b> each in an isolated workspace using git worktrees or remote machines.</p><p>Within this system, Composer can serve as one or more of those agents, performing tasks independently or collaboratively. Developers can compare multiple results from concurrent agent runs and select the best output.</p><p>Cursor 2.0 also includes supporting features that enhance Composer’s effectiveness:</p><ul><li><p><b>In-Editor Browser (GA)</b> – enables agents to run and test their code directly inside the IDE, forwarding DOM information to the model.</p></li><li><p><b>Improved Code Review</b> – aggregates diffs across multiple files for faster inspection of model-generated changes.</p></li><li><p><b>Sandboxed Terminals (GA)</b> – isolate agent-run shell commands for secure local execution.</p></li><li><p><b>Voice Mode</b> – adds speech-to-text controls for initiating or managing agent sessions.</p></li></ul><p>While these platform updates expand the overall Cursor experience, Composer is positioned as the technical core enabling fast, reliable agentic coding.</p><h3><b>Infrastructure and Training Systems</b></h3><p>To train Composer at scale, Cursor built a custom reinforcement learning infrastructure combining PyTorch and Ray for asynchronous training across thousands of NVIDIA GPUs. </p><p>The team developed specialized MXFP8 MoE kernels and hybrid sharded data parallelism, enabling large-scale model updates with minimal communication overhead.</p><p>This configuration allows Cursor to train models natively at low precision without requiring post-training quantization, improving both inference speed and efficiency. </p><p>Composer’s training relied on hundreds of thousands of concurrent sandboxed environments—each a self-contained coding workspace—running in the cloud. The company adapted its Background Agents infrastructure to schedule these virtual machines dynamically, supporting the bursty nature of large RL runs.</p><h3><b>Enterprise Use</b></h3><p>Composer’s performance improvements are supported by infrastructure-level changes across Cursor’s code intelligence stack. </p><p>The company has optimized its Language Server Protocols (LSPs) for faster diagnostics and navigation, especially in Python and TypeScript projects. These changes reduce latency when Composer interacts with large repositories or generates multi-file updates.</p><p>Enterprise users gain administrative control over Composer and other agents through team rules, audit logs, and sandbox enforcement. Cursor’s Teams and Enterprise tiers also support pooled model usage, SAML/OIDC authentication, and analytics for monitoring agent performance across organizations.</p><p>Pricing for individual users ranges from Free (Hobby) to Ultra ($200/month) tiers, with expanded usage limits for Pro+ and Ultra subscribers. </p><p>Business pricing starts at $40 per user per month for Teams, with enterprise contracts offering custom usage and compliance options.</p><h3><b>Composer’s Role in the Evolving AI Coding Landscape</b></h3><p>Composer’s focus on speed, reinforcement learning, and integration with live coding workflows differentiates it from other AI development assistants such as GitHub Copilot or Replit’s Agent. </p><p>Rather than serving as a passive suggestion engine, Composer is designed for continuous, agent-driven collaboration, where multiple autonomous systems interact directly with a project’s codebase.</p><p>This model-level specialization—training AI to function within the real environment it will operate in—represents a significant step toward practical, autonomous software development. Composer is not trained only on text data or static code, but within a dynamic IDE that mirrors production conditions.</p><p>Rush described this approach as essential to achieving real-world reliability: the model learns not just how to generate code, but how to integrate, test, and improve it in context.</p><h3><b>What It Means for Enterprise Devs and Vibe Coding</b></h3><p>With Composer, Cursor is introducing more than a fast model—it’s deploying an AI system optimized for real-world use, built to operate inside the same tools developers already rely on. </p><p>The combination of reinforcement learning, mixture-of-experts design, and tight product integration gives Composer a practical edge in speed and responsiveness that sets it apart from general-purpose language models.</p><p>While Cursor 2.0 provides the infrastructure for multi-agent collaboration, Composer is the core innovation that makes those workflows viable. </p><p>It’s the first coding model built specifically for agentic, production-level coding—and an early glimpse of what everyday programming could look like when human developers and autonomous models share the same workspace.</p>",
    "published": "Wed, 29 Oct 2025 19:28:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-03T09:00:04.919353",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "DevOps for AI: Continuous deployment pipelines for machine learning systems",
    "link": "https://www.artificialintelligence-news.com/news/devops-for-ai-continuous-deployment-pipelines-for-machine-learning-systems/",
    "summary": "<p>AI&#8217;s effects on continuous development and deployment pipelines are becoming difficult to ignore. However, decision-makers in software development functions need to consider a broad range of elements when considering the uses of the technology. The challenges of deploying AI at scale Deploying artificial intelligence isn&#8217;t the same as deploying, for example, a web app. Traditional [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/devops-for-ai-continuous-deployment-pipelines-for-machine-learning-systems/\">DevOps for AI: Continuous deployment pipelines for machine learning systems</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Mon, 03 Nov 2025 10:31:40 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-03T09:00:08.317521",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-03T10:31:40+00:00",
    "days_old": 0,
    "priority_score": 0.855
  },
  {
    "title": "How Lumana is redefining AI’s role in video surveillance",
    "link": "https://www.artificialintelligence-news.com/news/how-lumana-is-redefining-ais-role-in-video-surveillance/",
    "summary": "<p>For all the progress in artificial intelligence, most video security systems still fail at recognising context in real-world conditions. The majority of cameras can capture real-time footage, but struggle to interpret it. This is a problem turning into a growing concern for smart city designers, manufacturers and schools, each of which may depend on AI [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/how-lumana-is-redefining-ais-role-in-video-surveillance/\">How Lumana is redefining AI&#8217;s role in video surveillance</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Fri, 31 Oct 2025 15:36:18 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-03T09:00:08.317729",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-31T15:36:18+00:00",
    "days_old": 3,
    "priority_score": 0.855
  },
  {
    "title": "Finding return on AI investments across industries",
    "link": "https://www.technologyreview.com/2025/10/28/1126693/finding-return-on-ai-investments-across-industries/",
    "summary": "The market is officially three years post ChatGPT and many of the pundit bylines have shifted to using terms like “bubble” to suggest reasons behind generative AI not realizing material returns outside a handful of technology suppliers.&#160; In September, the MIT NANDA report made waves because the soundbite every author and influencer picked up on&#8230;",
    "published": "Tue, 28 Oct 2025 15:00:33 +0000",
    "source_name": "mit_tech_review",
    "source_url": "https://www.technologyreview.com/topic/artificial-intelligence/feed",
    "category": "ai_research",
    "weight": 0.9,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-03T09:00:09.720937",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.54
  },
  {
    "title": "Audrey Tang, hacker and Taiwanese digital minister: ‘AI is a parasite that fosters polarization’",
    "link": "https://www.reddit.com/r/artificial/comments/1on8aqc/audrey_tang_hacker_and_taiwanese_digital_minister/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1on8aqc/audrey_tang_hacker_and_taiwanese_digital_minister/\"> <img alt=\"Audrey Tang, hacker and Taiwanese digital minister: ‘AI is a parasite that fosters polarization’\" src=\"https://external-preview.redd.it/kJVhgMfpAjKOd4LlXeUpABRlxKkcZWc6-uaDDXS-j7g.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=abe06a4f97922010e092c368244047f7b574d14b\" title=\"Audrey Tang, hacker and Taiwanese digital minister: ‘AI is a parasite that fosters polarization’\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MetaKnowing\"> /u/MetaKnowing </a> <br /> <span><a href=\"https://english.elpais.com/technology/2025-11-02/audrey-tang-hacker-and-taiwanese-digital-minister-ai-is-a-parasite-that-fosters-polarization.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1on8aqc/audrey_tang_hacker_and_taiwanese_digital_minister/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-11-03T10:30:31+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-03T09:00:10.994983",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-03T10:30:31+00:00",
    "days_old": 0,
    "priority_score": 0.63
  },
  {
    "title": "Utah and California are starting to require businesses to tell you when you're talking to AI | States are cracking down on hidden AI, but the tech industry is pushing back",
    "link": "https://www.reddit.com/r/artificial/comments/1on83cx/utah_and_california_are_starting_to_require/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1on83cx/utah_and_california_are_starting_to_require/\"> <img alt=\"Utah and California are starting to require businesses to tell you when you're talking to AI | States are cracking down on hidden AI, but the tech industry is pushing back\" src=\"https://external-preview.redd.it/JIl1hdJkNcOfSmCiorntNz4g4jMQa9hj90EvE-HjNqY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8b3ea2e620abc791fb9edb7c35c656fc68b9618\" title=\"Utah and California are starting to require businesses to tell you when you're talking to AI | States are cracking down on hidden AI, but the tech industry is pushing back\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MetaKnowing\"> /u/MetaKnowing </a> <br /> <span><a href=\"https://www.techspot.com/news/110091-utah-california-starting-require-businesses-tell-you-when.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1on83cx/utah_and_california_are_starting_to_require/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-11-03T10:18:15+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-03T09:00:10.995292",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-03T10:18:15+00:00",
    "days_old": 0,
    "priority_score": 0.63
  }
]