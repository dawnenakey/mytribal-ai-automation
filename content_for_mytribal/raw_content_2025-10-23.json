[
  {
    "title": "‘AI is tearing companies apart’: Writer AI CEO slams Fortune 500 leaders for mismanaging tech",
    "link": "https://venturebeat.com/ai/ai-is-tearing-companies-apart-writer-ai-ceo-slams-fortune-500-leaders-for",
    "summary": "<p><a href=\"https://writer.com/blog/enterprise-ai-future-fireside/\"><u>May Habib</u></a>, co-founder and CEO of <a href=\"https://writer.com/\"><u>Writer AI</u></a>, delivered one of the bluntest assessments of corporate AI failures at the <a href=\"https://tedai-sanfrancisco.ted.com/\"><u>TED AI conference</u></a> on Tuesday, revealing that nearly half of Fortune 500 executives believe artificial intelligence is actively damaging their organizations — and placing the blame squarely on leadership&#x27;s shoulders.</p><p>The problem, according to Habib, isn&#x27;t the technology. It&#x27;s that business leaders are making a category error, treating AI transformation like previous technology rollouts and delegating it to IT departments. This approach, she warned, has led to &quot;billions of dollars spent on AI initiatives that are going nowhere.&quot;</p><p>&quot;Earlier this year, we did a survey of 800 Fortune 500 C-suite executives,&quot; Habib told the audience of Silicon Valley executives and investors. &quot;42% of them said AI is tearing their company apart.&quot;</p><p>The diagnosis challenges conventional wisdom about how enterprises should approach AI adoption. While most major companies have <a href=\"https://hai.stanford.edu/ai-index/2025-ai-index-report\"><u>stood up AI task forces</u></a>, <a href=\"https://the-decoder.com/by-2026-most-firms-expect-to-have-a-chief-ai-officer-on-staff/\"><u>appointed chief AI officers</u></a>, or <a href=\"https://www.ciodive.com/news/tech-spend-surge-slackens-cloud-ai-building-boom/752980/\"><u>expanded IT budgets</u></a>, Habib argues these moves reflect a fundamental misunderstanding of what AI represents: not another software tool, but a wholesale reorganization of how work gets done.</p><p>&quot;There is something leaders are missing when they compare AI to just another tech tool,&quot; Habib said. &quot;This is not like giving accountants calculators or bankers Excel or designers Photoshop.&quot;</p><h2><b>Why the &#x27;old playbook&#x27; of delegating to IT departments is failing companies</b></h2><p>Habib, whose company has spent five years building AI systems for Fortune 500 companies and logged two million miles visiting customer sites, said the pattern is consistent: &quot;When generative AI started showing up, we turned to the old playbook. We turned to IT and said, &#x27;Go figure this out.&#x27;&quot;</p><p>That approach fails, she argued, because AI fundamentally changes the economics and organization of work itself. &quot;For 100 years, enterprises have been built around the idea that execution is expensive and hard,&quot; Habib said. &quot;The enterprise built complex org charts, complex processes, all to manage people doing stuff.&quot;</p><p>AI inverts that model. &quot;Execution is going from scarce and expensive to programmatic, on-demand and abundant,&quot; she said. In this new paradigm, the bottleneck shifts from execution capacity to strategic design — a shift that requires business leaders, not IT departments, to drive transformation.</p><p>&quot;With AI technology, it can no longer be centralized. It&#x27;s in every workflow, every business,&quot; Habib said. &quot;It is now the most important part of a business leader&#x27;s job. It cannot be delegated.&quot;</p><p>The statement represents a direct challenge to how most large organizations have structured their AI initiatives, with centralized centers of excellence, dedicated AI teams, or IT-led implementations that business units are expected to adopt.</p><h2><b>A generational power shift is happening based on who understands AI workflow design</b></h2><p>Habib framed the shift in dramatic terms: &quot;A generational transfer of power is happening right now. It&#x27;s not about your age or how long you&#x27;ve been at a company. The generational transfer of power is about the nature of leadership itself.&quot;</p><p>Traditional leadership, she argued, has been defined by the ability to manage complexity — big teams, big budgets, intricate processes. &quot;The identity of leaders at these companies, people like us, has been tied to old school power structures: control, hierarchy, how big our teams are, how big our budgets are. Our value is measured by the sheer amount of complexity we could manage,&quot; Habib said. &quot;Today we reward leaders for this. We promote leaders for this.&quot;</p><p>AI makes that model obsolete. &quot;When I am able to 10x the output of my team or do things that could never be possible, work is no longer about the 1x,&quot; she said. &quot;Leadership is no longer about managing complex human execution.&quot;</p><p>Instead, Habib outlined three fundamental shifts that define what she calls &quot;AI-first leaders&quot; — executives her company has worked with who have successfully deployed AI agents solving &quot;$100 million plus problems.&quot;</p><h2><b>The first shift: Taking a machete to enterprise complexity</b></h2><p>The new leadership mandate, according to Habib, is &quot;taking a machete to the complexity that has calcified so many organizations.&quot; She pointed to the layers of friction that have accumulated in enterprises: &quot;Brilliant ideas dying in memos, the endless cycles of approvals, the death by 1,000 clicks, meetings about meetings — a death, by the way, that&#x27;s happening in 17 different browser tabs each for software that promises to be a single source of truth.&quot;</p><p>Rather than accepting this complexity as inevitable, AI-first leaders redesign workflows from first principles. &quot;There are very few legacy systems that can&#x27;t be replaced in your organization, that won&#x27;t be replaced,&quot; Habib said. &quot;But they&#x27;re not going to be replaced by another monolithic piece of software. They can only be replaced by a business leader articulating business logic and getting that into an agentic system.&quot;</p><p>She offered a concrete example: &quot;We have customers where it used to take them seven months to get a creative campaign — not even a product, a campaign. Now they can go from TikTok trend to digital shelf in 30 days. That is radical simplicity.&quot;</p><p>The catch, she emphasized, is that CIOs can&#x27;t drive this transformation alone. &quot;Your CIO can&#x27;t help flatten your org chart. Only a business leader can look at workflows and say, &#x27;This part is necessary genius, this part is bureaucratic scar tissue that has to go.&#x27;&quot;</p><h2><b>The second shift: Managing the fear as career ladders disappear</b></h2><p>When AI handles execution, &quot;your humans are liberated to do what they&#x27;re amazing at: judgment, strategy, creativity,&quot; Habib explained. &quot;The old leadership playbook was about managing headcount. We managed people against revenue: one business development rep for every three account executives, one marketer for every five salespeople.&quot;</p><p>But this liberation carries profound challenges that leaders must address directly. Habib acknowledged the elephant in the room that many executives avoid discussing: &quot;These changes are still frightening for people, even when it&#x27;s become unholy to talk about it.&quot; She&#x27;s witnessed the fear firsthand. &quot;It shows up as tears in an AI workshop when someone feels like their old skill set isn&#x27;t translated to the new.&quot;</p><p>She introduced a term for a common form of resistance: &quot;productivity anchoring&quot; — when employees &quot;cling to the hard way of doing things because they feel productive, because their self-worth is tied to them, even when empirically AI can be better.&quot;</p><p>The solution isn&#x27;t to look away. &quot;We have to design new pathways to impact, to show your people their value is not in executing a task. Their value is in orchestrating systems of execution, to ask the next great question,&quot; Habib said. She advocates replacing career &quot;ladders&quot; with &quot;lattices&quot; where &quot;people need to grow laterally, to expand sideways.&quot;</p><p>She was candid about the disruption: &quot;The first rungs on our career ladders are indeed going away. I know because my company is automating them.&quot; But she insisted this creates opportunity for work that is &quot;more creative, more strategic, more driven by curiosity and impact — and I believe a lot more human than the jobs that they&#x27;re replacing.&quot;</p><h2><b>The third shift: When execution becomes free, ambition becomes the only bottleneck</b></h2><p>The final shift is from optimization to creation. &quot;Before AI, we used to call it transformation when we took 12 steps and made them nine,&quot; Habib said. &quot;That&#x27;s optimizing the world as it is. We can now create a new world. That is the greenfield mindset.&quot;</p><p>She challenged executives to identify assumptions their industries are built on that AI now disrupts. Writer&#x27;s customers, she said, are already seeing new categories of growth: treating every customer like their only customer, democratizing premium services to broader markets, and entering new markets at unprecedented speed because &quot;AI strips away the friction to access new channels.&quot;</p><p>&quot;When execution is abundant, the only bottleneck is the scope of your own ambition,&quot; Habib declared.</p><h2><b>What this means for CIOs: Building the stadium while business leaders design the plays</b></h2><p>Habib didn&#x27;t leave IT leaders without a role — she redefined it. &quot;If tech is everyone&#x27;s job, you might be asking, what is mine?&quot; she addressed CIOs. &quot;Yours is to provide the mission critical infrastructure that makes this revolution possible.&quot;</p><p>As tens or hundreds of thousands of AI agents operate at various levels of autonomy within organizations, &quot;governance becomes existential,&quot; she explained. &quot;The business leader&#x27;s job is to design the play, but you have to build the stadium, you have to write the rule book, and you have to make sure these plays can win at championship scale.&quot;</p><p>The formulation suggests a partnership model: business leaders drive workflow redesign and strategic implementation while IT provides the infrastructure, governance frameworks, and security guardrails that make mass AI deployment safe and scalable. &quot;One can&#x27;t succeed without the other,&quot; Habib said.</p><p>For CIOs and technical leaders, this represents a fundamental shift from gatekeeper to enabler. When business units deploy agents autonomously, IT faces governance challenges unlike anything in enterprise software history. Success requires genuine partnership between business and IT — neither can succeed alone, forcing cultural changes in how these functions collaborate.</p><h2><b>A real example: From multi-day scrambles to instant answers during a market crisis</b></h2><p>To ground her arguments in concrete business impact, Habib described working with the chief client officer of a Fortune 500 wealth advisory firm during recent market volatility following tariff announcements.</p><p>&quot;Their phone was ringing off the hook with customers trying to figure out their market exposure,&quot; she recounted. &quot;Every request kicked off a multi-day, multi-person scramble: a portfolio manager ran the show, an analyst pulled charts, a relationship manager built the PowerPoint, a compliance officer had to review everything for disclosures. And the leader in all this — she was forwarding emails and chasing updates. This is the top job: managing complexity.&quot;</p><p>With an agentic AI system, the same work happens programmatically. &quot;A system of agents is able to assemble the answer faster than any number of people could have. No more midnight deck reviews. No more days on end&quot; of coordination, Habib said.</p><p>This isn&#x27;t about marginal productivity gains — it&#x27;s about fundamentally different operating models where senior executives shift from managing coordination to designing intelligent systems.</p><h2><b>Why so many AI initiatives are failing despite massive investment</b></h2><p>Habib&#x27;s arguments arrive as many enterprises face AI disillusionment. After initial excitement about generative AI, many companies have <a href=\"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/\"><u>struggled to move beyond pilots</u></a> and demonstrations to production deployments generating tangible business value.</p><p>Her diagnosis — that leaders are delegating rather than driving transformation — aligns with growing evidence that organizational factors, not technical limitations, explain most failures. Companies often lack clarity on use cases, struggle with data preparation, or face internal resistance to workflow changes that AI requires.</p><p>Perhaps the most striking aspect of Habib&#x27;s presentation was her willingness to acknowledge the human cost of AI transformation — and insist leaders address it rather than avoid it. &quot;Your job as a leader is to not look away from this fear. Your job is to face it with a plan,&quot; she told the audience.</p><p>She described &quot;productivity anchoring&quot; as a form of &quot;self-sabotage&quot; where employees resist AI adoption because their identity and self-worth are tied to execution tasks AI can now perform. The phenomenon suggests that successful AI transformation requires not just technical and strategic changes but psychological and cultural work that many leaders may be unprepared for.</p><h2><b>Two challenges: Get your hands dirty, then reimagine everything</b></h2><p>Habib closed by throwing down two gauntlets to her executive audience.</p><p>&quot;First, a small one: get your hands dirty with agentic AI. Don&#x27;t delegate. Choose a process that you oversee and automate it. See the difference from managing a complex process to redesigning it for yourself.&quot;</p><p>The second was more ambitious: &quot;Go back to your team and ask, what could we achieve if execution were free? What would work feel like, be like, look like if you&#x27;re unbound from the friction and process that slows us down today?&quot;</p><p>She concluded: &quot;The tools for creation are in your hands. The mandate for leadership is on your shoulders. What will you build?&quot;</p><p>For enterprise leaders accustomed to viewing AI as an IT initiative, Habib&#x27;s message is clear: that approach isn&#x27;t working, won&#x27;t work, and reflects a fundamental misunderstanding of what AI represents. Whether executives embrace her call to personally drive transformation — or continue delegating to IT departments — may determine which organizations thrive and which become cautionary tales.</p><p>The statistic she opened with lingers uncomfortably: 42% of Fortune 500 C-suite executives say AI is tearing their companies apart. Habib&#x27;s diagnosis suggests they&#x27;re tearing themselves apart by clinging to organizational models designed for an era when execution was scarce. The cure she prescribes requires leaders to do something most find uncomfortable: stop managing complexity and start dismantling it.</p>",
    "published": "Thu, 23 Oct 2025 13:02:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-23T09:39:54.396846",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Sakana AI's CTO says he's 'absolutely sick' of transformers, the tech that powers every major AI model",
    "link": "https://venturebeat.com/ai/sakana-ais-cto-says-hes-absolutely-sick-of-transformers-the-tech-that-powers",
    "summary": "<p>In a striking act of self-critique, one of the architects of the transformer technology that powers <a href=\"https://chatgpt.com/\"><u>ChatGPT</u></a>, <a href=\"https://claude.ai/\"><u>Claude</u></a>, and virtually every major AI system told an audience of industry leaders this week that artificial intelligence research has become dangerously narrow — and that he&#x27;s moving on from his own creation.</p><p><a href=\"https://scholar.google.com/citations?user=_3_P5VwAAAAJ&amp;hl=en\"><u>Llion Jones</u></a>, who co-authored the seminal 2017 paper &quot;<a href=\"https://arxiv.org/abs/1706.03762\"><u>Attention Is All You Need</u></a>&quot; and even coined the name &quot;transformer,&quot; delivered an unusually candid assessment at the <a href=\"https://tedai-sanfrancisco.ted.com/\"><u>TED AI conference</u></a> in San Francisco on Tuesday: Despite <a href=\"https://hbr.org/2025/10/is-ai-a-boom-or-a-bubble\"><u>unprecedented investment</u></a> and talent flooding into AI, the field has calcified around a single architectural approach, potentially blinding researchers to the next major breakthrough.</p><p>&quot;Despite the fact that there&#x27;s never been so much interest and resources and money and talent, this has somehow caused the narrowing of the research that we&#x27;re doing,&quot; Jones told the audience. The culprit, he argued, is the &quot;immense amount of pressure&quot; from investors demanding returns and researchers scrambling to stand out in an overcrowded field.</p><p>The warning carries particular weight given Jones&#x27;s role in AI history. The <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\"><u>transformer architecture</u></a> he helped develop at Google has become the foundation of the generative AI boom, enabling systems that can write essays, generate images, and engage in human-like conversation. His paper has been <a href=\"https://scholar.google.com/citations?user=_3_P5VwAAAAJ&amp;hl=en\"><u>cited more than 100,000 times</u></a>, making it one of the most influential computer science publications of the century.</p><p>Now, as CTO and co-founder of Tokyo-based <a href=\"https://sakana.ai/\"><u>Sakana AI</u></a>, Jones is explicitly abandoning his own creation. &quot;I personally made a decision in the beginning of this year that I&#x27;m going to drastically reduce the amount of time that I spend on transformers,&quot; he said. &quot;I&#x27;m explicitly now exploring and looking for the next big thing.&quot;</p><h2><b>Why more AI funding has led to less creative research, according to a transformer pioneer</b></h2><p>Jones painted a picture of an AI research community suffering from what he called a paradox: More resources have led to less creativity. He described researchers constantly checking whether they&#x27;ve been &quot;scooped&quot; by competitors working on identical ideas, and academics choosing safe, publishable projects over risky, potentially transformative ones.</p><p>&quot;If you&#x27;re doing standard AI research right now, you kind of have to assume that there&#x27;s maybe three or four other groups doing something very similar, or maybe exactly the same,&quot; Jones said, describing an environment where &quot;unfortunately, this pressure damages the science, because people are rushing their papers, and it&#x27;s reducing the amount of creativity.&quot;</p><p>He drew an analogy from AI itself — the &quot;exploration versus exploitation&quot; trade-off that governs how algorithms search for solutions. When a system exploits too much and explores too little, it finds mediocre local solutions while missing superior alternatives. &quot;We are almost certainly in that situation right now in the AI industry,&quot; Jones argued.</p><p>The implications are sobering. Jones recalled the period just before transformers emerged, when researchers were endlessly tweaking recurrent neural networks — the previous dominant architecture — for incremental gains. Once transformers arrived, all that work suddenly seemed irrelevant. &quot;How much time do you think those researchers would have spent trying to improve the recurrent neural network if they knew something like transformers was around the corner?&quot; he asked.</p><p>He worries the field is repeating that pattern. &quot;I&#x27;m worried that we&#x27;re in that situation right now where we&#x27;re just concentrating on one architecture and just permuting it and trying different things, where there might be a breakthrough just around the corner.&quot;</p><h2><b>How the &#x27;Attention is all you need&#x27; paper was born from freedom, not pressure</b></h2><p>To underscore his point, Jones described the conditions that allowed transformers to emerge in the first place — a stark contrast to today&#x27;s environment. The project, he said, was &quot;very organic, bottom up,&quot; born from &quot;talking over lunch or scrawling randomly on the whiteboard in the office.&quot;</p><p>Critically, &quot;we didn&#x27;t actually have a good idea, we had the freedom to actually spend time and go and work on it, and even more importantly, we didn&#x27;t have any pressure that was coming down from management,&quot; Jones recounted. &quot;No pressure to work on any particular project, publish a number of papers to push a certain metric up.&quot;</p><p>That freedom, Jones suggested, is largely absent today. Even researchers recruited for astronomical salaries — &quot;literally a million dollars a year, in some cases&quot; — may not feel empowered to take risks. &quot;Do you think that when they start their new position they feel empowered to try their wild ideas and more speculative ideas, or do they feel immense pressure to prove their worth and once again, go for the low hanging fruit?&quot; he asked.</p><h2><b>Why one AI lab is betting that research freedom beats million-dollar salaries</b></h2><p>Jones&#x27;s proposed solution is deliberately provocative: Turn up the &quot;explore dial&quot; and openly share findings, even at competitive cost. He acknowledged the irony of his position. &quot;It may sound a little controversial to hear one of the Transformers authors stand on stage and tell you that he&#x27;s absolutely sick of them, but it&#x27;s kind of fair enough, right? I&#x27;ve been working on them longer than anyone, with the possible exception of seven people.&quot;</p><p>At <a href=\"https://sakana.ai/\"><u>Sakana AI</u></a>, Jones said he&#x27;s attempting to recreate that pre-transformer environment, with nature-inspired research and minimal pressure to chase publications or compete directly with rivals. He offered researchers a mantra from engineer Brian Cheung: &quot;You should only do the research that wouldn&#x27;t happen if you weren&#x27;t doing it.&quot;</p><p>One example is Sakana&#x27;s &quot;<a href=\"https://sakana.ai/ctm/\"><u>continuous thought machine</u></a>,&quot; which incorporates brain-like synchronization into neural networks. An employee who pitched the idea told Jones he would have faced skepticism and pressure not to waste time at previous employers or academic positions. At Sakana, Jones gave him a week to explore. The project became successful enough to be spotlighted at <a href=\"https://neurips.cc/virtual/2025/poster/115192\"><u>NeurIPS</u></a>, a major AI conference.</p><p>Jones even suggested that freedom beats compensation in recruiting. &quot;It&#x27;s a really, really good way of getting talent,&quot; he said of the exploratory environment. &quot;Think about it, talented, intelligent people, ambitious people, will naturally seek out this kind of environment.&quot;</p><h2><b>The transformer&#x27;s success may be blocking AI&#x27;s next breakthrough</b></h2><p>Perhaps most provocatively, Jones suggested transformers may be victims of their own success. &quot;The fact that the current technology is so powerful and flexible... stopped us from looking for better,&quot; he said. &quot;It makes sense that if the current technology was worse, more people would be looking for better.&quot;</p><p>He was careful to clarify that he&#x27;s not dismissing ongoing transformer research. &quot;There&#x27;s still plenty of very important work to be done on current technology and bringing a lot of value in the coming years,&quot; he said. &quot;I&#x27;m just saying that given the amount of talent and resources that we have currently, we can afford to do a lot more.&quot;</p><p>His ultimate message was one of collaboration over competition. &quot;Genuinely, from my perspective, this is not a competition,&quot; Jones concluded. &quot;We all have the same goal. We all want to see this technology progress so that we can all benefit from it. So if we can all collectively turn up the explore dial and then openly share what we find, we can get to our goal much faster.&quot;</p><h2><b>The high stakes of AI&#x27;s exploration problem</b></h2><p>The remarks arrive at a pivotal moment for artificial intelligence. The industry grapples with mounting evidence that simply building larger transformer models <a href=\"https://www.wired.com/story/the-ai-industrys-scaling-obsession-is-headed-for-a-cliff/\"><u>may be approaching diminishing returns</u></a>. Leading researchers have begun openly discussing whether the current paradigm has fundamental limitations, with some suggesting that architectural innovations — not just scale — will be needed for continued progress toward more capable AI systems.</p><p>Jones&#x27;s warning suggests that finding those innovations may require dismantling the very incentive structures that have driven AI&#x27;s recent boom. With <a href=\"https://hai.stanford.edu/ai-index/2025-ai-index-report/economy\"><u>tens of billions of dollars flowing into AI development annually</u></a> and fierce competition among labs driving secrecy and rapid publication cycles, the exploratory research environment he described seems increasingly distant.</p><p>Yet his insider perspective carries unusual weight. As someone who helped create the technology now dominating the field, Jones understands both what it takes to achieve breakthrough innovation and what the industry risks by abandoning that approach. His decision to walk away from transformers — the architecture that made his reputation — adds credibility to a message that might otherwise sound like contrarian positioning.</p><p>Whether AI&#x27;s power players will heed the call remains uncertain. But Jones offered a pointed reminder of what&#x27;s at stake: The next transformer-scale breakthrough could be just around the corner, pursued by researchers with the freedom to explore. Or it could be languishing unexplored while thousands of researchers race to publish incremental improvements on architecture that, in Jones&#x27;s words, one of its creators is &quot;absolutely sick of.&quot;</p><p>After all, he&#x27;s been working on transformers longer than almost anyone. He would know when it&#x27;s time to move on.</p>",
    "published": "Thu, 23 Oct 2025 13:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-23T09:39:54.397184",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Research finds that 77% of data engineers have heavier workloads despite AI tools: Here's why and what to do about it",
    "link": "https://venturebeat.com/data-infrastructure/research-finds-that-77-of-data-engineers-have-heavier-workloads-despite-ai",
    "summary": "<p>Data engineers should be working faster than ever. AI-powered tools promise to automate pipeline optimization, accelerate data integration and handle the repetitive grunt work that has defined the profession for decades.</p><p>Yet, according to a new survey of 400 senior technology executives by MIT Technology Review Insights in partnership with <a href=\"https://www.snowflake.com/en/\">Snowflake</a>, 77% say their data engineering teams&#x27; workloads are getting heavier, not lighter.</p><p>The culprit? The very AI tools meant to help are creating a new set of problems.</p><p>While 83% of organizations have already deployed AI-based data engineering tools, 45% cite integration complexity as a top challenge. Another 38% are struggling with tool sprawl and fragmentation.</p><p>&quot;Many data engineers are using one tool to collect data, one tool to process data and another to run analytics on that data,&quot; Chris Child, VP of product for data engineering at <a href=\"https://venturebeat.com/ai/the-usd1-trillion-ai-problem-why-snowflake-tableau-and-blackrock-are-giving\">Snowflake</a>, told VentureBeat. &quot;Using several tools along this data lifecycle introduces complexity, risk and increased infrastructure management, which data engineers can&#x27;t afford to take on.&quot;</p><p>The result is a productivity paradox. AI tools are making individual tasks faster, but the proliferation of disconnected tools is making the overall system more complex to manage. For enterprises racing to deploy AI at scale, this fragmentation represents a critical bottleneck.</p><h2>From SQL queries to LLM pipelines: The daily workflow shift</h2><p>The survey found that data engineers spent an average of 19% of their time on AI projects two years ago. Today, that figure has jumped to 37%. Respondents expect it to hit 61% within two years.</p><p>But what does that shift actually look like in practice?</p><p>Child offered a concrete example. Previously, if the CFO of a company needed to make forecast predictions, they would tap the data engineering team to help build a system that correlates unstructured data like vendor contracts with structured data like revenue numbers into a static dashboard. Connecting these two worlds of different data types was extremely time-consuming and expensive, requiring lawyers to manually read through each document for key contract terms and upload that information into a database.</p><p>Today, that same workflow looks radically different.</p><p>&quot;Data engineers can use a tool like <a href=\"https://venturebeat.com/data-infrastructure/snowflakes-openflow-tackles-ais-toughest-engineering-challenge-data-ingestion-at-scale\">Snowflake Openflow</a> to seamlessly bring the unstructured PDF contracts living in a source like Box, together with the structured financial figures into a single platform like Snowflake, making the data accessible to LLMs,&quot; Child said. &quot;What used to take hours of manual work is now near instantaneous.&quot;</p><p>The shift isn&#x27;t just about speed. It&#x27;s about the nature of the work itself.</p><p>Two years ago, a typical data engineer&#x27;s day consisted of tuning clusters, writing SQL transformations and ensuring data readiness for human analysts. Today, that same engineer is more likely to be debugging LLM-powered transformation pipelines and setting up governance rules for AI model workflows.</p><p>&quot;Data engineers&#x27; core skill isn&#x27;t just coding,&quot; Child said. &quot;It&#x27;s orchestrating the data foundation and ensuring trust, context and governance so AI outputs are reliable.&quot;</p><h2>The tool stack problem: When help becomes hindrance</h2><p>Here&#x27;s where enterprises are getting stuck.</p><p>The promise of AI-powered data tools is compelling: automate pipeline optimization, accelerate debugging, streamline integration. But in practice, many organizations are discovering that each new AI tool they add creates its own integration headaches.</p><p>The survey data bears this out. While AI has led to improvements in output quantity (74% report increases) and quality (77% report improvements), those gains are being offset by the operational overhead of managing disconnected tools.</p><p>&quot;The other problem we&#x27;re seeing is that AI tools often make it easy to build a prototype by stitching together several data sources with an out-of-the-box LLM,&quot; Child said. &quot;But then when you want to take that into production, you realize that you don&#x27;t have the data accessible and you don&#x27;t know what governance you need, so it becomes difficult to roll the tool out to your users.&quot;</p><p>For technical decision-makers evaluating their data engineering stack right now, Child offered a clear framework. </p><p>&quot;Teams should prioritize AI tools that accelerate productivity, while at the same time eliminate infrastructure and operational complexity,&quot; he said. &quot;This allows engineers to move their focus away from managing the &#x27;glue work&#x27; of data engineering and closer to business outcomes.&quot;</p><h2>The agentic AI deployment window: 12 months to get it right</h2><p>The survey revealed that 54% of organizations plan to deploy agentic AI within the next 12 months. Agentic AI refers to autonomous agents that can make decisions and take actions without human intervention. Another 20% have already begun doing so.</p><p>For <a href=\"https://venturebeat.com/data-infrastructure/worlds-largest-open-source-multimodal-dataset-delivers-17x-training\">data engineering teams</a>, agentic AI represents both an enormous opportunity and a significant risk. Done right, autonomous agents can handle repetitive tasks like detecting schema drift or debugging transformation errors. Done wrong, they can corrupt datasets or expose sensitive information.</p><p>&quot;Data engineers must prioritize pipeline optimization and monitoring in order to truly deploy agentic AI at scale,&quot; Child said. &quot;It&#x27;s a low-risk, high-return starting point that allows agentic AI to safely automate repetitive tasks like detecting schema drift or debugging transformation errors when done correctly.&quot;</p><p>But Child was emphatic about the guardrails that must be in place first.</p><p>&quot;Before organizations let agents near production data, two safeguards must be in place: strong governance and lineage tracking, and active human oversight,&quot; he said. &quot;Agents must inherit fine-grained permissions and operate within an established governance framework.&quot;</p><p>The risks of skipping those steps are real. &quot;Without proper lineage or access governance, an agent could unintentionally corrupt datasets or expose sensitive information,&quot; Child warned.</p><h2>The perception gap that&#x27;s costing enterprises AI success</h2><p>Perhaps the most striking finding in the survey is a disconnect at the C-suite level.</p><p>While 80% of chief data officers and 82% of chief AI officers consider data engineers integral to business success, only 55% of CIOs share that view.</p><p>&quot;This shows that the data-forward leaders are seeing data engineering&#x27;s strategic value, but we need to do more work to help the rest of the C-suite recognize that investing in a unified, scalable data foundation and the people helping drive this is an investment in AI success, not just IT operations,&quot; Child said.</p><p>That perception gap has real consequences.</p><p>Data engineers in the surveyed organizations are already influential in decisions about AI use-case feasibility (53% of respondents) and business units&#x27; use of AI models (56%). But if CIOs don&#x27;t recognize data engineers as strategic partners, they&#x27;re unlikely to give those teams the resources, authority or seat at the table they need to prevent the kinds of tool sprawl and integration problems the survey identified.</p><p>The gap appears to correlate with visibility. Chief data officers and chief AI officers work directly with data engineering teams daily and understand the complexity of what they&#x27;re managing. CIOs, focused more broadly on infrastructure and operations, may not see the strategic architecture work that data engineers are increasingly doing.</p><p>This disconnect also shows up in how different executives rate the challenges facing data engineering teams. Chief AI officers are significantly more likely than CIOs to agree that data engineers&#x27; workloads are becoming increasingly heavy (93% vs. 75%). They&#x27;re also more likely to recognize data engineers&#x27; influence on overall AI strategy.</p><h2>What data engineers need to learn now</h2><p>The survey identified three critical skills data engineers need to develop: AI expertise, business acumen and communication abilities.</p><p>For an enterprise with a 20-person data engineering team, that presents a practical challenge. Do you hire for these skills, train existing engineers or restructure the team? Child&#x27;s answer suggested the priority should be business understanding.</p><p>&quot;The most important skill right now is for data engineers to understand what is critical to their end business users and prioritize how they can make those questions easier and faster to answer,&quot; he said.</p><p>The lesson for enterprises: Business context matters more than adding technical certifications. Child stressed that understanding the business impact of &#x27;why&#x27; data engineers are performing certain tasks will allow them to anticipate the needs of customers better, delivering value more immediately to the business.</p><p> &quot;The organizations with data engineering teams that prioritize this business understanding will set themselves apart from competition.&quot;</p><p>For enterprises looking to lead in AI, the solution to the data engineering productivity crisis isn&#x27;t more AI tools. The organizations that will move fastest are consolidating their tool stacks now, deploying governance infrastructure before agents go into production and elevating data engineers from support staff to strategic architects. </p><p>The window is narrow. With 54% planning agentic AI deployment within 12 months and data engineers expected to spend 61% of their time on AI projects within two years, teams that haven&#x27;t addressed tool sprawl and governance gaps will find their AI initiatives stuck in permanent pilot mode.</p>",
    "published": "Thu, 23 Oct 2025 13:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-23T09:39:54.397495",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "What enterprises can take away from Microsoft CEO Satya Nadella's shareholder letter",
    "link": "https://venturebeat.com/ai/what-enterprises-can-take-away-from-microsoft-ceo-satya-nadellas-shareholder",
    "summary": "<p>One of the leading architects of the current generative AI boom — Microsoft CEO Satya Nadella, famed for having the software giant take an early investment in OpenAI (and later saying he was &quot;<a href=\"https://www.geekwire.com/2025/im-good-for-my-80-billion-what-microsoft-ceo-satya-nadella-really-meant-by-his-stargate-zinger/\">good for my $80 billion</a>&quot;) — published his<a href=\"https://www.linkedin.com/pulse/my-annual-letter-thinking-decades-executing-quarters-satya-nadella-7orpc/\"> latest annual letter yesterday on LinkedIn</a> (a Microsoft subsidiary), and it&#x27;s chock full of interesting ideas about the near-term future that enterprise technical decision makers would do well to pay attention to, as it could aid in their own planning and tech stack development.</p><p>In a companion <a href=\"https://x.com/satyanadella/status/1980736083714535694\">post on X</a>, Nadella wrote, “AI is radically changing every layer of the tech stack, and we’re changing with it.&quot; </p><p>The full letter reinforces that message: Microsoft sees itself not just participating in the AI revolution, but shaping its infrastructure, security, tooling and governance for decades to come.</p><p>While the message is addressed to Microsoft shareholders, the implications reach much further. The letter is a strategic signal to enterprise engineering leaders: CIOs, CTOs, AI leads, platform architects and security directors. Nadella outlines the direction of Microsoft’s innovation, but also what it expects from its customers and partners. The AI era is here, but it will be built by those who combine technical vision with operational discipline.</p><p>Below are the five most important takeaways for enterprise technical decision makers.</p><h3><b>1. Security and reliability are now the foundation of the AI stack</b></h3><p>Nadella makes security the first priority in the letter and ties it directly to Microsoft’s relevance going forward. Through its Secure Future Initiative (SFI), Microsoft has assigned the equivalent of 34,000 engineers to secure its identity systems, networks and software supply chain. Its Quality Excellence Initiative (QEI) aims to increase platform resiliency and strengthen global service uptime.</p><p>Microsoft’s positioning makes it clear that enterprises will no longer get away with “ship fast, harden later” AI deployments. Nadella calls security “non-negotiable,” signaling that AI infrastructure must now meet the standards of mission-critical software. That means identity-first architecture, zero-trust execution environments and change management discipline are now table stakes for enterprise AI.</p><h3><b>2. AI infrastructure strategy is hybrid, open and sovereignty-ready</b></h3><p>Nadella commits Microsoft to building “planet-scale systems” and backs that up with numbers: more than 400 Azure datacenters across 70 regions, two gigawatts of new compute capacity added this year, and new liquid-cooled GPU clusters rolling out across Azure. Microsoft also introduced Fairwater, a massive new AI datacenter in Wisconsin positioned to deliver unprecedented scale. Just as important, Microsoft is now officially multi-model. Azure AI Foundry offers access to more than 11,000 models including OpenAI, Meta, Mistral, Cohere and xAI. Microsoft is no longer pushing a single-model future, but a hybrid AI strategy.</p><p>Enterprises should interpret this as validation of “portfolio architectures,” where closed, open and domain-specific models coexist. Nadella also emphasizes growing investment in sovereign cloud offerings for regulated industries, previewing a world where AI systems will have to meet regional data residency and compliance requirements from day one.</p><h3><b>3. AI agents—not just chatbots—are now Microsoft’s future</b></h3><p>The AI shift inside Microsoft is no longer about copilots that answer questions. It is now about AI agents that perform work. Nadella points to the rollout of Agent Mode in Microsoft 365 Copilot, which turns natural language requests into multistep business workflows. GitHub Copilot evolves from code autocomplete into a “peer programmer” capable of executing tasks asynchronously. In security operations, Microsoft has deployed AI agents that autonomously respond to incidents. In healthcare, Copilot for Dragon Medical documents clinical encounters automatically.</p><p>This represents a major architectural pivot. Enterprises will need to move beyond prompt-response interfaces and begin engineering agent ecosystems that safely take actions inside business systems. That requires workflow orchestration, API integration strategies and strong guardrails. Nadella’s letter frames this as the next software platform shift.</p><h3><b>4. Unified data platforms are required to unlock AI value</b></h3><p>Nadella devotes significant attention to <a href=\"https://venturebeat.com/data-infrastructure/enterprise-ai-success-is-about-more-than-just-data-its-about-knowledge-heres\">Microsoft Fabric</a> and OneLake, calling Fabric the company’s fastest-growing data and analytics product ever. Fabric promises to centralize enterprise data from multiple cloud and analytics environments. OneLake provides a universal storage layer that binds analytics and AI workloads together.</p><p>Microsoft’s message is blunt: siloed data means stalled AI. Enterprise teams that want AI at scale must unify operational and analytical data into a single architecture, enforce consistent data contracts and standardize metadata governance. AI success is now a data engineering problem more than a model problem.</p><h3><b>5. Trust, compliance and responsible AI are now mandatory for deployment</b></h3><p>“People want technology they can trust,” Nadella writes. Microsoft now publishes Responsible AI Transparency Reports and aligns parts of its development process with UN human rights guidance. Microsoft is also committing to digital resilience in Europe and proactive safeguards against misuse of AI-generated content.</p><p>This shifts responsible AI out of the realm of corporate messaging and into engineering practice. Enterprises will need model documentation, reproducibility practices, audit trails, risk monitoring and human-in-the-loop checkpoints. Nadella signals that compliance will become integrated with product delivery—not an afterthought layered on top.</p><h3><b>The real meaning of Microsoft’s AI strategy</b></h3><p>Taken together, these five pillars send a clear message to enterprise leaders: AI maturity is no longer about building prototypes or proving use cases. System-level readiness now defines success. Nadella frames Microsoft’s mission as helping customers “think in decades and execute in quarters,” and that is more than corporate poetry. It is a call to build AI platforms engineered for longevity.</p><p>The companies that win in enterprise AI will be the ones that invest early in secure cloud foundations, unify their data architectures, enable agent-based workflows and embrace responsible AI as a prerequisite for scale—not a press release. Nadella is betting that the next industrial transformation will be powered by AI infrastructure, not AI demos. With this letter, he has made Microsoft’s ambition clear: to become the platform on which that transformation is built.</p>",
    "published": "Thu, 23 Oct 2025 01:34:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-23T09:39:54.397747",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Kai-Fu Lee's brutal assessment: America is already losing the AI hardware war to China",
    "link": "https://venturebeat.com/ai/kai-fu-lees-brutal-assessment-america-is-already-losing-the-ai-hardware-war",
    "summary": "<p>China is on track to dominate consumer artificial intelligence applications and robotics manufacturing within years, but the United States will maintain its substantial lead in enterprise AI adoption and cutting-edge research, according to <a href=\"https://en.wikipedia.org/wiki/Kai-Fu_Lee\"><u>Kai-Fu Lee</u></a>, one of the world&#x27;s most prominent AI scientists and investors.</p><p>In a rare, unvarnished assessment delivered via video link from Beijing to the <a href=\"https://tedai-sanfrancisco.ted.com/\"><u>TED AI conference</u></a> in San Francisco Tuesday, Lee — a former executive at Apple, Microsoft, and Google who now runs both a major venture capital firm and his own AI company — laid out a technology landscape splitting along geographic and economic lines, with profound implications for both commercial competition and national security.</p><p>&quot;China&#x27;s robotics has the advantage of having integrated AI into much lower costs, better supply chain and fast turnaround, so companies like <a href=\"https://www.unitree.com/\"><u>Unitree</u></a> are actually the farthest ahead in the world in terms of building affordable, embodied humanoid AI,&quot; Lee said, referring to a Chinese robotics manufacturer that has undercut Western competitors on price while advancing capabilities.</p><p>The comments, made to a room filled with Silicon Valley executives, investors, and researchers, represented one of the most detailed public assessments from Lee about the comparative strengths and weaknesses of the world&#x27;s two AI superpowers — and suggested that the race for artificial intelligence leadership is becoming less a single contest than a series of parallel competitions with different winners.</p><h2><b>Why venture capital is flowing in opposite directions in the U.S. and China</b></h2><p>At the heart of Lee&#x27;s analysis lies a fundamental difference in how capital flows in the two countries&#x27; innovation ecosystems. American venture capitalists, Lee said, are pouring money into generative AI companies building large language models and enterprise software, while Chinese investors are betting heavily on robotics and hardware.</p><p>&quot;The VCs in the US don&#x27;t fund robotics the way the VCs do in China,&quot; Lee said. &quot;Just like the VCs in China don&#x27;t fund generative AI the way the VCs do in the US.&quot;</p><p>This investment divergence reflects different economic incentives and market structures. In the United States, where companies have grown accustomed to paying for software subscriptions and where labor costs are high, enterprise AI tools that boost white-collar productivity command premium prices. In China, where software subscription models have historically struggled to gain traction but manufacturing dominates the economy, robotics offers a clearer path to commercialization.</p><p>The result, Lee suggested, is that each country is pulling ahead in different domains — and may continue to do so.</p><p>&quot;China&#x27;s got some challenges to overcome in getting a company funded as well as OpenAI or Anthropic,&quot; Lee acknowledged, referring to the leading American AI labs. &quot;But I think U.S., on the flip side, will have trouble developing the investment interest and value creation in the robotics&quot; sector.</p><h2><b>Why American companies dominate enterprise AI while Chinese firms struggle with subscriptions</b></h2><p>Lee was explicit about one area where the United States maintains what appears to be a durable advantage: getting businesses to actually adopt and pay for AI software.</p><p>&quot;The enterprise adoption will clearly be led by the United States,&quot; Lee said. &quot;The Chinese companies have not yet developed a habit of paying for software on a subscription.&quot;</p><p>This seemingly mundane difference in business culture — whether companies will pay monthly fees for software — has become a critical factor in the AI race. The explosion of spending on tools like <a href=\"https://github.com/features/copilot\"><u>GitHub Copilot</u></a>, <a href=\"https://chatgpt.com/business/enterprise?utm_source=google&amp;utm_medium=paidsearch_brand&amp;utm_campaign=GOOG_B_SEM_GBR_Core_ENT_BAU_ACQ_PER_BRD_ALL_NAMER_US_EN_080625&amp;utm_term=chatgpt%20enterprise&amp;utm_content=182507886919&amp;utm_ad=779434575256&amp;utm_match=b&amp;gad_source=1&amp;gad_campaignid=22855802308&amp;gbraid=0AAAAA-I0E5deWS9iAj-S2JPixEaUT67Un&amp;gclid=CjwKCAjwgeLHBhBuEiwAL5gNEQgjDKgZm5up9BDA-oZ1HLMAECMm5XlfJerkJ9BbJgtkYf9GcAAQUhoCrskQAvD_BwE\"><u>ChatGPT Enterprise</u></a>, and other AI-powered productivity software has fueled American companies&#x27; ability to invest billions in further research and development.</p><p>Lee noted that China has historically overcome similar challenges in consumer technology by developing alternative business models. &quot;In the early days of internet software, China was also well behind because people weren&#x27;t willing to pay for software,&quot; he said. &quot;But then advertising models, e-commerce models really propelled China forward.&quot;</p><p>Still, he suggested, someone will need to &quot;find a new business model that isn&#x27;t just pay per software per use or per month basis. That&#x27;s going to not happen in China anytime soon.&quot;</p><p>The implication: American companies building enterprise AI tools have a window — perhaps a substantial one — where they can generate revenue and reinvest in R&amp;D without facing serious Chinese competition in their core market.</p><h2><b>How ByteDance, Alibaba and Tencent will outpace Meta and Google in consumer AI</b></h2><p>Where Lee sees China pulling ahead decisively is in consumer-facing AI applications — the kind embedded in social media, e-commerce, and entertainment platforms that billions of people use daily.</p><p>&quot;In terms of consumer usage, that&#x27;s likely to happen,&quot; Lee said, referring to China matching or surpassing the United States in AI deployment. &quot;The Chinese giants, like <a href=\"https://www.bytedance.com/en/\"><u>ByteDance</u></a> and <a href=\"https://www.alibaba.com/\"><u>Alibaba</u></a> and <a href=\"https://www.tencent.com/\"><u>Tencent</u></a>, will definitely move a lot faster than their equivalent in the United States, companies like <a href=\"https://www.meta.com/\"><u>Meta</u></a>, <a href=\"https://www.youtube.com/\"><u>YouTube</u></a> and so on.&quot;</p><p>Lee pointed to a cultural advantage: Chinese technology companies have spent the past decade obsessively optimizing for user engagement and product-market fit in brutally competitive markets. &quot;The Chinese giants really work tenaciously, and they have mastered the art of figuring out product market fit,&quot; he said. &quot;Now they have to add technology to it. So that is inevitably going to happen.&quot;</p><p>This assessment aligns with recent industry observations. ByteDance&#x27;s <a href=\"https://www.tiktok.com/en/\"><u>TikTok</u></a> became the world&#x27;s most downloaded app through sophisticated AI-driven content recommendation, and Chinese companies have pioneered AI-powered features in areas like live-streaming commerce and short-form video that Western companies later copied.</p><p>Lee also noted that China has already deployed AI more widely in certain domains. &quot;There are a lot of areas where China has also done a great job, such as using computer vision, speech recognition, and translation more widely,&quot; he said.</p><h2><b>The surprising open-source shift that has Chinese models beating Meta&#x27;s Llama</b></h2><p>Perhaps Lee&#x27;s most striking data point concerned <a href=\"https://venturebeat.com/ai/why-open-source-ai-became-an-american-national-priority\"><u>open-source AI development</u></a> — an area where China appears to have seized leadership from American companies in a remarkably short time.</p><p>&quot;The 10 highest rated open source [models] are from China,&quot; Lee said. &quot;These companies have now eclipsed Meta&#x27;s Llama, which used to be number one.&quot;</p><p>This represents a significant shift. Meta&#x27;s <a href=\"https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way\"><u>Llama models</u></a> were widely viewed as the gold standard for open-source large language models as recently as early 2024. But Chinese companies — including Lee&#x27;s own firm, <a href=\"http://01.ai\"><u>01.AI</u></a>, along with <a href=\"https://www.alibaba.com/\"><u>Alibaba</u></a>, <a href=\"https://www.baidu.com/\"><u>Baidu</u></a>, and others — have released a flood of open-source models that, according to various benchmarks, now outperform their American counterparts.</p><p>The open-source question has become a flashpoint in AI development. Lee made an extensive case for why open-source models will prove essential to the technology&#x27;s future, even as closed models from companies like OpenAI command higher prices and, often, superior performance.</p><p>&quot;I think open source has a number of major advantages,&quot; Lee argued. With open-source models, &quot;you can examine it, tune it, improve it. It&#x27;s yours, and it&#x27;s free, and it&#x27;s important for building if you want to build an application or tune the model to do something specific.&quot;</p><p>He drew an analogy to operating systems: &quot;People who work in operating systems loved Linux, and that&#x27;s why its adoption went through the roof. And I think in the future, open source will also allow people to tune a sovereign model for a country, make it work better for a particular language.&quot;</p><p>Still, Lee predicted both approaches will coexist. &quot;I don&#x27;t think open source models will win,&quot; he said. &quot;I think just like we have Apple, which is closed, but provides a somewhat better experience than Android... I think we&#x27;re going to see more apps using open-source models, more engineers wanting to build open-source models, but I think more money will remain in the closed model.&quot;</p><h2><b>Why China&#x27;s manufacturing advantage makes the robotics race &#x27;not over, but&#x27; nearly decided</b></h2><p>On robotics, Lee&#x27;s message was blunt: the combination of China&#x27;s manufacturing prowess, lower costs, and aggressive investment has created an advantage that will be difficult for American companies to overcome.</p><p>When asked directly whether the robotics race was already over with China victorious, Lee hedged only slightly. &quot;It&#x27;s not over, but I think the U.S. is still capable of coming up with the best robotic research ideas,&quot; he said. &quot;But the VCs in the U.S. don&#x27;t fund robotics the way the VCs do in China.&quot;</p><p>The challenge is structural. Building robots requires not just software and AI, but hardware manufacturing at scale — precisely the kind of integrated supply chain and low-cost production that China has spent decades perfecting. While American labs at universities and companies like <a href=\"https://bostondynamics.com/\"><u>Boston Dynamics</u></a> continue to produce impressive research prototypes, turning those prototypes into affordable commercial products requires the manufacturing ecosystem that China possesses.</p><p>Companies like <a href=\"https://www.unitree.com/\"><u>Unitree</u></a> have demonstrated this advantage concretely. The company&#x27;s humanoid robots and quadrupedal robots cost a fraction of their American-made equivalents while offering comparable or superior capabilities — a price-to-performance ratio that could prove decisive in commercial markets.</p><h2><b>What worries Lee most: not AGI, but the race itself</b></h2><p>Despite his generally measured tone about China&#x27;s AI development, Lee expressed concern about one area where he believes the global AI community faces real danger — not the far-future risk of superintelligent AI, but the near-term consequences of moving too fast.</p><p>When asked about <a href=\"https://venturebeat.com/ai/study-warns-of-security-risks-as-os-agents-gain-control-of-computers-and-phones\"><u>AGI risks</u></a>, Lee reframed the question. &quot;I&#x27;m less afraid of AI becoming self-aware and causing danger for humans in the short term,&quot; he said, &quot;but more worried about it being used by bad people to do terrible things, or by the AI race pushing people to work so hard, so fast and furious and move fast and break things that they build products that have problems and holes to be exploited.&quot;</p><p>He continued: &quot;I&#x27;m very worried about that. In fact, I think some terrible event will happen that will be a wake up call from this sort of problem.&quot;</p><p>Lee&#x27;s perspective carries unusual weight because of his unique vantage point spanning both Chinese and American AI development. Over a career spanning more than three decades, he has held senior positions at <a href=\"https://www.apple.com/\"><u>Apple</u></a>, <a href=\"https://www.microsoft.com/en-us/\"><u>Microsoft</u></a>, and <a href=\"https://www.google.com/?zx=1761178473681&amp;no_sw_cr=1\"><u>Google</u></a>, while also founding <a href=\"https://www.sinovationventures.com/\"><u>Sinovation Ventures</u></a>, which has invested in more than 400 companies across both countries. His AI company, <a href=\"http://01.ai\"><u>01.AI</u></a>, founded in 2023, has released several <a href=\"https://huggingface.co/01-ai\"><u>open-source models</u></a> that rank among the most capable in the world.</p><p>For American companies and policymakers, Lee&#x27;s analysis presents a complex strategic picture. The United States appears to have clear advantages in enterprise AI software, fundamental research, and computing infrastructure. But China is moving faster in consumer applications, manufacturing robotics at lower costs, and potentially pulling ahead in open-source model development.</p><p>The bifurcation suggests that rather than a single &quot;winner&quot; in AI, the world may be heading toward a technology landscape where different countries excel in different domains — with all the economic and geopolitical complications that implies.</p><p>As the <a href=\"https://tedai-sanfrancisco.ted.com/\"><u>TED AI conference</u></a> continued Wednesday, Lee&#x27;s assessment hung over subsequent discussions. His message seemed clear: the AI race is not one contest, but many — and the United States and China are each winning different races.</p><p>Standing in the conference hall afterward, one venture capitalist, who asked not to be named, summed up the mood in the room: &quot;We&#x27;re not competing with China anymore. We&#x27;re competing on parallel tracks.&quot; Whether those tracks eventually converge — or diverge into entirely separate technology ecosystems — may be the defining question of the next decade.</p>",
    "published": "Wed, 22 Oct 2025 12:30:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-23T09:39:54.398093",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Simplifying the AI stack: The key to scalable, portable intelligence from cloud to edge",
    "link": "https://venturebeat.com/ai/simplifying-the-ai-stack-the-key-to-scalable-portable-intelligence-from",
    "summary": "<p><i>Presented by Arm</i></p><hr /><p>A simpler software stack is the key to portable, scalable AI across cloud and edge.<b> </b></p><p>AI is now powering real-world applications, yet fragmented software stacks are holding it back. Developers routinely rebuild the same models for different hardware targets, losing time to glue code instead of shipping features. The good news is that a shift is underway. Unified toolchains and optimized libraries are making it possible to deploy models across platforms without compromising performance.</p><p>Yet one critical hurdle remains: software complexity. Disparate tools, hardware-specific optimizations, and layered tech stacks continue to bottleneck progress. To unlock the next wave of AI innovation, the industry must pivot decisively away from siloed development and toward streamlined, end-to-end platforms.</p><p>This transformation is already taking shape. Major cloud providers, edge platform vendors, and open-source communities are converging on unified toolchains that simplify development and accelerate deployment, from cloud to edge. In this article, we’ll explore why simplification is the key to scalable AI, what’s driving this momentum, and how next-gen platforms are turning that vision into real-world results.</p><h3><b>The bottleneck: fragmentation, complexity, and inefficiency</b></h3><p>The issue isn’t just hardware variety; it’s duplicated effort across frameworks and targets that slows time-to-value.</p><p><b>Diverse hardware targets</b>: GPUs, NPUs, CPU-only devices, mobile SoCs, and custom accelerators.</p><p><b>Tooling and framework fragmentation</b>: TensorFlow, PyTorch, ONNX, MediaPipe, and others.</p><p><b>Edge constraints</b>: Devices require real-time, energy-efficient performance with minimal overhead.</p><p>According to <a href=\"https://www.gartner.com/en/documents/3994810\">Gartner Research</a>, these mismatches create a key hurdle: over 60% of AI initiatives stall before production, driven by integration complexity and performance variability. </p><h3><b>What software simplification looks like</b></h3><p>Simplification is coalescing around five moves that cut re-engineering cost and risk:</p><p><b>Cross-platform abstraction layers</b> that minimize re-engineering when porting models.</p><p><b>Performance-tuned libraries</b> integrated into major ML frameworks.</p><p><b>Unified architectural designs</b> that scale from datacenter to mobile.</p><p><b>Open standards and runtimes</b> (e.g., ONNX, MLIR) reducing lock-in and improving compatibility.</p><p><b>Developer-first ecosystems</b> emphasizing speed, reproducibility, and scalability.</p><p>These shifts are making AI more accessible, especially for startups and academic teams that previously lacked the resources for bespoke optimization. Projects like Hugging Face’s Optimum and MLPerf benchmarks are also helping standardize and validate cross-hardware performance.</p><p><b>Ecosystem momentum and real-world signals</b> Simplification is no longer aspirational; it’s happening now. Across the industry, software considerations are influencing decisions at the IP and silicon design level, resulting in solutions that are production-ready from day one. Major ecosystem players are driving this shift by aligning hardware and software development efforts, delivering tighter integration across the stack.</p><p>A key catalyst is the rapid rise of edge inference, where AI models are deployed directly on devices rather than in the cloud. This has intensified demand for streamlined software stacks that support end-to-end optimization, from silicon to system to application. Companies like Arm are responding by enabling tighter coupling between their compute platforms and software toolchains, helping developers accelerate time-to-deployment without sacrificing performance or portability. The emergence of multi-modal and general-purpose foundation models (e.g., LLaMA, Gemini, Claude) has also added urgency. These models require flexible runtimes that can scale across cloud and edge environments. AI agents, which interact, adapt, and perform tasks autonomously, further drive the need for high-efficiency, cross-platform software.</p><p>MLPerf Inference v3.1 included over 13,500 performance results from 26 submitters, validating multi-platform benchmarking of AI workloads. Results spanned both data center and edge devices, demonstrating the diversity of optimized deployments now being tested and shared.</p><p>Taken together, these signals make clear that the market’s demand and incentives are aligning around a common set of priorities, including maximizing performance-per-watt, ensuring portability, minimizing latency, and delivering security and consistency at scale.</p><h3><b>What must happen for successful simplification</b></h3><p>To realize the promise of simplified AI platforms, several things must occur:</p><p><b>Strong hardware/software co-design</b>: hardware features that are exposed in software frameworks (e.g., matrix multipliers, accelerator instructions), and conversely, software that is designed to take advantage of underlying hardware.</p><p><b>Consistent, robust toolchains and libraries</b>: developers need reliable, well-documented libraries that work across devices. Performance portability is only useful if the tools are stable and well supported.</p><p><b>Open ecosystem</b>: hardware vendors, software framework maintainers, and model developers need to cooperate. Standards and shared projects help avoid re-inventing the wheel for every new device or use case.</p><p><b>Abstractions that don’t obscure performance</b>: while high-level abstraction helps developers, they must still allow tuning or visibility where needed. The right balance between abstraction and control is key.</p><p><b>Security, privacy, and trust built in</b>: especially as more compute shifts to devices (edge/mobile), issues like data protection, safe execution, model integrity, and privacy matter.</p><h3><b>Arm as one example of ecosystem-led simplification </b></h3><p>Simplifying AI at scale now hinges on system-wide design, where silicon, software, and developer tools evolve in lockstep. This approach enables AI workloads to run efficiently across diverse environments, from cloud inference clusters to battery-constrained edge devices. It also reduces the overhead of bespoke optimization, making it easier to bring new products to market faster. Arm (Nasdaq:Arm) is advancing this model with a platform-centric focus that pushes hardware-software optimizations up through the software stack. At <a href=\"https://newsroom.arm.com/blog/arm-computex-2025?utm_source=vb&amp;utm_medium=sponsored-content&amp;utm_content=longform_txt_na_sw-simplification&amp;utm_campaign=mk30_brand-paid_brand-tl_thirdparty_mediabuy_na\">COMPUTEX 2025</a>, Arm demonstrated how its latest Arm9 CPUs, combined with AI-specific ISA extensions and the Kleidi libraries, enable tighter integration with widely used frameworks like PyTorch, ExecuTorch, ONNX Runtime, and MediaPipe. This alignment reduces the need for custom kernels or hand-tuned operators, allowing developers to unlock hardware performance without abandoning familiar toolchains. </p><p>The real-world implications are significant. In the data center, Arm-based platforms are delivering improved performance-per-watt, critical for scaling AI workloads sustainably. On consumer devices, these optimizations enable ultra-responsive user experiences and background intelligence that’s always on, yet power efficient.</p><p>More broadly, the industry is coalescing around simplification as a design imperative, embedding AI support directly into hardware roadmaps, optimizing for software portability, and standardizing support for mainstream AI runtimes. Arm’s approach illustrates how deep integration across the compute stack can make scalable AI a practical reality.</p><h3><b>Market validation and momentum</b></h3><p>In 2025, <a href=\"https://newsroom.arm.com/blog/half-of-compute-shipped-to-top-hyperscalers-in-2025-will-be-arm-based?utm_source=vb&amp;utm_medium=sponsored-content&amp;utm_content=longform_txt_na_sw-simplification&amp;utm_campaign=mk30_brand-paid_brand-tl_thirdparty_mediabuy_na\">nearly half of the compute shipped to major hyperscalers will run on Arm-based architectures</a>, a milestone that underscores a significant shift in cloud infrastructure. As AI workloads become more resource-intensive, cloud providers are prioritizing architectures that deliver superior performance-per-watt and support seamless software portability. This evolution marks a strategic pivot toward energy-efficient, scalable infrastructure optimized for the performance and demands of modern AI.</p><p>At the edge, Arm-compatible inference engines are enabling real-time experiences, such as live translation and always-on voice assistants, on battery-powered devices. These advancements bring powerful AI capabilities directly to users, without sacrificing energy efficiency.</p><p>Developer momentum is accelerating as well. In a recent collaboration, GitHub and Arm introduced native Arm Linux and Windows runners for GitHub Actions, streamlining CI workflows for Arm-based platforms. These tools lower the barrier to entry for developers and enable more efficient, cross-platform development at scale. </p><h3><b>What comes next</b></h3><p>Simplification doesn’t mean removing complexity entirely; it means managing it in ways that empower innovation. As the AI stack stabilizes, winners will be those who deliver seamless performance across a fragmented landscape.</p><p>From a future-facing perspective, expect:</p><p><b>Benchmarks as guardrails:</b> MLPerf + OSS suites guide where to optimize next.</p><p><b>More upstream, fewer forks:</b> Hardware features land in mainstream tools, not custom branches.</p><p><b>Convergence of research + production:</b> Faster handoff from papers to product via shared runtimes.</p><h2><b>Conclusion</b></h2><p>AI’s next phase isn’t about exotic hardware; it’s also about software that travels well. When the same model lands efficiently on cloud, client, and edge, teams ship faster and spend less time rebuilding the stack.</p><p>Ecosystem-wide simplification, not brand-led slogans, will separate the winners. The practical playbook is clear: unify platforms, upstream optimizations, and measure with open benchmarks. <a href=\"https://www.arm.com/markets/artificial-intelligence/software?utm_source=vb&amp;utm_medium=sponsored-content&amp;utm_content=longform_txt_na_sw-simplification&amp;utm_campaign=mk30_brand-paid_brand-tl_thirdparty_mediabuy_na\">Explore how Arm AI software platforms</a> are enabling this future — efficiently, securely, and at scale.</p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p><p>\n</p>",
    "published": "Wed, 22 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-23T09:39:54.398398",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "Qwen's new Deep Research update lets you turn its reports into webpages, podcasts in seconds",
    "link": "https://venturebeat.com/ai/qwens-new-deep-research-update-lets-you-turn-its-reports-into-webpages",
    "summary": "<p>Chinese e-commerce giant Alibaba’s <a href=\"https://venturebeat.com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks\">famously prolific Qwen Team</a> of AI model researchers and engineers has introduced a major expansion to its Qwen Deep Research tool, which is available as an optional modality the user can activate on the web-based Qwen Chat (a competitor to ChatGPT).</p><p>The update lets users generate not only comprehensive research reports with well-organized citations, but also interactive web pages and multi-speaker podcasts — all within 1-2 clicks.</p><p>This functionality is part of a <b>proprietary release</b>, distinct from many of Qwen’s previous open-source model offerings. </p><p>While the feature relies on the open-source models <b>Qwen3-Coder</b>, <b>Qwen-Image</b>, and <b>Qwen3-TTS</b> to power its core capabilities, the end-to-end experience — including research execution, web deployment, and audio generation — is <b>hosted and operated by Qwen</b>. </p><p>This means users benefit from a managed, integrated workflow without needing to configure infrastructure. That said, developers with access to the open-source models could theoretically replicate similar functionality on private or commercial systems.</p><p>The update was announced via the team’s official<a href=\"https://x.com/Alibaba_Qwen/status/1980609551486624237\"> X account (@Alibaba_Qwen)</a> today, October 21, 2025, stating:</p><blockquote><p>“Qwen Deep Research just got a major upgrade. It now creates not only the report, but also a live webpage and a podcast — powered by Qwen3-Coder, Qwen-Image, and Qwen3-TTS. Your insights, now visual and audible.”</p></blockquote><h3><b>Multi-Format Research Output</b></h3><p>The core workflow begins with a user request inside the Qwen Chat interface. From there, Qwen collaborates by asking clarifying questions to shape the research scope, pulls data from the web and official sources, and analyzes or resolves any inconsistencies it finds — even generating custom code when needed.</p><p>A <a href=\"https://x.com/Alibaba_Qwen/status/1980609551486624237\">demo video posted by Qwen on X</a> walks through this process on Qwen Chat using the U.S. SaaS market as an example. </p><p>In it, Qwen retrieves data from multiple industry sources, identifies discrepancies in market size estimates (e.g., $206 billion vs. $253 billion), and highlights ambiguities in the U.S. share of global figures. The assistant comments on differences in scope between sources and calculates a compound annual growth rate (CAGR) of 19.8% from 2020 to 2023, providing contextual analysis to back up the raw numbers.</p><p>Once the research is complete, users can click on the &quot;eyeball&quot; icon below the output result (see screenshot), which will bring up a PDF-style report in the right hand pane.</p><p>Then, when viewing the report in the right-hand pane, the user can click the &quot;Create&quot; button in the upper-right hand corner and select from the following two options:</p><ol><li><p><b>&quot;Web Dev&quot; </b>which produces a <b>live, professional-grade web page</b>, automatically deployed and <b>hosted by Qwen</b>, using Qwen3-Coder for structure and Qwen-Image for visuals.</p></li><li><p>&quot;<b>Podcast</b>,&quot; which, as it states, produces an audio <b>podcast</b>, featuring dynamic, multi-speaker narration generated by Qwen3-TTS, also <b>hosted by Qwen</b> for easy sharing and playback.</p></li></ol><p>This enables users to quickly convert a single research project into multiple forms of content — written, visual, and audible — with minimal extra input.</p><p>The website includes inline graphics generated by Qwen Image, making it suitable for use in public presentations, classrooms, or publishing. </p><p>The podcast feature allows users to select between 17 different speaker names as the host and 7 as the co-host, though I wasn&#x27;t able to find a way to preview the voice outputs before selecting them. It appears designed for deep listening on the go. </p><p>There was no way to change the language output that I could see, so mine came out in English, like my reports and initial prompts, though the Qwen LLMs are multi-modal. The voices were slightly more robotic than other AI tools I&#x27;ve used.</p><p>Here&#x27;s an example of a web page I generated <a href=\"https://chat.qwen.ai/s/deploy/65743dcf-7e0e-455b-b430-5004c8f36841\">on commonalities in authoritarian regimes throughout history</a>, <a href=\"https://chat.qwen.ai/s/deploy/caf2033e-725b-43dc-b4d0-721063728774\">another one on UFO or UAP sightings</a>, and below this paragraph, a podcast on UFO or UAP sightings. </p><p>While the website is hosted via a public link, the podcast must be downloaded by the user and can&#x27;t be linked to publicly, from what I could tell in my brief usage so far.</p><p>Note the podcast is much different than the actual report — not just a straight read-through audio version of it, rather, a new format of two hosts discussing and bantering about the subject using the report as the jumping off point. </p><p>The web page versions of the report also include new graphics not found in the PDF report.</p><h3><b>Comparisons to Google&#x27;s NotebookLM</b></h3><p>While the new capabilities have been well received by many early users, comparisons to other research assistants have surfaced — particularly Google’s <b>NotebookLM</b>, which recently exited beta.</p><p>AI commentator and newsletter writer <a href=\"https://x.com/kimmonismus/status/1980612332767072444\">Chubby (@kimmonismus) noted on X</a>:</p><blockquote><p>“I am really grateful that Qwen provides regular updates. That’s great.</p></blockquote><blockquote><p>But the attempt to build a NotebookLM clone inside Qwen-3-max doesn’t sound very promising compared to Google’s version.”</p></blockquote><p>While NotebookLM is built around organizing and querying existing documents and web pages, Qwen Deep Research focuses more on <b>generating new research content from scratch</b>, aggregating sources from the open web, and presenting it across multiple modalities. </p><p>The comparison suggests that while the two tools overlap in general concept — AI-assisted research — they diverge in approach and target user experience.</p><h3><b>Availability</b></h3><p>Qwen Deep Research is now live and available through the <b>Qwen Chat app</b>. The feature can be accessed with <a href=\"https://chat.qwen.ai/?inputFeature=deep_research\">the following URL.</a></p><p>No pricing details have been provided for Qwen3-Max or the specific Deep Research capabilities as of this writing.</p><h3><b>What&#x27;s Next For Qwen Deep Research?</b></h3><p>By combining research guidance, data analysis, and multi-format content creation into a single tool, Qwen Deep Research aims to streamline the path from idea to publishable output. </p><p>The integration of code, visuals, and voice makes it especially attractive to content creators, educators, and independent analysts who want to scale their research into web- or podcast-friendly forms without switching platforms.</p><p>Still, comparisons to more specialized offerings like NotebookLM raise questions about how Qwen’s generalized approach stacks up on depth, precision, and refinement. Whether the strength of its multi-format execution outweighs those concerns may come down to user priorities — and whether they value single-click publishing over tight integration with existing notes and materials.</p><p>For now, Qwen is signaling that research doesn’t end with a document — it begins with one.</p><p>Let me know if you want this repackaged into something shorter or tailored to a particular audience — newsletter, press-style blog, internal team explainer, etc.</p>",
    "published": "Tue, 21 Oct 2025 18:32:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-23T09:39:54.398640",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "DeepSeek drops open-source model that compresses text 10x through images, defying conventions",
    "link": "https://venturebeat.com/ai/deepseek-drops-open-source-model-that-compresses-text-10x-through-images",
    "summary": "<p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a>, the Chinese artificial intelligence research company that has repeatedly challenged assumptions about <a href=\"https://www.reuters.com/technology/artificial-intelligence/big-tech-faces-heat-chinas-deepseek-sows-doubts-billion-dollar-spending-2025-01-27/\"><u>AI development costs</u></a>, has released a <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>new model</u></a> that fundamentally reimagines how large language models process information—and the implications extend far beyond its modest branding as an optical character recognition tool.</p><p>The company&#x27;s <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR model</u></a>, released Monday with full <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>open-source code</u></a> and <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>weights</u></a>, achieves what researchers describe as a paradigm inversion: compressing text through visual representation up to 10 times more efficiently than traditional text tokens. The finding challenges a core assumption in AI development and could pave the way for language models with dramatically expanded context windows, potentially reaching tens of millions of tokens.</p><p>&quot;We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping,&quot; the research team wrote in their <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>technical paper</u></a>. &quot;Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio &lt; 10×), the model can achieve decoding (OCR) precision of 97%.&quot;</p><p>The implications have resonated across the AI research community. <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Andrej Karpathy</u></a>, co-founder of OpenAI and former director of AI at Tesla, said in a post that the work raises fundamental questions about how AI systems should process information. &quot;Maybe it makes more sense that all inputs to LLMs should only ever be images,&quot; Karpathy wrote. &quot;Even if you happen to have pure text input, maybe you&#x27;d prefer to render it and then feed that in.&quot;</p><h2><b>How DeepSeek achieved 10x compression by treating text as images</b></h2><p>While <a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> marketed the release as an <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>OCR model</u></a> — a technology for converting images of text into digital characters — the research paper reveals more ambitious goals. The model demonstrates that visual representations can serve as a superior compression medium for textual information, inverting the conventional hierarchy where text tokens were considered more efficient than vision tokens.</p><p>&quot;Traditionally, vision LLM tokens almost seemed like an afterthought or &#x27;bolt on&#x27; to the LLM paradigm,&quot; wrote <a href=\"https://x.com/doodlestein/status/1980282222893535376\"><u>Jeffrey Emanuel</u></a>, an AI researcher, in a detailed analysis of the paper. &quot;And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens...But that gets inverted now from the ideas in this paper.&quot;</p><p>The model&#x27;s architecture consists of two primary components: <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepEncoder</u></a>, a novel 380-million-parameter vision encoder, and a 3-billion-parameter mixture-of-experts language decoder with 570 million activated parameters. DeepEncoder combines Meta&#x27;s <a href=\"https://segment-anything.com/\"><u>Segment Anything Model (SAM)</u></a> for local visual perception with <a href=\"https://openai.com/index/clip/\"><u>OpenAI&#x27;s CLIP model</u></a> for global visual understanding, connected through a 16x compression module.</p><p>To validate their compression claims, DeepSeek researchers tested the model on the <a href=\"https://github.com/ucaslcl/Fox\"><u>Fox benchmark</u></a>, a dataset of diverse document layouts. The results were striking: using just 100 vision tokens, the model achieved 97.3% accuracy on documents containing 700-800 text tokens — representing an effective compression ratio of 7.5x. Even at compression ratios approaching 20x, accuracy remained around 60%.</p><h2><b>The practical impact: Processing 200,000 pages per day on a single GPU</b></h2><p>The efficiency gains translate directly to production capabilities. According to the company, a single <a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><u>Nvidia A100-40G GPU</u></a> can process more than 200,000 pages per day using <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR</u></a>. Scaling to a cluster of 20 servers with eight GPUs each, throughput reaches 33 million pages daily — sufficient to rapidly construct training datasets for other AI models.</p><p>On <a href=\"https://github.com/opendatalab/OmniDocBench\"><u>OmniDocBench</u></a>, a comprehensive document parsing benchmark, <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR</u></a> outperformed GOT-OCR2.0 (which uses 256 tokens per page) while using only 100 vision tokens. More dramatically, it surpassed MinerU2.0 — which requires more than 6,000 tokens per page on average — while using fewer than 800 vision tokens.</p><p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> designed the model to support five distinct resolution modes, each optimized for different compression ratios and use cases. The &quot;Tiny&quot; mode operates at 512×512 resolution with just 64 vision tokens, while &quot;Gundam&quot; mode combines multiple resolutions dynamically for complex documents. &quot;Gundam mode consists of n×640×640 tiles (local views) and a 1024×1024 global view,&quot; the researchers wrote.</p><h2><b>Why this breakthrough could unlock 10 million token context windows</b></h2><p>The compression breakthrough has immediate implications for one of the most pressing challenges in AI development: expanding the context windows that determine how much information language models can actively consider. Current state-of-the-art models typically handle context windows measured in hundreds of thousands of tokens. DeepSeek&#x27;s approach suggests a path to windows ten times larger.</p><p>&quot;The potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting,&quot; <a href=\"https://x.com/doodlestein/status/1980282222893535376\"><u>Emanuel wrote</u></a>. &quot;You could basically cram all of a company&#x27;s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective.&quot;</p><p>The researchers explicitly frame their work in terms of context compression for language models. &quot;Through DeepSeek-OCR, we demonstrate that vision-text compression can achieve significant token reduction (7-20×) for different historical context stages, offering a promising direction for addressing long-context challenges in large language models,&quot; <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>they wrote</u></a>.</p><p><a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>The paper</u></a> includes a speculative but intriguing diagram illustrating how the approach could implement memory decay mechanisms similar to human cognition. Older conversation rounds could be progressively downsampled to lower resolutions, consuming fewer tokens while maintaining key information — a form of computational forgetting that mirrors biological memory.</p><h2><b>How visual processing could eliminate the &#x27;ugly&#x27; tokenizer problem</b></h2><p>Beyond compression, Karpathy highlighted how the approach challenges fundamental assumptions about how language models should process text. Traditional tokenizers—the systems that break text into units for processing—have long been criticized for their complexity and limitations.</p><p>&quot;I already ranted about how much I dislike the tokenizer,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Karpathy wrote</u></a>. &quot;Tokenizers are ugly, separate, not end-to-end stage. It &#x27;imports&#x27; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network.&quot;</p><p>Visual processing of text could eliminate these issues while enabling new capabilities. The approach naturally handles formatting information lost in pure text representations: bold text, colors, layout, embedded images. &quot;Input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful,&quot; Karpathy noted.</p><p>The implications resonate with human cognitive science. <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel drew a parallel to Hans Bethe</u></a>, the renowned physicist who memorized vast amounts of reference data: &quot;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&quot;</p><h2><b>The model&#x27;s training: 30 million PDF pages across 100 languages</b></h2><p>The model&#x27;s capabilities rest on an extensive training regimen using diverse data sources. DeepSeek collected 30 million PDF pages covering approximately 100 languages, with Chinese and English accounting for 25 million pages. The training data spans nine document types — academic papers, financial reports, textbooks, newspapers, handwritten notes, and others.</p><p>Beyond document OCR, the training incorporated what the researchers call &quot;OCR 2.0&quot; data: 10 million synthetic charts, 5 million chemical formulas, and 1 million geometric figures. The model also received 20% general vision data for tasks like image captioning and object detection, plus 10% text-only data to maintain language capabilities.</p><p>The training process employed pipeline parallelism across 160 <a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><u>Nvidia A100-40G GPUs</u></a> (20 nodes with 8 GPUs each), with the vision encoder divided between two pipeline stages and the language model split across two others. &quot;For multimodal data, the training speed is 70B tokens/day,&quot; the researchers reported.</p><h2><b>Open source release accelerates research and raises competitive questions</b></h2><p>True to DeepSeek&#x27;s pattern of open development, the company released the complete model weights, training code, and inference scripts on <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>GitHub</u></a> and <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>Hugging Face</u></a>. The GitHub repository gained over 4,000 stars within 24 hours of release, according to Dataconomy.</p><p>The breakthrough raises questions about whether other AI labs have developed similar techniques but kept them proprietary. Emanuel speculated that Google&#x27;s Gemini models, which feature large context windows and strong OCR performance, might employ comparable approaches. &quot;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel wrote</u></a>.</p><p>Google&#x27;s <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\"><u>Gemini 2.5 Pro</u></a> offers a 1-million-token context window, with plans to expand to 2 million, though the company has not publicly detailed the technical approaches enabling this capability. OpenAI&#x27;s <a href=\"https://openai.com/index/introducing-gpt-5/\"><u>GPT-5</u></a> supports 400,000 tokens, while Anthropic&#x27;s <a href=\"https://www.anthropic.com/news/claude-sonnet-4-5\"><u>Claude 4.5</u></a> offers 200,000 tokens, with a 1-million-token window available in beta for eligible organizations.</p><h2><b>The unanswered question: Can AI reason over compressed visual tokens?</b></h2><p>While the compression results are impressive, researchers acknowledge important open questions. &quot;It&#x27;s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel noted</u></a>. &quot;Can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality?&quot;</p><p>The <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepSeek paper</u></a> focuses primarily on the compression-decompression capability, measured through OCR accuracy, rather than downstream reasoning performance. This leaves open whether language models could reason effectively over large contexts represented primarily as compressed visual tokens.</p><p>The researchers acknowledge their work represents &quot;an initial exploration into the boundaries of vision-text compression.&quot; They note that &quot;OCR alone is insufficient to fully validate true context optical compression&quot; and plan future work including &quot;digital-optical text interleaved pretraining, needle-in-a-haystack testing, and other evaluations.&quot;</p><p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> has established a pattern of achieving competitive results with dramatically lower computational resources than Western AI labs. The company&#x27;s earlier <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3\"><u>DeepSeek-V3 model</u></a> reportedly cost <a href=\"https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/\"><u>just $5.6 million to train</u></a>—though this figure represents only the final training run and excludes R&amp;D and infrastructure costs—compared to hundreds of millions for comparable models from OpenAI and Anthropic.</p><p>Industry analysts have questioned the $5.6 million figure, with some estimates placing the company&#x27;s total infrastructure and operational costs <a href=\"https://www.cnbc.com/2025/01/30/chinas-deepseek-has-some-big-ai-claims-not-all-experts-are-convinced-.html\"><u>closer to $1.3 billion</u></a>, though still lower than American competitors&#x27; spending.</p><h2><b>The bigger picture: Should language models process text as images?</b></h2><p><a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepSeek-OCR</u></a> poses a fundamental question for AI development: should language models process text as text, or as images of text? The research demonstrates that, at least for compression purposes, visual representation offers significant advantages. Whether this translates to effective reasoning over vast contexts remains to be determined.</p><p>&quot;From another perspective, optical contexts compression still offers substantial room for research and improvement, representing a promising new direction,&quot; the researchers concluded<a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u> in their paper</u></a>.</p><p>For the AI industry, the work adds another dimension to the race for longer context windows — a competition that has intensified as language models are applied to increasingly complex tasks requiring vast amounts of information. The open-source release ensures the technique will be widely explored, tested, and potentially integrated into future AI systems.</p><p>As Karpathy framed the deeper implication: &quot;OCR is just one of many useful vision -&gt; text tasks. And text -&gt; text tasks can be made to be vision -&gt;text tasks. Not vice versa.&quot; In other words, the path forward for AI might not run through better tokenizers — it might bypass text tokens altogether.</p><p>\n</p>",
    "published": "Tue, 21 Oct 2025 18:30:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-23T09:39:54.399031",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Google's new vibe coding AI Studio experience lets anyone build, deploy apps live in minutes",
    "link": "https://venturebeat.com/ai/googles-new-vibe-coding-ai-studio-experience-lets-anyone-build-deploy-apps",
    "summary": "<p>Google AI Studio has gotten a big vibe coding upgrade with a new interface, buttons, suggestions and community features that allow anyone with an idea for an app — even complete novices, laypeople, or non-developers like yours truly — to bring it into existence and deploy it live, on the web, for anyone to use, within <i>minutes</i>.</p><p>The updated Build tab is available now at <a href=\"http://ai.studio/build\">ai.studio/build</a>, and it’s free to start. </p><p>Users can experiment with building applications without needing to enter payment information upfront, though certain advanced features like Veo 3.1 and Cloud Run deployment require a paid API key.</p><p>The new features appear to me to make Google&#x27;s AI models and offerings even more competitive, perhaps preferred, for many general users to dedicated AI startup rivals like Anthropic&#x27;s Claude Code and OpenAI&#x27;s Codex, respectively, two &quot;vibe coding&quot; focused products that are beloved by developers — but seem to have a higher barrier to entry or may require more technical know-how.</p><h3><b>A Fresh Start: Redesigned Build Mode</b></h3><p>The updated Build tab serves as the entry point to vibe coding. It introduces a new layout and workflow where users can select from Google’s suite of AI models and features to power their applications. The default is Gemini 2.5 Pro, which is great for most cases.</p><p>Once selections are made, users simply describe what they want to build, and the system automatically assembles the necessary components using Gemini’s APIs.</p><p>This mode supports mixing capabilities like Nano Banana (a lightweight AI model), Veo (for video understanding), Imagine (for image generation), Flashlight (for performance-optimized inference), and Google Search.</p><p>Patrick Löber, Developer Relations at Google DeepMind, highlighted that the experience is meant to help users “supercharge your apps with AI” using a simple prompt-to-app pipeline.</p><p>In a video demo he posted on X and LinedIn, he showed how just a few clicks led to the automatic generation of a garden planning assistant app, complete with layouts, visuals, and a conversational interface.</p><div></div><h3><b>From Prompt to Production: Building and Editing in Real Time</b></h3><p>Once an app is generated, users land in a fully interactive editor. On the left, there’s a traditional code-assist interface where developers can chat with the AI model for help or suggestions. On the right, a code editor displays the full source of the app.</p><p>Each component—such as React entry points, API calls, or styling files—can be edited directly. Tooltips help users understand what each file does, which is especially useful for those less familiar with TypeScript or frontend frameworks.</p><p>Apps can be saved to GitHub, downloaded locally, or shared directly. Deployment is possible within the Studio environment or via Cloud Run if advanced scaling or hosting is needed.</p><h3><b>Inspiration on Demand: The ‘I’m Feeling Lucky’ Button</b></h3><p>One standout feature in this update is the “I’m Feeling Lucky” button. Designed for users who need a creative jumpstart, it generates randomized app concepts and configures the app setup accordingly. Each press yields a different idea, complete with suggested AI features and components.</p><p>Examples produced during demos include:</p><ul><li><p>An interactive map-based chatbot powered by Google Search and conversational AI.</p></li><li><p>A dream garden designer using image generation and advanced planning tools.</p></li><li><p>A trivia game app with an AI host whose personality users can define, integrating both Imagine and Flashlight with Gemini 2.5 Pro for conversation and reasoning.</p></li></ul><p>Logan Kilpatrick, Lead of Product for Google AI Studio and Gemini AI, noted in a demo video of his own that this feature encourages discovery and experimentation. </p><p>“You get some really, really cool, different experiences,” he said, emphasizing its role in helping users find novel ideas quickly.</p><div></div><h3><b>Hands-On Test: From Prompt to App in 65 Seconds</b></h3><p>To test the new workflow, I prompted Gemini with:</p><p><i>A randomized dice rolling web application where the user can select between common dice sizes (6 sides, 10 sides, etc) and then see an animated die rolling and choose the color of their die as well.</i></p><p><b>Within 65 seconds (just over a minute) AI Studio returned a fully working web app</b> featuring:</p><ul><li><p>Dice size selector (d4, d6, d8, d10, d12, d20)</p></li><li><p>Color customization options for the die</p></li><li><p>Animated rolling effect with randomized results</p></li><li><p>Clean, modern UI built with React, TypeScript, and Tailwind CSS</p></li></ul><p>The platform also generated a complete set of structured files, including App.tsx, constants.ts, and separate components for dice logic and controls. </p><p>After generation, it was easy to iterate: adding sound effects for each interaction (rolling, choosing a die, changing color) required only a single follow-up prompt to the built-in assistant. This was also suggested by Gemini, too, by the way. </p><p>From there, the app can be previewed live or exported using built-in controls to:</p><ul><li><p>Save to GitHub</p></li><li><p>Download the full codebase</p></li><li><p>Copy the project for remixing</p></li><li><p>Deploy via integrated tools</p></li></ul><p>My brief, hands-on test showed just how quickly even small utility apps can go from idea to interactive prototype—without leaving the browser or writing boilerplate code manually.</p><h3><b>AI-Suggested Enhancements and Feature Refinement</b></h3><p>In addition to code generation, Google AI Studio now offers context-aware feature suggestions. These recommendations, generated by Gemini’s Flashlight capability, analyze the current app and propose relevant improvements.</p><p>In one example, the system suggested implementing a feature that displays the history of previously generated images in an image studio tab. These iterative enhancements allow builders to expand app functionality over time without starting from scratch.</p><p>Kilpatrick emphasized that users can continue to refine their projects as they go, combining both automatic generation and manual adjustments. “You can go in and continue to edit and sort of refine the experience that you want iteratively,” he said.</p><h3><b>Free to Start, Flexible to Grow</b></h3><p>The new experience is available at no cost for users who want to experiment, prototype, or build lightweight apps. There’s no requirement to enter credit card information to begin using vibe coding.</p><p>However, more powerful capabilities — such as using models like Veo 3.1 or deploying through Cloud Run — do require switching to a paid API key.</p><p>This pricing structure is intended to lower the barrier to entry for experimentation while providing a clear path to scale when needed.</p><h3><b>Built for All Skill Levels</b></h3><p>One of the central goals of the vibe coding launch is to make AI app development accessible to more people. The system supports both high-level visual builders and low-level code editing, creating a workflow that works for developers across experience levels.</p><p>Kilpatrick mentioned that while he’s more familiar with Python than TypeScript, he still found the editor useful because of the helpful file descriptions and intuitive layout. </p><p>This focus on usability could make AI Studio a compelling option for developers exploring AI for the first time.</p><h3><b>More to Come: A Week of Launches</b></h3><p>The launch of vibe coding is the first in a series of announcements expected throughout the week. While specific future features haven’t been revealed yet, both Kilpatrick and Löber hinted that additional updates are on the way.</p><p>With this update, Google AI Studio positions itself as a flexible, user-friendly environment for building AI-powered applications—whether for fun, prototyping, or production deployment. The focus is clear: make the power of Gemini’s APIs accessible without unnecessary complexity.</p>",
    "published": "Tue, 21 Oct 2025 17:45:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-23T09:39:54.399288",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "New 'Markovian Thinking' technique unlocks a path to million-token AI reasoning",
    "link": "https://venturebeat.com/ai/new-markovian-thinking-technique-unlocks-a-path-to-million-token-ai",
    "summary": "<p>Researchers at Mila have proposed a new technique that makes large language models (LLMs) vastly more efficient when performing complex reasoning. Called <a href=\"https://arxiv.org/abs/2510.06557\"><u>Markovian Thinking</u></a>, the approach allows LLMs to engage in lengthy reasoning without incurring the prohibitive computational costs that currently limit such tasks.</p><p>The team’s implementation, an environment named Delethink, structures the reasoning chain into fixed-size chunks, breaking the scaling problem that plagues very long LLM responses. Initial estimates show that for a 1.5B parameter model, this method can cut the costs of training by more than two-thirds compared to standard approaches.</p><h2>The quadratic curse of long-chain reasoning</h2><p>For an LLM to solve a complex problem, it often needs to generate a long series of intermediate “thinking” tokens, often referred to as chain-of-thought (CoT). In recent years, researchers have found that using <a href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost\"><u>reinforcement learning</u></a> (RL) to train models to produce longer CoTs (sometimes referred to as LongCoT) has significantly improved their reasoning capabilities.</p><p>However, the standard method for this has a critical flaw: The AI&#x27;s &quot;state&quot; (the prompt plus all the reasoning tokens it has generated thus far in its processing) grows with every new reasoning token. For modern <a href=\"https://bdtechtalks.com/2022/05/02/what-is-the-transformer/\"><u>transformer-based models</u></a>, this means the computational cost explodes quadratically as the reasoning chain gets longer, making it prohibitively expensive to train models for very complex tasks.</p><p>Most current attempts to manage this cost focus on limiting how much thinking the model does, implicitly preferring shorter solutions or terminating the process early. While these methods offer some relief, the Mila researchers still operate within the LongCoT framework and are thus fundamentally bound by its quadratic nature.</p><p>Instead of trying to control the computational growth, Mila created an RL environment that avoids the quadratic problem altogether. As co-author Amirhossein Kazemnejad explained, the goal is to enable capabilities like multi-week reasoning and scientific discovery. &quot;That regime (and the RL needed to enable such capabilities) is not supported by the current LongCoT paradigm, because of quadratic compute cost,&quot; he said.</p><h2>Thinking in chunks with Delethink</h2><p>The researchers&#x27; solution is a paradigm they call the &quot;Markovian Thinker,&quot; where the model reasons while keeping the size of its reasoning context window constant. The core idea is to change the RL setup to separate &quot;how long the model thinks&quot; from &quot;how much context it must process.&quot; If done correctly, a Markovian Thinker turns the quadratic growth problem into linear compute and fixed memory requirements for LLM reasoning.</p><p>The researchers put this paradigm into practice through Delethink, which forces the model to reason in a sequence of fixed-size chunks, such as 8,000 tokens at a time. Within each chunk, the model reasons as it normally would, using the classic attention mechanism. But when it reaches the limit of the chunk, the environment resets the context, creating a new prompt that includes the original query plus a short &quot;carryover&quot; from the previous chunk. For example, the carryover could be the last few tokens of the previous chunk of CoT or a summary of the most important results.</p><p>This rearrangement of the problem forces the model to learn how to embed a summary of its progress, or a &quot;textual Markovian state,&quot; into this carryover to continue its reasoning in the next chunk. This addresses the common concern of whether the model can remember important details from earlier steps. </p><p>According to Kazemnejad, the model learns what to remember. &quot;With training... the model is forced to learn to carry forward the task-critical state,&quot; he explained. He added crucial clarification for practical use: The original input prompt is not modified, including the documents or contextual data added to it. “Our approach is aimed at the reasoning phase and does not modify the prompt,&quot; he said.</p><h2>Delethink in action</h2><p>To test their approach, the researchers trained R1-Distill-1.5B with Delethink on a dataset of competition-level math problems, then evaluated it against several benchmarks. The model was trained to reason for up to 24,000 tokens but with fixed 8,000-token chunks. </p><p>The researchers <!-- -->compared this to models trained with the standard LongCoT-RL method. Their findings indicate that the model trained with Delethink could reason up to 24,000 tokens, and matched or surpassed a LongCoT model trained with the same 24,000-token budget on math benchmarks. On other tasks like coding and PhD-level questions, Delethink also matched or slightly beat its LongCoT counterpart. “Overall, these results indicate that Delethink uses its thinking tokens as effectively as LongCoT-RL with reduced compute,” the researchers write.</p><p>The benefits become even more pronounced when scaling beyond the training budget. While models trained with LongCoT quickly plateaued at their training limits, the Delethink-trained model continued to improve its performance. For instance, some math problems were only solved after the model reasoned for up to 140,000 tokens, far beyond its 24,000-token training budget. This linear compute advantage is substantial for enterprise applications. The researchers estimate that training a model to an average thinking length of 96,000 tokens would require 27 H100-GPU-months with LongCoT, versus just 7 with Delethink.</p><p>This efficiency extends directly to inference, the primary operational cost for most enterprises. &quot;Models trained in Markovian Thinking use the same inference style (delethink-tracing) during test time, which provides the same advantages of linear compute and constant memory after training,&quot; said Kazemnejad. He offered a practical example: An AI agent could &quot;debug a large codebase and think for a long time... which of course reduces the cost significantly compared to the conventional LongCoT approach.&quot;</p><p>Interestingly, the researchers found that off-the-shelf reasoning models, even without any specific training, already exhibit some ability to think in a Markovian way. This finding has immediate practical implications for developers. &quot;In practice, this means that — without Delethink-RL— these models can already run a delethink-tracing wrapper and perform competitively with LongCoT on our benchmarked tasks,&quot; Kazemnejad said.</p><p>Their experiments with larger models such as <a href=\"https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b\"><u>GPT-OSS 120B</u></a> showed robust performance with Delethink across a range of complex tasks. This latent ability provides a strong starting point for RL training, helping explain why the method is so effective. “Together, these results suggest that Delethink is compatible and scales with state-of-the-art models,” the researchers conclude.</p><p>The success of Markovian Thinking shows it may be possible for &quot;next-generation reasoning models to think for millions of tokens,&quot; the researchers note. This opens the door to fundamentally new AI capabilities, moving beyond current constraints. </p><p>&quot;Markovian Thinking... opens the path for models that can &#x27;think&#x27; for very long horizons, which we view as a necessary step toward eventual scientific discovery,&quot; Kazemnejad said. &quot;Our approach removes a key bottleneck and can allow training for much longer horizon tasks, which enables next-gen capabilities.&quot;</p>",
    "published": "Tue, 21 Oct 2025 05:12:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-23T09:39:54.399544",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "How accounting firms are using AI agents to reclaim time and trust",
    "link": "https://www.artificialintelligence-news.com/news/finance-ai-reclaiming-time-trust-with-openai-chatgpt/",
    "summary": "<p>For CFOs and CIOs under pressure to modernise finance operations, automation – as seen in several generations of RPA (robotic process automation) – isn’t enough. It&#8217;s apparent that transparency and explainability matter just as much. Accounting firms and finance functions inside organisations are now turning to AI systems that reason, not just compute. One of [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/finance-ai-reclaiming-time-trust-with-openai-chatgpt/\">How accounting firms are using AI agents to reclaim time and trust</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Tue, 21 Oct 2025 11:41:15 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-23T09:39:57.576624",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.95
  },
  {
    "title": "'AI' data centers are so power-hungry, they're now using old jet engines",
    "link": "https://www.reddit.com/r/artificial/comments/1odyvu2/ai_data_centers_are_so_powerhungry_theyre_now/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1odyvu2/ai_data_centers_are_so_powerhungry_theyre_now/\"> <img alt=\"'AI' data centers are so power-hungry, they're now using old jet engines\" src=\"https://external-preview.redd.it/oofWerD4DJfYPFDAWow0tg75yxVv8vn0-v_Yl-eV71A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10bd3d9f931af995757d4618210f94ba11d724c1\" title=\"'AI' data centers are so power-hungry, they're now using old jet engines\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fcking_Chuck\"> /u/Fcking_Chuck </a> <br /> <span><a href=\"https://www.pcworld.com/article/2949900/ai-data-centers-are-so-power-hungry-theyre-now-using-old-jet-engines.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1odyvu2/ai_data_centers_are_so_powerhungry_theyre_now/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-23T09:38:46+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-23T09:40:00.049397",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-23T09:38:46+00:00",
    "days_old": 0,
    "priority_score": 0.63
  },
  {
    "title": "Microsoft Edge begs you to use Copilot AI instead of ChatGPT",
    "link": "https://www.reddit.com/r/artificial/comments/1odwv2r/microsoft_edge_begs_you_to_use_copilot_ai_instead/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1odwv2r/microsoft_edge_begs_you_to_use_copilot_ai_instead/\"> <img alt=\"Microsoft Edge begs you to use Copilot AI instead of ChatGPT\" src=\"https://external-preview.redd.it/frbKd7Mr-J64sfXKAKljHl9PxeJs6RKk835EZ66snZw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a8cd8e6b012d431139040be5bc91c760ea92209\" title=\"Microsoft Edge begs you to use Copilot AI instead of ChatGPT\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fcking_Chuck\"> /u/Fcking_Chuck </a> <br /> <span><a href=\"https://www.pcworld.com/article/2949767/microsoft-edge-begs-you-to-use-copilot-ai-instead-of-chatgpt.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1odwv2r/microsoft_edge_begs_you_to_use_copilot_ai_instead/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-23T07:25:05+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-23T09:40:00.049524",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-23T07:25:05+00:00",
    "days_old": 0,
    "priority_score": 1.0499999999999998
  },
  {
    "title": "Australian-made LLM beats OpenAI and Google at legal retrieval",
    "link": "https://www.reddit.com/r/artificial/comments/1odqnxn/australianmade_llm_beats_openai_and_google_at/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1odqnxn/australianmade_llm_beats_openai_and_google_at/\"> <img alt=\"Australian-made LLM beats OpenAI and Google at legal retrieval\" src=\"https://external-preview.redd.it/NLZqYLoL-fbJ-SgiwzHPMNSJqlh9AIgrrDR-IJ5JtJ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a49ae63c4b26dc827be895f007791b5c58a4e7d\" title=\"Australian-made LLM beats OpenAI and Google at legal retrieval\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><strong>&quot;</strong><a href=\"https://isaacus.com/\">Isaacus</a>, an Australian foundational legal AI startup, has launched <a href=\"https://isaacus.com/blog/introducing-kanon-2-embedder\"><strong>Kanon 2 Embedder</strong></a>, a state-of-the-art legal embedding LLM, and unveiled the <a href=\"https://huggingface.co/blog/isaacus/introducing-mleb\">Massive Legal Embedding Benchmark (MLEB)</a>, an open-source benchmark for evaluating legal information retrieval performance across six jurisdictions (the US, UK, EU, Australia, Singapore, and Ireland) and five domains (cases, statutes, regulations, contracts, and academia).</p> <p>Kanon 2 Embedder ranks first on MLEB as of 23 October 2025, delivering <strong>9% higher accuracy than OpenAI Text Embedding 3 Large</strong> and <strong>6% higher accuracy than Google Gemini Embedding</strong> while running <strong>&gt;30% faster</strong> than both LLMs. Kanon 2 Embedder leads a field of 20 LLMs, including Qwen3 Embedding 8B, IBM Granite Embedding R2, and Microsoft E5 Large Instruct.&quot;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Neon0asis\"> /u/Neon0asis </a> <br /> <span><a href=\"https://huggingface.co/blog/isaacus/kanon-2-embedder\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1odqnxn/australianmade_llm_beats_openai_and_google_at/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-23T01:37:00+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-23T09:40:00.049652",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-23T01:37:00+00:00",
    "days_old": 0,
    "priority_score": 0.8399999999999999
  }
]