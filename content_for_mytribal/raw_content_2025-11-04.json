[
  {
    "title": "Forget Fine-Tuning: SAP’s RPT-1 Brings Ready-to-Use AI for Business Tasks",
    "link": "https://venturebeat.com/ai/forget-fine-tuning-saps-rpt-1-brings-ready-to-use-ai-for-business-tasks",
    "summary": "<p>SAP aims to displace more general large language models with the release of its own foundational “tabular” model, which the company claims will reduce training requirements for enterprises. </p><p>The model, called SAP RPT-1, is a pre-trained model with business and enterprise knowledge out of the box. SAP calls it a Relational Foundation Model, meaning it can do predictions based on relational databases even without fine-tuning or additional training.</p><p>Walter Sun, SAP&#x27;s global head of AI, told VentureBeat in an interview that the value of the new model lies in its ability to perform various enterprise tasks, such as predictive analytics, out of the box. </p><p>“Everyone knows about language models, and there’s a bunch of good ones that already exist,” Sun said. “But we trained the model on data on business transactions, basically Excel spreadsheets, and so we have a model that can do predictive analytics where the value is that it’s out of the box, meaning you don’t need to have specifics of a company to do tasks analogous to a language model.” </p><p>Sun said that right out of the gate, RPT-1 can essentially build out a business model for enterprises based on its knowledge gained from data from SAP’s decades of information. Organizations can plug the model directly into applications, even without additional fine-tuning.</p><p>RPT-1, SAP’s first large family of AI models, will be generally available in “Q4 of 2025” and be deployed via SAP’s AI Foundation. While RPT-1 is currently available, the company stated that additional models will be made available soon, including an open-source, state-of-the-art model. </p><p>SAP will also release a no-code playground environment to experiment with the model. \n</p><h2>Tabular models vs LLMs\n</h2><p>Tabular or relational AI models learned from spreadsheets, unlike LLMs, which learned from text and code. RPT-1 not only understands numbers and the relationships between different cells, but it’s also able to provide more structured and precise answers. </p><p>When enterprises decide to use RPT-1, they can add more direction to the model through a bit of context engineering, since the model is semantically aware and learns based on how it is being used. </p><p>SAP researchers first proposed the idea that tabular models can both exhibit semantic awareness and learn from content through a paper <a href=\"https://arxiv.org/pdf/2506.10707\"><u>published in June</u></a>. It proposed ConTextTab introduced context-aware pretraining. It utilizes semantic signals, such as table headers or column types, to guide model training, enabling the model to build a relational structure with the data. It’s this architecture that makes the model work best for tasks with precise answers, such as for financial or enterprise use cases.</p><p>The RPT models build on the ConTextTab work that lets it learn structured business data, say from SAP’s knowledge graph, and then be able to add more context through usage. </p><p>SAP researchers did test ConTextTab against benchmarks, saying it “is competitive” against similar models like TabPFN and TabIFL. </p><h2>Industry-specific models continue to grow</h2><p>\nMany enterprises prefer to fine-tune general LLMs like GPT-5 or Claude, to basically retrain the model to answer only questions relevant to their business. However, a shift towards <a href=\"https://venturebeat.com/ai/microsoft-brings-ai-to-the-farm-and-factory-floor-partnering-with-industry-giants\"><u>industry-specific models has begun to take root</u></a>. </p><p>Sun said that his experience at a previous company, building a very narrow, highly customized AI model for sentiment analysis, influenced a lot of what makes RPT-1 different. </p><p>“It was a very customized model, a narrow model that takes specific feedback for specific products but it wasn’t scalable,” Sun said. “When LLMs came about, that one model measures sentiment. But there are use cases that we can do that LLMs cannot do.”</p><p>He said these use cases include predictions, such as determining when a shopper will return to a grocery store, which may involve numerical analysis along with an understanding of the shopper’s buying habits. However, some LLMs have begun integrating into spreadsheets, and AI model providers encourage users to upload similar data to teach them context. <a href=\"https://www.microsoft.com/\"><u>Microsoft</u></a> added new <a href=\"https://venturebeat.com/ai/microsofts-copilot-can-now-build-apps-and-automate-your-job-heres-how-it\"><u>capabilities to Copilot</u></a>, including the ability to work in Excel. <a href=\"https://www.anthropic.com/\"><u>Anthropic</u></a> <a href=\"https://venturebeat.com/ai/anthropic-rolls-out-claude-ai-for-finance-integrates-with-excel-to-rival\"><u>integrated its Claude</u></a> model with Excel, complementing its <a href=\"https://venturebeat.com/ai/financial-firms-get-a-purpose-built-claude-as-anthropic-bets-on-vertical-ai-platforms\"><u>Claude for Finance service</u></a>. Chinese startup <a href=\"https://manus.im/\"><u>Manus</u></a> also offers a <a href=\"https://venturebeat.com/data-infrastructure/chinese-startup-manus-challenges-chatgpt-in-data-visualization-which-should-enterprises-use\"><u>data visualization tool</u></a> that understands spreadsheets, and ChatGPT can create charts from uploaded spreadsheets and other data sources. </p><p>However, SAP noted that it is more than just reading a spreadsheet; RPT-1 should stand out amongst its competitors because it requires fewer additional pieces of information about a business to provide its responses. </p><p>\n</p>",
    "published": "Tue, 04 Nov 2025 05:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-04T09:13:14.257592",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Inside Zendesk’s dual AI leap: From reliable agents to real-time intelligence with GPT-5 and HyperArc",
    "link": "https://venturebeat.com/ai/inside-zendesks-dual-ai-leap-from-reliable-agents-to-real-time-intelligence",
    "summary": "<p><i>Presented by Zendesk</i></p><hr /><p>Agentic AI is currently transforming three key areas of work — creative, coding, and support — says Shashi Upadhyay, president of engineering, AI, and product at Zendesk. But he notes that support presents a distinct challenge. </p><p>&quot;Support is special because you’re putting an autonomous AI agent right in front of your customer,&quot; Upadhyay says. &quot;You have to be confident that it’s going to do the right thing for the customer and by the customer. Every step forward in AI should make service more dependable for both customers and human agents.&quot; </p><p>Zendesk, recently named a Leader in the <a href=\"https://www.zendesk.com/blog/zip1-gartner-mq-crm-2025/\">2025 Gartner Magic Quadrant</a> for the CRM Customer Engagement Center, started implementing AI agents about a year and a half ago. Since then, they&#x27;ve seen that AI agents can solve almost 80% of all incoming customer requests on their own. For the remaining 20%, the AI agent can hand it over to a human to help solve the more complex problems. </p><p>&quot;Autonomous AI agents work 24/7, with no wait or queue time. You have a problem; they provide an answer right away. All of that adds up,&quot; he says. &quot;Not only do you get higher resolutions, higher automation, but you can also improve the CSAT at the same time. Because 80% is such a promising number, and the results are so solid, we believe it’s only a matter of time before everyone adopts this technology. We already see that across the board.&quot;</p><p>The company&#x27;s efforts to advance its standard of usability, depth of insight, and time to value for organizations of all sizes require continuous testing, integration of advanced models like ChatGPT-5, and a major upgrade of its analytics capabilities and real-time, gen AI–powered insights with the acquisition of HyperArc, an AI-native analytics platform.</p><h3><b>Designing, testing, and deploying a better agent</b></h3><p>&quot;In a support context especially, it’s important AI agents behave consistently with the brand of the company, policies, and regulatory requirements you may have,&quot; Upadhyay says. &quot;We test every agent, every model continuously across all our customers. We do it before we release it and we do it after we release it, across five categories.&quot; </p><p>Those categories — automation rate, execution, precision, latency, and safety — form the foundation of Zendesk’s ongoing benchmarking program. Each model is scored on how accurately it resolves issues, how well it follows instructions, how fast it responds, and whether it stays within clearly defined guardrails. The goal isn’t just to make AI faster — it’s to make it dependable, accountable, and aligned with the standards that define great customer service.</p><p>That testing is reinforced by Zendesk’s QA agent — an automated monitor that keeps a constant eye on every conversation. If an exchange starts to drift off course, whether in tone or accuracy, the system immediately flags it and alerts a human agent to step in. It’s an added layer of assurance that keeps the customer experience on track, even when AI is running the first line of support.</p><h3><b>GPT-5 for next-level agents</b></h3><p>In the world of support and service, the move from simple chatbots that answer basic queries or solve uncomplicated problems, to agents that actually take action, is groundbreaking. An agent that can understand that a customer wants to return an item, confirm whether it&#x27;s eligible for a return, process the return, and issue a refund, is a powerful upgrade. With the introduction of ChatGPT-5, Zendesk recognized an opportunity to integrate that ability into its Resolution Platform.</p><p>&quot;We worked <a href=\"https://www.zendesk.com/blog/zip2-zendesk-ai-gpt-5/\">very closely with OpenAI</a> because GPT-5 was a pretty big improvement in model capabilities, going from being able to answer questions, to being able to reason and take action,&quot; Upadhyay says. &quot;First, it does a much better job at solving problems autonomously. Secondly, it&#x27;s much better at understanding your intent, which improves the customer experience because you feel understood. Last but not least, it has 95%-plus reliability on executing correctly.&quot;</p><p>Those gains ripple across Zendesk’s AI agents, Copilot, and App Builder. GPT-5 cuts workflow failures by 30%, thanks to its ability to adapt to unexpected complexity without losing context, and reduces fallback escalations by more than 20%, with more complete and accurate responses. The result: faster resolutions, fewer hand-offs, and AI that behaves more like a seasoned support professional than a scripted assistant.</p><p>Plus, GPT-5 is better at handling ambiguity, and able to clarify vague customer input, which improves routing and increases automated workflows in over 65% of conversations. It has greater accuracy across five languages, and makes agents more productive with more concise, contextually relevant answers that align with tone guidelines.</p><p>And in App Builder, GPT-5 delivered 25% to 30% faster overall performance, with more prompt iterations per minute, speeding app builder development workflows.</p><h3><b>Filling in the analytics gap</b></h3><p>Traditionally, support analytics has focused on structured data — the kind that fits neatly into a table: when a ticket was opened, who handled it, how long it took to resolve, and when it was closed. But the most valuable insights often live in unstructured data — the conversations themselves, spread across email, chat, voice, and messaging apps like WhatsApp.</p><p>&quot;Customers often don’t realize how much intelligence sits in their support interactions,&quot; Upadhyay says. &quot;What we’re pushing for with analytics is ways in which we can improve the entire company with the insights that are sitting in support data.&quot;</p><p>To surface those deeper insights, Zendesk turned to HyperArc, an AI-native analytics company known for its proprietary HyperGraph engine and generative-AI-powered insights. The acquisition gave new life to Explore, Zendesk’s analytics platform, transforming it into a modern solution capable of merging structured and unstructured data, supporting conversational interfaces, and drawing on persistent memory to use past interactions as context for new queries.</p><p>&quot;Your support interactions are telling you everything that’s not working in your business today, all that information is sitting in these millions of tickets that you’ve collected over time,&quot; Upadhyay says. &quot;We wanted to make that completely visible. Now we have this genius AI agent that can analyze it all and come back with explicit recommendations. That doesn’t just improve support. It improves the entire company.&quot;</p><p>That visibility now translates into actionable intelligence. The system can pinpoint where issues are most persistent, identify the patterns behind them, and suggest ways to resolve them. It can even anticipate problems before they happen. During high-pressure events like Black Friday, for example, it can analyze historical data to flag recurring issues, predict where new bottlenecks might appear, and recommend preventive measures — turning reactive support into proactive strategy.</p><p>&quot;That’s where HyperArc shines,&quot; Upadhyay says. It doesn’t just help you understand the past — it helps you plan better for the future.&quot;</p><p>By integrating HyperArc’s AI-native intelligence, Zendesk is moving customer service toward continuous learning — where every interaction builds trust and sharpens performance, setting the stage for AI that can see what’s coming next.</p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>",
    "published": "Tue, 04 Nov 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-04T09:13:14.258141",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "The beginning of the end of the transformer era? Neuro-symbolic AI startup AUI announces new funding at $750M valuation",
    "link": "https://venturebeat.com/ai/the-beginning-of-the-end-of-the-transformer-era-neuro-symbolic-ai-startup",
    "summary": "<p>The buzzed-about but still stealthy New York City startup <a href=\"https://www.aui.io/\">Augmented Intelligence Inc (AUI)</a>, which seeks to go beyond the popular &quot;transformer&quot; architecture used by most of today&#x27;s LLMs such as ChatGPT and Gemini, has<b> raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million</b>, VentureBeat can exclusively reveal.</p><p>The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.</p><p>AUI relies on a fusion of the transformer tech and a newer technology called &quot;neuro-symbolic AI,&quot; described in greater detail below. </p><p>&quot;We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,&quot; said <b>Ohad Elhelo</b>, <b>AUI co-founder and CEO</b> in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside <b>co-founder and Chief Product Officer Ori Cohen.</b></p><p>The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the <a href=\"https://cloud.google.com/blog/topics/partners/google-cloud-partners-with-aui/\">company’s announced go-to-market partnership with Google</a> in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.</p><p>According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.</p><p>AUI is the <a href=\"https://venturebeat.com/ai/has-this-stealth-startup-finally-cracked-the-code-on-enterprise-ai-agent\">company behind Apollo-1</a>, a new foundation model built for task-oriented dialog, which it describes as the &quot;economic half&quot; of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. </p><p>The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.</p><p>Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”</p><h3><b>A Distinctive Neuro-Symbolic Architecture</b></h3><p>Apollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper &quot;Attention Is All You Need&quot; — AUI&#x27;s system integrates two layers:</p><ul><li><p>Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.</p></li><li><p>A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.</p></li></ul><p>This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.</p><p>Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”</p><p>However, enterprises that have already built systems built around transformer LLMs needn&#x27;t worry. AUI wants to make adopting its new technology just as easy. </p><p>&quot;Apollo-1 deploys like any modern foundation model,&quot; Elhelo told VentureBeat in a text last night. &quot;It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.&quot;</p><h3><b>Generalization and Domain Flexibility</b></h3><p>Apollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.</p><p>Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.</p><p>Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. </p><p>For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.</p><p>As Elhelo explained to VentureBeat, LLMs are &quot;not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”</p><h3><b>Availability and Developer Access</b></h3><p>Apollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a <a href=\"https://www.theinformation.com/articles/startup-teaching-ai-agents-shop\">previous report by <i>The Information</i></a><i>, </i>which broke the initial news on the startup.</p><p>Enterprises can integrate with Apollo-1 either via:</p><ul><li><p>A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; or</p></li><li><p>A standard API, using OpenAI-compatible formats.</p></li></ul><p>The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.</p><h3><b>Enterprise Fit: When Reliability Beats Fluency</b></h3><p>While LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. </p><p>Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.</p><p>Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”</p>",
    "published": "Mon, 03 Nov 2025 14:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-11-04T09:13:14.259206",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Meet Denario, the AI ‘research assistant’ that is already getting its own papers published",
    "link": "https://venturebeat.com/ai/meet-denario-the-ai-research-assistant-that-is-already-getting-its-own",
    "summary": "<p>An <a href=\"https://arxiv.org/abs/2510.26887\"><u>international team of researchers</u></a> has released an <a href=\"https://huggingface.co/spaces/astropilot-ai/Denario\"><u>artificial intelligence system</u></a> capable of autonomously conducting scientific research across multiple disciplines — generating papers from initial concept to publication-ready manuscript in approximately 30 minutes for about $4 each.</p><p>The system, called <a href=\"https://astropilot-ai.github.io/DenarioPaperPage/\"><u>Denario</u></a>, can formulate research ideas, review existing literature, develop methodologies, write and execute code, create visualizations, and draft complete academic papers. In a demonstration of its versatility, the team <a href=\"https://github.com/AstroPilot-AI/DenarioExamplePapers\"><u>used Denario to generate papers</u></a> spanning astrophysics, biology, chemistry, medicine, neuroscience, and other fields, with one AI-generated paper already accepted for publication at an <a href=\"https://agents4science.stanford.edu/\"><u>academic conference</u></a>.</p><p>&quot;The goal of Denario is not to automate science, but to develop a research assistant that can accelerate scientific discovery,&quot; the researchers wrote in a paper released Monday describing the system. The team is making the software <a href=\"https://github.com/AstroPilot-AI/Denario\"><u>publicly available</u></a> as an open-source tool.</p><p>This achievement marks a turning point in the application of large language models to scientific work, potentially transforming how researchers approach early-stage investigations and literature reviews. However, the research also highlights substantial limitations and raises pressing questions about validation, authorship, and the changing nature of scientific labor.</p><h3><b>From data to draft: how AI agents collaborate to conduct research</b></h3><p>At its core, <a href=\"https://huggingface.co/spaces/astropilot-ai/Denario\"><u>Denario</u></a> operates not as a single AI brain but as a digital research department where specialized AI agents collaborate to push a project from conception to completion. The process can begin with the &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Idea Module</u></a>,&quot; which employs a fascinating adversarial process where an &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Idea Maker</u></a>&quot; agent proposes research projects that are then scrutinized by an &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Idea Hater</u></a>&quot; agent, which critiques them for feasibility and scientific value. This iterative loop refines raw concepts into robust research directions.</p><p>Once a hypothesis is solidified, a &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Literature Module</u></a>&quot; scours academic databases like Semantic Scholar to check the idea&#x27;s novelty, followed by a &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Methodology Module</u></a>&quot; that lays out a detailed, step-by-step research plan. The heavy lifting is then done by the &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Analysis Module</u></a>,&quot; a virtual workhorse that writes, debugs, and executes its own Python code to analyze data, generate plots, and summarize findings. Finally, the &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Paper Module</u></a>&quot; takes the resulting data and plots and drafts a complete scientific paper in LaTeX, the standard for many scientific fields. In a final, recursive step, a &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Review Module</u></a>&quot; can even act as an AI peer-reviewer, providing a critical report on the generated paper&#x27;s strengths and weaknesses.</p><p>This modular design allows a human researcher to intervene at any stage, providing their own idea or methodology, or to simply use Denario as an end-to-end autonomous system. &quot;The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis,&quot; the paper explains.</p><p>To validate its capabilities, the Denario team has put the system to the test, generating a vast repository of papers across numerous disciplines. In a striking proof of concept, one paper fully generated by Denario was accepted for publication at the <a href=\"https://www.nature.com/articles/d41586-025-03363-3\"><u>Agents4Science 2025 conference</u></a> — a peer-reviewed venue where AI systems themselves are the primary authors. The paper, titled &quot;QITT-Enhanced Multi-Scale Substructure Analysis with Learned Topological Embeddings for Cosmological Parameter Estimation from Dark Matter Halo Merger Trees,&quot; successfully combined complex ideas from quantum physics, machine learning, and cosmology to analyze simulation data.</p><h3><b>The ghost in the machine: AI’s ‘vacuous’ results and ethical alarms</b></h3><p>While the successes are notable, the research paper is refreshingly candid about Denario&#x27;s significant limitations and failure modes. The authors stress that the system currently &quot;behaves more like a good undergraduate or early graduate student rather than a full professor in terms of big picture, connecting results...etc.&quot; This honesty provides a crucial reality check in a field often dominated by hype.</p><p>The paper dedicates entire sections to &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Failure Modes</u></a>&quot; and &quot;<a href=\"https://arxiv.org/pdf/2510.26887\"><u>Ethical Implications</u></a>,&quot; a level of transparency that enterprise leaders should note. The authors report that in one instance, the system &quot;hallucinated an entire paper without implementing the necessary numerical solver,&quot; inventing results to fit a plausible narrative. In another test on a pure mathematics problem, the AI produced text that had the <i>form</i> of a mathematical proof but was, in the authors&#x27; words, &quot;mathematically vacuous.&quot;</p><p>These failures underscore a critical point for any organization looking to deploy agentic AI: the systems can be brittle and are prone to confident-sounding errors that require expert human oversight. The Denario paper serves as a vital case study in the importance of keeping a human in the loop for validation and critical assessment.</p><p>The authors also confront the profound ethical questions raised by their creation. They warn that &quot;AI agents could be used to quickly flood the scientific literature with claims driven by a particular political agenda or specific commercial or economic interests.&quot; They also touch on the &quot;Turing Trap,&quot; a phenomenon where the goal becomes mimicking human intelligence rather than augmenting it, potentially leading to a &quot;homogenization&quot; of research that stifles true, paradigm-shifting innovation.</p><h3><b>An open-source co-pilot for the world&#x27;s labs</b></h3><p>Denario is not just a theoretical exercise locked away in an academic lab. The entire system is <a href=\"https://github.com/AstroPilot-AI/Denario\"><u>open-source</u></a> under a GPL-3.0 license and is accessible to the broader community. The main project and its graphical user interface, DenarioApp, are <a href=\"https://github.com/AstroPilot-AI/Denario\"><u>available on GitHub</u></a>, with installation managed via standard Python tools. For enterprise environments focused on reproducibility and scalability, the project also provides official Docker images. A public demo hosted on <a href=\"https://huggingface.co/spaces/astropilot-ai/Denario\"><u>Hugging Face Spaces</u></a> allows anyone to experiment with its capabilities.</p><p>For now, Denario remains what its creators call a powerful assistant, but not a replacement for the seasoned intuition of a human expert. This framing is deliberate. The Denario project is less about creating an automated scientist and more about building the ultimate co-pilot, one designed to handle the tedious and time-consuming aspects of modern research.</p><p>By handing off the grueling work of coding, debugging, and initial drafting to an AI agent, the system promises to free up human researchers for the one task it cannot automate: the deep, critical thinking required to ask the right questions in the first place.</p>",
    "published": "Mon, 03 Nov 2025 09:40:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-11-04T09:13:14.260025",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Developers beware: Google’s Gemma model controversy exposes model lifecycle risks",
    "link": "https://venturebeat.com/ai/developers-beware-googles-gemma-model-controversy-exposes-model-lifecycle",
    "summary": "<p>The recent controversy surrounding <a href=\"https://www.google.com/\"><u>Google</u></a>’s Gemma model has once again highlighted the dangers of using developer test models and the fleeting nature of model availability. </p><p>Google pulled its <a href=\"https://venturebeat.com/ai/google-unveils-open-source-gemma-3-model-with-128k-context-window\"><u>Gemma 3 model</u></a> from AI Studio following a statement from Senator Marsha Blackburn (R-Tenn.) that the Gemma model <a href=\"https://x.com/MarshaBlackburn/status/1985189610834612546\"><u>willfully hallucinated falsehoods</u></a> about her. Blackburn said the model fabricated news stories about her that go beyond “harmless hallucination” and function as a defamatory act. </p><p>In response, Google <a href=\"https://x.com/NewsFromGoogle/status/1984412221531885853\"><u>posted on X</u></a> on October 31 that it will remove Gemma from AI Studio, stating that this is “to prevent confusion.” Gemma remains available via API. </p><p>It is also available via AI Studio, which, the company described, is &quot;a developer tool (in fact, to use it you need to attest you&#x27;re a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this to be a consumer tool or model, or to be used this way. To prevent this confusion, access to Gemma is no longer available on AI Studio.&quot;</p><p>To be clear, Google has the right to remove its model from its platform, especially if people have found hallucinations and falsehoods that could proliferate. It also underscores the danger of relying mainly on experimental models and why enterprise developers need to save projects before AI models are sunsetted or removed. Technology companies like Google continue to face political controversies, which often influence their deployments. </p><p>VentureBeat reached out to Google for additional information and was pointed to their October 31 posts. We also contacted the office of Sen. Blackburn, who reiterated her stance outlined in a statement that AI companies should “shut [models] down until you can control it.&quot;</p><h2>Developer experiments</h2><p>The Gemma family of models, which includes a <a href=\"https://venturebeat.com/ai/google-unveils-ultra-small-and-efficient-open-source-ai-model-gemma-3-270m-that-can-run-on-smartphones\"><u>270M parameter version</u></a>, is best suited for small, quick apps and tasks that can run on devices such as smartphones and laptops. Google said the Gemma models were “built specifically for the developer and research community. They are not meant for factual assistance or for consumers to use.”</p><p>Nevertheless, non-developers could still access Gemma because it is on the <a href=\"https://venturebeat.com/ai/googles-new-vibe-coding-ai-studio-experience-lets-anyone-build-deploy-apps\"><u>AI Studio platform</u></a>, a more beginner-friendly space for developers to play around with Google AI models compared to Vertex AI. So even if Google never intended Gemma and AI Studio to be accessible to, say, Congressional staffers, these situations can still occur. </p><p>It also shows that as models continue to improve, these models still produce inaccurate and potentially harmful information. Enterprises must continually weigh the benefits of using models like Gemma against their potential inaccuracies. </p><h2>Project continuity </h2><p>Another concern is the control that AI companies have over their models. The adage “you don’t own anything on the internet” remains true. If you don’t own a physical or local copy of software, it’s easy for you to lose access to it if the company that owns it decides to take it away. Google did not clarify with VentureBeat if current projects on AI Studio powered by Gemma are saved. </p><p>Similarly, <a href=\"https://venturebeat.com/ai/chatgpt-users-dismayed-as-openai-pulls-popular-models-gpt-4o-o3-and-more-enterprise-api-remains-for-now\"><u>OpenAI</u></a> users were disappointed when the company announced that it would <a href=\"https://venturebeat.com/ai/chatgpt-users-dismayed-as-openai-pulls-popular-models-gpt-4o-o3-and-more-enterprise-api-remains-for-now\"><u>remove popular older models</u></a> on ChatGPT. Even after walking back his statement and <a href=\"https://venturebeat.com/ai/openai-returns-old-models-to-chatgpt-as-sam-altman-admits-bumpy-gpt-5-rollout\"><u>reinstating GPT-4o</u></a> back to ChatGPT, OpenAI CEO  Sam Altman continues to field questions around keeping and supporting the model. </p><p>AI companies can, and should, remove their models if they create harmful outputs. AI models, no matter how mature, remain works in progress and are constantly evolving and improving. But, since they are experimental in nature, models can easily become tools that technology companies and lawmakers can wield as leverage. Enterprise developers must ensure that their work can be saved before models are removed from platforms. </p>",
    "published": "Mon, 03 Nov 2025 05:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-04T09:13:14.260387",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Moving past speculation: How deterministic CPUs deliver predictable AI performance",
    "link": "https://venturebeat.com/ai/moving-past-speculation-how-deterministic-cpus-deliver-predictable-ai",
    "summary": "<p>For more than three decades, modern CPUs have relied on speculative execution to keep pipelines full. When it emerged in the 1990s, speculation was hailed as a breakthrough — just as pipelining and superscalar execution had been in earlier decades. Each marked a generational leap in microarchitecture. By predicting the outcomes of branches and memory loads, processors could avoid stalls and keep execution units busy. </p><p>But this architectural shift came at a cost: Wasted energy when predictions failed, increased complexity and vulnerabilities such as Spectre and Meltdown. These challenges set the stage for an alternative: A deterministic, time-based execution model. As David Patterson <a href=\"https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/patterson80.pdf\">observed in 1980</a>, “A RISC potentially gains in speed merely from a simpler design.” Patterson’s principle of simplicity underpins a new alternative to speculation: A deterministic, time-based execution model.&quot;</p><p>For the first time since speculative execution became the dominant paradigm, a fundamentally new approach has been invented. This breakthrough is embodied in a series of six recently issued U.S. patents, sailing through the U.S. Patent and Trademark Office (USPTO). Together, they introduce a <a href=\"https://patents.google.com/patent/US11829187B2/en\">radically different</a> instruction execution model. Departing sharply from conventional speculative techniques, this novel deterministic framework replaces guesswork with a time-based, latency-tolerant mechanism. Each instruction is assigned a precise execution slot within the pipeline, resulting in a rigorously ordered and predictable flow of execution. This reimagined model redefines how modern processors can handle latency and concurrency with greater efficiency and reliability. </p><p>A simple time counter is used to deterministically set the exact time of when instructions should be executed in the future. Each instruction is dispatched to an execution queue with a preset execution time based on resolving its data dependencies and availability of resources — read buses, execution units and the write bus to the register file. Each instruction remains queued until its scheduled execution slot arrives. This new deterministic approach may represent the first major architectural challenge to speculation since it <a href=\"https://patents.google.com/patent/US11829187B2/en\">became the standard</a>.</p><p>The architecture extends naturally into matrix computation, with a RISC-V instruction set proposal under community review. Configurable general matrix multiply (GEMM) units, ranging from 8×8 to 64×64, can operate using either register-based or direct-memory acceess (DMA)-fed operands. This flexibility supports a wide range of AI and high-performance computing (HPC) workloads. Early analysis suggests scalability that rivals Google’s TPU cores, while maintaining significantly lower cost and power requirements. </p><p>Rather than a direct comparison with general-purpose CPUs, the more accurate reference point is vector and matrix engines: Traditional CPUs still depend on speculation and branch prediction, whereas this design applies deterministic scheduling directly to GEMM and vector units. This efficiency stems not only from the configurable GEMM blocks but also from the time-based execution model, where instructions are decoded and assigned precise execution slots based on operand readiness and resource availability. </p><p>Execution is never a random or heuristic choice among many candidates, but a predictable, pre-planned flow that keeps compute resources continuously busy. Planned matrix benchmarks will provide direct comparisons with TPU GEMM implementations, highlighting the ability to deliver datacenter-class performance without datacenter-class overhead.</p><p>Critics may argue that static scheduling introduces latency into instruction execution. In reality, the latency already exists — waiting on data dependencies or memory fetches. Conventional CPUs attempt to hide it with speculation, but when predictions fail, the resulting pipeline flush introduces delay and wastes power. </p><p>The time-counter approach acknowledges this latency and fills it deterministically with useful work, avoiding rollbacks. As the first patent notes, instructions retain out-of-order efficiency: “A <a href=\"https://patents.google.com/patent/US11829187B2/en\">microprocessor</a> with a time counter for statically dispatching instructions enables execution based on predicted timing rather than speculative issue and recovery,&quot; with preset execution times but without the overhead of register renaming or speculative comparators.</p><h2>Why speculation stalled</h2><p>Speculative execution boosts performance by predicting outcomes before they’re known — executing instructions ahead of time and discarding them if the guess was wrong. While this approach can accelerate workloads, it also introduces unpredictability and power inefficiency. Mispredictions inject “No Ops” into the pipeline, stalling progress and wasting energy on work that never completes. </p><p>These issues are magnified in modern <a href=\"https://venturebeat.com/ai/large-reasoning-models-almost-certainly-can-think\">AI and machine learning (ML)</a> workloads, where vector and matrix operations dominate and memory access patterns are irregular. Long fetches, non-cacheable loads and misaligned vectors frequently trigger pipeline flushes in speculative architectures.</p><p>The result is performance cliffs that vary wildly across datasets and problem sizes, making consistent tuning nearly impossible. Worse still, speculative side effects have exposed vulnerabilities that led to high-profile security exploits. As data intensity grows and memory systems strain, speculation struggles to keep pace — undermining its original promise of seamless acceleration.</p><h2>Time-based execution and deterministic scheduling</h2><p>At the core of this invention is a <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">vector coprocessor</a> with a time counter for statically dispatching instructions. Rather than relying on speculation, instructions are issued only when data dependencies and latency windows are fully known. This eliminates guesswork and costly pipeline flushes while preserving the throughput advantages of out-of-order execution. Architectures built on this patented framework feature deep pipelines — typically spanning 12 stages — combined with wide front ends supporting up to 8-way decode and large reorder buffers exceeding 250 entries</p><p>As illustrated in Figure 1, the architecture mirrors a conventional RISC-V processor at the top level, with instruction fetch and decode stages feeding into execution units. The innovation emerges in the integration of a <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">time counter and register scoreboard</a>, strategically positioned between fetch/decode and the vector execution units. Instead of relying on speculative comparators or register renaming, they utilize a Register Scoreboard and <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">Time Resource Matrix</a> (TRM) to deterministically schedule instructions based on operand readiness and resource availability.  </p><p><i>Figure 1: High-level block diagram of deterministic processor. A time counter and scoreboard sit between fetch/decode and vector execution units, ensuring instructions issue only when operands are ready.</i></p><p>A typical program running on the deterministic processor begins much like it does on any conventional RISC-V system: Instructions are fetched from memory and decoded to determine whether they are scalar, vector, matrix or custom extensions. The difference emerges at the point of dispatch. Instead of issuing instructions speculatively, the processor employs a cycle-accurate time counter, working with a register scoreboard, to decide exactly when each instruction can be executed. This mechanism provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.</p><p>In conjunction with a register scoreboard, the time-resource matrix associates instructions with execution cycles, allowing the processor to plan dispatch deterministically across available resources. The scoreboard tracks operand readiness and hazard information, enabling scheduling without register renaming or speculative comparators. By monitoring dependencies such as read-after-write (RAW) and write-after-read, it ensures hazards are resolved without costly pipeline flushes. As noted <a href=\"https://patents.google.com/patent/US12112172B2/en?oq=U.S.+Patent+No.+12%2c112%2c172\">in the patent</a>, “in a multi-threaded microprocessor, the time counter and scoreboard permit rescheduling around cache misses, branch flushes, and RAW hazards without speculative rollback.”</p><p>Once operands are ready, the instruction is dispatched to the appropriate execution unit. Scalar operations use standard artithmetic logic units (ALUs), while vector and matrix instructions execute in wide execution units connected to a <a href=\"https://venturebeat.com/ai/abstract-or-die-why-ai-enterprises-cant-afford-rigid-vector-stacks\">large vector</a> register file. Because instructions launch only when conditions are safe, these units stay highly utilized without the wasted work or recovery cycles caused by mis-predicted speculation. </p><p>The key enabler of this approach is a simple time counter that orchestrates execution according to data readiness and resource availability, ensuring instructions advance only when operands are ready and resources available. The same principle applies to memory operations: The interface predicts latency windows for loads and stores, allowing the processor to fill those slots with independent instructions and keep execution flowing.</p><h2>Programming model differences</h2><p>From the programmer’s perspective, the flow remains familiar — RISC-V code compiles and executes in the usual way. The crucial difference lies in the execution contract: Rather than relying on dynamic speculation to hide latency, the processor guarantees predictable dispatch and completion times. This eliminates the performance cliffs and wasted energy of speculation while still providing the throughput benefits of out-of-order execution. </p><p>This perspective underscores how deterministic execution preserves the familiar RISC-V programming model while eliminating the unpredictability and wasted effort of speculation. As <a href=\"https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/about/interview.html\">John Hennessy put it</a>: &quot;It’s stupid to do work in run time that you can do in compile time”— a remark reflecting the foundations of RISC and its forward-looking design philosophy.</p><p>The RISC-V ISA provides opcodes for custom and extension instructions, including floating-point, DSP, and vector operations. The result is a processor that executes instructions deterministically while retaining the benefits of out-of-order performance. By eliminating speculation, the design simplifies hardware, reduces power consumption and avoids pipeline flushes. </p><p>These efficiency gains grow even more significant in vector and matrix operations, where wide execution units require consistent utilization to reach peak performance. Vector extensions require wide register files and large execution units, which in speculative processors necessitate expensive register renaming to recover from branch mispredictions. In the deterministic design, vector instructions are executed only after commit, eliminating the need for renaming.</p><p>Each instruction is scheduled against a <a href=\"https://patents.google.com/patent/US12001848B2/en?oq=U.S.+Patent+No.+12%2c001%2c848\">cycle-accurate time counter</a>: “The time counter provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.” The vector register scoreboard resolves data dependency before issuing instructions to execution pipeline.  Instructions are dispatched in a known order at the correct cycle, making execution both predictable and efficient.</p><p>Vector execution units (integer and floating point) connect directly to a large vector register file. Because instructions are never flushed, there is no renaming overhead. The scoreboard ensures safe access, while the time counter aligns execution with memory readiness. A dedicated memory block predicts the return cycle of loads. Instead of stalling or speculating, the processor schedules independent instructions into latency slots, keeping <a href=\"https://patents.google.com/patent/US12001848B2/en?oq=U.S.+Patent+No.+12%2c001%2c848\">execution units busy</a>. “A vector coprocessor with a time counter for statically dispatching instructions ensures high utilization of wide execution units while avoiding misprediction penalties.”</p><p>In today’s CPUs, compilers and programmers write code assuming the hardware will dynamically reorder instructions and speculatively execute branches. The hardware handles hazards with register renaming, branch prediction and recovery mechanisms. Programmers benefit from performance, but at the cost of unpredictability and power consumption.</p><p>In the deterministic time-based architecture, instructions are dispatched only when the time counter indicates their operands will be ready. This means the compiler (or runtime system) doesn’t need to insert guard code for misprediction recovery. Instead, compiler scheduling becomes simpler, as instructions are guaranteed to issue at the correct cycle without rollbacks. For programmers, the ISA remains RISC-V compatible, but deterministic extensions reduce reliance on speculative safety nets.</p><h2>Application in AI and ML</h2><p>In <a href=\"https://venturebeat.com/ai/under-the-hood-of-ai-agents-a-technical-guide-to-the-next-frontier-of-gen-ai\">AI/ML kernels</a>, vector loads and matrix operations often dominate runtime. On a speculative CPU, misaligned or non-cacheable loads can trigger stalls or flushes, starving wide vector and matrix units and wasting energy on discarded work. A deterministic design instead issues these operations with cycle-accurate timing, ensuring high utilization and steady throughput. For programmers, this means fewer performance cliffs and more predictable scaling across problem sizes. And because the patents extend the RISC-V ISA rather than replace it, deterministic processors remain fully compatible with the RVA23 profile and mainstream toolchains such as GCC, LLVM, FreeRTOS, and Zephyr.</p><p>In practice, the deterministic model doesn’t change how code is written — it remains RISC-V assembly or high-level languages compiled to RISC-V instructions. What changes is the execution contract: Rather than relying on speculative guesswork, programmers can expect predictable latency behavior and higher efficiency without tuning code around microarchitectural quirks.</p><p>The industry is at an inflection point. AI/ML workloads are dominated by vector and matrix math, where GPUs and TPUs excel — but only by consuming massive power and adding architectural complexity. In contrast, general-purpose CPUs, still tied to speculative execution models, lag behind.</p><p>A deterministic processor delivers predictable performance across a wide range of workloads, ensuring consistent behavior regardless of task complexity. Eliminating speculative execution enhances energy efficiency and avoids unnecessary computational overhead. Furthermore, deterministic design scales naturally to vector and matrix operations, making it especially well-suited for AI workloads that rely on high-throughput parallelism. This new deterministic approach may represent the next such leap: The first major architectural challenge to speculation since speculation itself became the standard.</p><p>Will deterministic CPUs replace speculation in mainstream computing? That remains to be seen. But with issued patents, proven novelty and growing pressure from AI workloads, the timing is right for a paradigm shift. Taken together, these advances signal deterministic execution as the next architectural leap — redefining performance and efficiency just as speculation once did.</p><p>Speculation marked the last revolution in CPU design; determinism may well represent the next.</p><p><i>Thang Tran is the founder and CTO of Simplex Micro.</i></p><p><i>Read more from our </i><a href=\"https://venturebeat.com/datadecisionmakers\"><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href=\"https://r39crwmcu9m.typeform.com/to/NEzWFTji\"><i>guidelines here</i></a><i>. </i></p>",
    "published": "Sun, 02 Nov 2025 05:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-04T09:13:14.261006",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "Large reasoning models almost certainly can think",
    "link": "https://venturebeat.com/ai/large-reasoning-models-almost-certainly-can-think",
    "summary": "<p>Recently, there has been a lot of hullabaloo about the idea that large reasoning models (LRM) are unable to think. This is mostly due to a research article published by Apple, &quot;<a href=\"https://machinelearning.apple.com/research/illusion-of-thinking\"><u>The Illusion of Thinking</u></a>&quot; Apple argues that LRMs must not be able to think; instead, they just perform pattern-matching. The evidence they provided is that LRMs with <a href=\"https://venturebeat.com/ai/dont-believe-reasoning-models-chains-of-thought-says-anthropic\">chain-of-thought (CoT</a>) reasoning are unable to carry on the calculation using a predefined algorithm as the problem grows.</p><p>This is a fundamentally flawed argument. If you ask a human who already knows the algorithm for solving the Tower-of-Hanoi problem to solve a Tower-of-Hanoi problem with twenty discs, for instance, he or she would almost certainly fail to do so. By that logic, we must conclude that humans cannot think either. However, this argument only points to the idea that there is no evidence that LRMs cannot think. This alone certainly does not mean that LRMs can think — just that we cannot be sure they don’t.</p><p>In this article, I will make a bolder claim: LRMs almost certainly can think. I say ‘almost’ because there is always a chance that further research would surprise us. But I think my argument is pretty conclusive.</p><h2>What is thinking?</h2><p>Before we try to understand if LRMs can think, we need to define what we mean by thinking. But first, we have to make sure that humans can think per the definition. We will only consider thinking in relation to problem solving, which is the matter of contention.</p><p><b>1. Problem representation (frontal and parietal lobes)</b></p><p>When you think about a problem, the process engages your prefrontal cortex. This region is responsible for working memory, attention and executive functions — capacities that let you hold the problem in mind, break it into sub-components and set goals. Your parietal cortex helps encode symbolic structure for math or puzzle problems.</p><p><b>2. Mental simulation (morking Memory and inner speech)</b></p><p>This has two components: One is an auditory loop that lets you talk to yourself — very similar to <a href=\"https://venturebeat.com/ai/llms-generate-fluent-nonsense-when-reasoning-outside-their-training-zone\">CoT generation</a>. The other is visual imagery, which allows you to manipulate objects visually. Geometry was so important for navigating the world that we developed specialized capabilities for it. The auditory part is linked to Broca’s area and the auditory cortex, both reused from language centers. The visual cortex and parietal areas primarily control the visual component.</p><p><b>3. Pattern matching and retrieval (Hippocampus and Temporal Lobes)</b></p><p>These actions depend on past experiences and stored knowledge from long-term memory:</p><ul><li><p>The hippocampus helps retrieve related memories and facts.</p></li><li><p>The temporal Lobe brings in semantic knowledge — meanings, rules, categories.</p></li></ul><p>This is similar to how neural networks depend on their training to process the task.</p><p><b>4. Monitoring and evaluation (Anterior Cingulate Cortex)</b></p><p>Our anterior cingulate cortex (ACC) monitors for errors, conflicts or impasses — it’s where you notice contradictions or dead ends. This process is essentially based on pattern matching from prior experience.</p><p><b>5. Insight or reframing (default mode network and right hemisphere)</b></p><p>When you&#x27;re stuck, your brain might shift into <b>default mode </b>— a more relaxed, internally-directed network. This is when you step back, let go of the current thread and sometimes ‘suddenly’ see a new angle (the classic “aha!” moment).</p><p>This is similar to how <b>DeepSeek-R1</b> was trained for CoT reasoning without having CoT examples in its training data. Remember, the brain continuously learns as it processes data and solves problems.</p><p>In contrast, <b>LRMs</b> aren’t allowed to change based on real-world feedback during prediction or generation. But with DeepSeek-R1’s CoT training, learning <i>did</i> happen as it attempted to solve the problems — essentially updating while reasoning.</p><h2>Similarities betweem CoT reasoning and biological thinking</h2><p>LRM does not have all of the faculties mentioned above. For example, an LRM is very unlikely to do too much visual reasoning in its circuit, although a little may happen. But it certainly does not generate intermediate images in the CoT generation.</p><p>Most humans can make spatial models in their heads to solve problems. Does this mean we can conclude that LRMs cannot think? I would disagree. Some humans also find it difficult to form spatial models of the concepts they think about. This condition is called <i>aphantasia</i>. People with this condition can think just fine. In fact, they go about life as if they don’t lack any ability at all. Many of them are actually great at symbolic reasoning and quite good at math — often enough to compensate for their lack of visual reasoning. We might expect our neural network models also to be able to circumvent this limitation.</p><p>If we take a more abstract view of the human thought process described earlier, we can see mainly the following things involved:</p><p>1.  Pattern-matching is used for recalling learned experience, problem representation and monitoring and evaluating chains of thought.</p><p>2.  Working memory is to store all the intermediate steps.</p><p>3.  Backtracking search concludes that the CoT is not going anywhere and backtracks to some reasonable point.</p><p>Pattern-matching in an LRM <a href=\"https://venturebeat.com/ai/abstract-or-die-why-ai-enterprises-cant-afford-rigid-vector-stacks\">comes from its training</a>. The whole point of training is to learn both knowledge of the world and the patterns to process that knowledge effectively. Since an LRM is a layered network, the entire working memory needs to fit within one layer. The weights store the knowledge of the world and the patterns to follow, while processing happens between layers using the learned patterns stored as model parameters.</p><p>Note that even in CoT, the entire text — including the input, CoT and part of the output already generated — must fit into each layer. Working memory is just one layer (in the case of the attention mechanism, this includes the KV-cache).</p><p>CoT is, in fact, very similar to what we do when we are talking to ourselves (which is almost always). We nearly always verbalize our thoughts, and so does a CoT reasoner.</p><p>There is also good evidence that CoT reasoner can take backtracking steps when a certain line of reasoning seems futile. In fact, this is what the Apple researchers saw when they tried to ask the LRMs to solve bigger instances of simple puzzles. The LRMs correctly recognized that trying to solve the puzzles directly would not fit in their working memory, so they tried to figure out better shortcuts, just like a human would do. This is even more evidence that LRMs are thinkers, not just blind followers of predefined patterns.</p><h2>But why would a next-token-predictor learn to think?</h2><p><a href=\"https://www.talentica.com/blogs/why-neural-networks-can-learn-anything/\"><u>Neural networks of sufficient size can learn any computation, including thinking</u></a>. But a next-word-prediction system can also learn to think. Let me elaborate. </p><p>A general idea is LRMs cannot think because, at the end of the day, they are just predicting the next token; it is only a &#x27;glorified auto-complete.&#x27; This view is fundamentally incorrect — not that it is an &#x27;auto-complete,&#x27; but that an &#x27;auto-complete&#x27; does not have to think. In fact, next word prediction is far from a limited representation of thought. On the contrary, it is the most general form of knowledge representation that anyone can hope for. Let me explain.</p><p>Whenever we want to represent some knowledge, we need a language or a system of symbolism to do so. Different formal languages exist that are very precise in terms of what they can express. However, such languages are fundamentally limited in the kinds of knowledge they can represent.</p><p>For example, first-order predicate logic cannot represent properties of all predicates that satisfy a certain property, because it doesn&#x27;t allow predicates over predicates.</p><p>Of course, there are higher-order predicate calculi that can represent predicates on predicates to arbitrary depths. But even they cannot express ideas that lack precision or are abstract in nature.</p><p>Natural language, however, is complete in expressive power — you can describe any concept in any level of detail or abstraction. In fact, you can even describe concepts <i>about</i> natural language using natural language itself. That makes it a strong candidate for knowledge representation.</p><p>The challenge, of course, is that this expressive richness makes it harder to process the information encoded in natural language. But we don’t necessarily need to understand how to do it manually — we can simply program the machine using data, through a process called training.</p><p>A next-token prediction machine essentially computes a probability distribution over the next token, given a context of preceding tokens. Any machine that aims to compute this probability accurately must, in some form, represent world knowledge.</p><p>A simple example: Consider the incomplete sentence, &quot;The highest mountain peak in the world is Mount ...&quot; — to predict the next word as Everest, the model must have this knowledge stored somewhere. If the task requires the model to compute the answer or solve a puzzle, the next-token predictor needs to output CoT tokens to carry the logic forward.</p><p>This implies that, even though it’s predicting one token at a time, the model must internally represent at least the next few tokens in its working memory — enough to ensure it stays on the logical path.</p><p>If you think about it, humans also predict the next token — whether during speech or when thinking using the inner voice. A perfect auto-complete system that always outputs the right tokens and produces correct answers would have to be omniscient. Of course, we’ll never reach that point — because not every answer is computable.</p><p>However, a parameterized model that can represent knowledge by tuning its parameters, and that can learn through data and reinforcement, can certainly learn to think.</p><h2>Does it produce the effects of thinking?</h2><p>At the end of the day, the ultimate test of thought is a system’s ability to solve problems that require thinking. If a system can answer previously unseen questions that demand some level of reasoning, it must have learned to think — or at least to reason — its way to the answer.</p><p>We know that proprietary LRMs perform very well on certain reasoning benchmarks. However, since there&#x27;s a possibility that some of these models were fine-tuned on benchmark test sets through a backdoor, we’ll focus only on <b>open-source models</b> for fairness and transparency.</p><p>We evaluate them using the following benchmarks:</p><p>As one can see, in some benchmarks, LRMs are able to solve a significant number of logic-based questions. While it’s true that they still lag behind human performance in many cases, it’s important to note that the human baseline often comes from individuals trained specifically on those benchmarks. In fact, in certain cases, LRMs outperform the average untrained human.</p><h2>Conclusion</h2><p>Based on the benchmark results, the striking similarity between CoT reasoning and biological reasoning, and the theoretical understanding that any system with sufficient representational capacity, enough training data, and adequate computational power can perform any computable task — LRMs meet those criteria to a considerable extent.</p><p>It is therefore reasonable to conclude that LRMs almost certainly possess the ability to think.</p><p><i>Debasish Ray Chawdhuri is a senior principal engineer at</i><a href=\"https://www.talentica.com/\"><i> </i><i><u>Talentica Software</u></i></a><i> and a Ph.D. candidate in Cryptography at IIT Bombay. </i></p><p><i>Read more from our </i><a href=\"https://venturebeat.com/datadecisionmakers\"><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href=\"https://r39crwmcu9m.typeform.com/to/NEzWFTji\"><i>guidelines here</i></a><i>. </i></p>",
    "published": "Sat, 01 Nov 2025 05:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-04T09:13:14.261536",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "CrowdStrike & NVIDIA’s open source AI gives enterprises the edge against machine-speed attacks",
    "link": "https://venturebeat.com/security/crowdstrike-nvidia-open-source-ai-soc-machine-speed-attacks",
    "summary": "<p>Every SOC leader knows the feeling: drowning in alerts, blind to the real threat, stuck playing defense in a war waged at the speed of AI. </p><p>Now <a href=\"https://www.crowdstrike.com/en-us/platform/charlotte-ai/\">CrowdStrike</a> and <a href=\"https://www.nvidia.com/en-us/\">NVIDIA</a> are flipping the script. Armed with autonomous agents powered by Charlotte AI and NVIDIA Nemotron models, security teams aren&#x27;t just reacting; they&#x27;re striking back at attackers before their next move. Welcome to cybersecurity&#x27;s new arms race. Combining open source&#x27;s many strengths with agentic AI will shift the balance of power against adversarial AI. </p><p>CrowdStrike and NVIDIA&#x27;s agentic ecosystem combines <a href=\"https://www.crowdstrike.com/en-us/platform/charlotte-ai/\">Charlotte AI AgentWorks,</a> <a href=\"https://www.nvidia.com/en-us/ai-data-science/foundation-models/nemotron/\">NVIDIA Nemotron</a> open models, <a href=\"https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/index.html\">NVIDIA NeMo Data Designer</a> synthetic data, <a href=\"https://developer.nvidia.com/nemo-agent-toolkit\">NVIDIA Nemo Agent Toolkit</a>, and <a href=\"https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/\">NVIDIA NIM microservices</a>. </p><p>&quot;This collaboration redefines security operations by enabling analysts to build and deploy specialized <a href=\"https://www.nvidia.com/en-us/glossary/ai-agents/\">AI agents</a> at scale, leveraging trusted, enterprise-grade security with Nemotron models,&quot; <a href=\"https://blogs.nvidia.com/blog/nemotron-open-source-ai/\">writes </a>Bryan Catanzaro, vice president, Applied Deep Learning Research at NVIDIA. </p><p>The partnership is designed to enable autonomous agents to learn quickly, reducing risks, threats, and false positives. Achieving that takes a heavy load off SOC leaders and their teams, who fight data fatigue nearly every day due to inaccurate data.</p><p>The announcement at GTC Washington, D.C., signals the arrival of machine-speed defense that can finally match machine-speed attacks. </p><h2><b>Transforming elite analyst expertise into datasets at machine scale</b></h2><p>The partnership is differentiated by how the AI agents are designed to continually aggregate telemetry data, including insights from <a href=\"https://www.crowdstrike.com/en-us/services/falcon-complete-next-gen-mdr/\">CrowdStrike Falcon Complete Managed Detection and Response</a> analysts. </p><p>&quot;What we&#x27;re able to do is take the intelligence, take the data, take the experience of our Falcon Complete analysts, and turn these experts into datasets. Turn the datasets into AI models, and then be able to create agents based on, really, the whole composition and experience that we&#x27;ve built up within the company so that our customers can benefit at scale from these agents always,&quot; said Daniel Bernard, CrowdStrike&#x27;s Chief Business Officer, during a recent briefing. </p><p>Capitalizing on the strengths of the NVIDIA Nemotron open models, organizations will be able to have their autonomous agents continually learn by training on the datasets from Falcon Complete, the world&#x27;s largest MDR service handling millions of triage decisions monthly. </p><p>CrowdStrike has previous experience in AI detection triage to the point of launching a service that scales this capability across its customer base. <a href=\"https://www.crowdstrike.com/en-us/platform/charlotte-ai/\">Charlotte AI Detection Triage,</a> designed to integrate into existing <a href=\"https://venturebeat.com/security/the-ai-paradox-how-tomorrows-cutting-edge-tools-can-become-dangerous-cyber-threats-and-how-to-prepare/\">security workflows</a> and continuously adapt to evolving threats, automates alert assessment with over 98% accuracy and cuts manual triage by more than 40 hours per week.</p><p>Elia Zaitsev, CrowdStrike&#x27;s chief technology officer, in explaining how Charlotte AI Detection Triage is able to deliver that level of performance, <a href=\"https://venturebeat.com/security/crowdstrikes-ai-slashes-soc-workloads-over-40-hours-a-week\"><b>told VentureBeat</b></a>: &quot;We wouldn&#x27;t have achieved this without the support of our Falcon Complete team. They perform triage within their workflow, manually addressing millions of detections. The high-quality, human-annotated dataset they provide is what enabled us to reach an accuracy of over 98%.&quot;</p><p>Lessons learned with Charlotte AI Detection Triage directly apply to the NVIDIA partnership, further increasing the value it has the potential to deliver to SOCs who need help dealing with the deluge of alerts. </p><h2><b>Open source is table stakes for this partnership to work </b></h2><p>NVIDIA&#x27;s Nemotron open models address what many security leaders identify as the most critical barrier to AI adoption in regulated environments, which is the lack of clarity regarding how the model works, what its weights are, and how secure it is. </p><p>Justin Boitano, Vice President, Enterprise and Edge Computing at NVIDIA, speaking for NVIDIA during a recent press briefing, explained: &quot;Open models are where people start in trying to build their own specialized domain knowledge. You want to own the IP ultimately. Not everybody wants to export their data, and then sort of import or pay for the intelligence that they consume. A lot of sovereign countries, many enterprises in regulated industries want to maintain all that data privacy and security.&quot; </p><p>John Morello, CTO and co-founder of Gutsy (now <a href=\"https://www.minimus.io/\">Minimus</a>), <a href=\"https://venturebeat.com/security/how-open-source-llms-enable-security-teams-to-stay-ahead-of-evolving-threats\"><b>told VentureBeat</b></a> that &quot;the open-source nature of <a href=\"https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/\">Google&#x27;s BERT</a> open-source language model allows Gutsy to customize and train their model for specific security use cases while maintaining privacy and efficiency.&quot; Morello emphasized that practitioners cite &quot;more transparency and better assurances of data privacy, along with great availability of expertise and more integration options across their architectures, as key reasons for going with open source.&quot;</p><h2><b>Keeping adversarial AI&#x27;s balance of power in check </b></h2><p>Cisco&#x27;s DJ Sampath, senior vice president of Cisco&#x27;s AI software and platform group, articulated the industry-wide imperative for open-source security models <a href=\"https://venturebeat.com/security/cisco-warns-enterprises-without-tapping-machine-data-your-ai-strategy-is\"><b>during a recent interview with VentureBeat</b></a>: &quot;The reality is that attackers have access to open-source models too. The goal is to empower as many defenders as possible with robust models to strengthen security.&quot; </p><p>Sampath explained that when Cisco released Foundation-Sec-8B, their open-source security model, at RSAC 2025, it was driven by a sense of responsibility: &quot;Funding for open-source projects has stalled, and there is a growing need for sustainable funding sources within the community. It is a corporate responsibility to provide these models while enabling communities to engage with AI from a defensive standpoint.&quot;</p><p>The commitment to transparency extends to the most sensitive aspects of AI development. When concerns emerged about DeepSeek R1&#x27;s training data and potential compromise, NVIDIA responded decisively. </p><p>As Boitano explained to VentureBeat, &quot;Government agencies were super concerned. They wanted the reasoning capabilities of DeepSeek, but they were a little concerned with, obviously, what might be trained into the DeepSeek model, which is what actually inspired us to completely open source everything in Nemotron models, including reasoning datasets.&quot;</p><p>For practitioners managing open-source security at scale, this transparency is core to their companies. Itamar Sher, CEO of <a href=\"https://www.seal.security/\">Seal Security</a>, emphasized <a href=\"https://venturebeat.com/security/how-open-source-llms-enable-security-teams-to-stay-ahead-of-evolving-threats\"><b>to VentureBeat</b></a> that &quot;open-source models offer transparency,&quot; though he noted that &quot;managing their cycles and compliance remains a significant concern.&quot; Sher&#x27;s company uses generative AI to automate vulnerability remediation in open-source software, and as a recognized CVE Naming Authority (CNA), Seal can identify, document, and assign vulnerabilities, enhancing security across the ecosystem.</p><h2><b>A key partnership goal: bringing intelligence to the Edge</b></h2><p>&quot;Bringing the intelligence closer to where data is and decisions are made is just going to be a big advancement for security operations teams around the industry,&quot; Boitano emphasized. This edge deployment capability is especially critical for government agencies with fragmented and often legacy IT environments. </p><p>VentureBeat asked Boitano how the initial discussions went with government agencies briefed on the partnership and its design goals before work began. &quot;The feeling across agencies that we&#x27;ve talked to is they always feel like, unfortunately, they&#x27;re behind the curve on these technology adoption,&quot; Boitano explained. &quot;The response was, anything you guys can do to help us secure the endpoints. It was a tedious and long process to get open models onto these, you know, higher side networks.&quot;</p><p>NVIDIA and CrowdStrike have done the foundational work, including STIG hardening, FIPS encryption, air-gap compatibility, and removing the barriers that delayed open-model adoption on higher-side networks. The <a href=\"https://blogs.nvidia.com/blog/us-technology-leaders-ai-factory-design-government/\">NVIDIA AI Factory for Government reference design</a> provides comprehensive guidance for deploying AI agents in federal and high-assurance organizations while meeting the strictest security requirements.</p><p>As Boitano explained, the urgency is existential: &quot;Having AI defense that&#x27;s running in your estate that can search for and detect these anomalies, and then alert and respond much faster, is just the natural consequence. It&#x27;s the only way to protect against the speed of AI at this point.&quot;</p>",
    "published": "Sat, 01 Nov 2025 01:10:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-11-04T09:13:14.261991",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "DevOps for AI: Continuous deployment pipelines for machine learning systems",
    "link": "https://www.artificialintelligence-news.com/news/devops-for-ai-continuous-deployment-pipelines-for-machine-learning-systems/",
    "summary": "<p>AI&#8217;s effects on continuous development and deployment pipelines are becoming difficult to ignore. However, decision-makers in software development functions need to consider a broad range of elements when considering the uses of the technology. The challenges of deploying AI at scale Deploying artificial intelligence isn&#8217;t the same as deploying, for example, a web app. Traditional [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/devops-for-ai-continuous-deployment-pipelines-for-machine-learning-systems/\">DevOps for AI: Continuous deployment pipelines for machine learning systems</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Mon, 03 Nov 2025 10:31:40 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-04T09:13:18.451752",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-03T10:31:40+00:00",
    "days_old": 1,
    "priority_score": 0.855
  },
  {
    "title": "How Lumana is redefining AI’s role in video surveillance",
    "link": "https://www.artificialintelligence-news.com/news/how-lumana-is-redefining-ais-role-in-video-surveillance/",
    "summary": "<p>For all the progress in artificial intelligence, most video security systems still fail at recognising context in real-world conditions. The majority of cameras can capture real-time footage, but struggle to interpret it. This is a problem turning into a growing concern for smart city designers, manufacturers and schools, each of which may depend on AI [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/how-lumana-is-redefining-ais-role-in-video-surveillance/\">How Lumana is redefining AI&#8217;s role in video surveillance</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Fri, 31 Oct 2025 15:36:18 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-04T09:13:18.462454",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-31T15:36:18+00:00",
    "days_old": 4,
    "priority_score": 0.855
  },
  {
    "title": "Finding return on AI investments across industries",
    "link": "https://www.technologyreview.com/2025/10/28/1126693/finding-return-on-ai-investments-across-industries/",
    "summary": "The market is officially three years post ChatGPT and many of the pundit bylines have shifted to using terms like “bubble” to suggest reasons behind generative AI not realizing material returns outside a handful of technology suppliers.&#160; In September, the MIT NANDA report made waves because the soundbite every author and influencer picked up on&#8230;",
    "published": "Tue, 28 Oct 2025 15:00:33 +0000",
    "source_name": "mit_tech_review",
    "source_url": "https://www.technologyreview.com/topic/artificial-intelligence/feed",
    "category": "ai_research",
    "weight": 0.9,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-04T09:13:19.738971",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.54
  },
  {
    "title": "Fox News Falls for AI-Generated Footage of Poor People Raging About Food Stamps Being Shut Down, Runs False Story That Has to Be Updated With Huge Correction",
    "link": "https://www.reddit.com/r/artificial/comments/1oo3zcb/fox_news_falls_for_aigenerated_footage_of_poor/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1oo3zcb/fox_news_falls_for_aigenerated_footage_of_poor/\"> <img alt=\"Fox News Falls for AI-Generated Footage of Poor People Raging About Food Stamps Being Shut Down, Runs False Story That Has to Be Updated With Huge Correction\" src=\"https://external-preview.redd.it/EN1GVlrv_YL1JloSoU0Ii9djlR9IB5zhC2ZeM8kGV4c.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=efc757b25d4f8b62612f70d2c7d13892f2ba6bb0\" title=\"Fox News Falls for AI-Generated Footage of Poor People Raging About Food Stamps Being Shut Down, Runs False Story That Has to Be Updated With Huge Correction\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MetaKnowing\"> /u/MetaKnowing </a> <br /> <span><a href=\"https://www.yahoo.com/news/articles/fox-news-falls-racist-ai-183734288.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1oo3zcb/fox_news_falls_for_aigenerated_footage_of_poor/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-11-04T10:30:02+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-04T09:13:22.888200",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-04T10:30:02+00:00",
    "days_old": 0,
    "priority_score": 0.63
  },
  {
    "title": "Sam Altman sometimes wishes OpenAI were public so haters could short the stock — ‘I would love to see them get burned on that’ | Fortune",
    "link": "https://www.reddit.com/r/artificial/comments/1onjcou/sam_altman_sometimes_wishes_openai_were_public_so/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1onjcou/sam_altman_sometimes_wishes_openai_were_public_so/\"> <img alt=\"Sam Altman sometimes wishes OpenAI were public so haters could short the stock — ‘I would love to see them get burned on that’ | Fortune\" src=\"https://external-preview.redd.it/zITmkmNMIX7z0_BfoYYb6hnJQwZmTetRlcqaotwH3rI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a42476c0a41fba2ab16930d9faec477cd2c8985d\" title=\"Sam Altman sometimes wishes OpenAI were public so haters could short the stock — ‘I would love to see them get burned on that’ | Fortune\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fortune\"> /u/fortune </a> <br /> <span><a href=\"https://fortune.com/2025/11/03/sam-altman-openai-public-short-sellers-stock-get-burned-ipo/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1onjcou/sam_altman_sometimes_wishes_openai_were_public_so/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-11-03T18:15:04+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-11-04T09:13:22.888549",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-03T18:15:04+00:00",
    "days_old": 0,
    "priority_score": 0.63
  },
  {
    "title": "OpenAI signs massive AI compute deal with Amazon",
    "link": "https://arstechnica.com/ai/2025/11/openai-signs-massive-ai-compute-deal-with-amazon/",
    "summary": "Deal will provide access to hundreds of thousands of Nvidia chips that power ChatGPT.",
    "published": "Mon, 03 Nov 2025 17:23:11 +0000",
    "source_name": "ars_technica",
    "source_url": "https://feeds.arstechnica.com/arstechnica/index",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-11-04T09:13:26.640459",
    "ready_for_mytribal": true,
    "parsed_date": "2025-11-03T17:23:11+00:00",
    "days_old": 0,
    "priority_score": 0.9600000000000002
  }
]