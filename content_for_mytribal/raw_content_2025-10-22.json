[
  {
    "title": "GM is bringing Google Gemini-powered AI assistant to cars in 2026",
    "link": "https://techcrunch.com/2025/10/22/gm-is-bringing-google-gemini-powered-ai-assistant-to-cars-in-2026/",
    "summary": "GM is the latest automaker to lean into generative AI-based assistants that promise to respond to driver requests in a more natural-sounding way. Stellantis is collaborating with French AI firm Mistral, Mercedes is integrating ChatGPT, and Tesla has brought xAI’s Grok to its vehicles.",
    "published": "Wed, 22 Oct 2025 15:00:00 +0000",
    "source_name": "tech_crunch",
    "source_url": "https://techcrunch.com/feed/",
    "category": "tech_news",
    "weight": 0.9,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:02.240634",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-22T15:00:00+00:00",
    "days_old": 0,
    "priority_score": 0.81
  },
  {
    "title": "Several users reportedly complain to FTC that ChatGPT is causing psychological harm",
    "link": "https://techcrunch.com/2025/10/22/several-users-reportedly-complain-to-ftc-that-chatgpt-is-causing-psychological-harm/",
    "summary": "At least seven people have complained to the U.S. Federal Trade Commission that ChatGPT caused them to experience severe delusions, paranoia and emotional crises, Wired reported.",
    "published": "Wed, 22 Oct 2025 14:16:45 +0000",
    "source_name": "tech_crunch",
    "source_url": "https://techcrunch.com/feed/",
    "category": "tech_news",
    "weight": 0.9,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:02.242516",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-22T14:16:45+00:00",
    "days_old": 0,
    "priority_score": 0.81
  },
  {
    "title": "Simplifying the AI stack: The key to scalable, portable intelligence from cloud to edge",
    "link": "https://venturebeat.com/ai/simplifying-the-ai-stack-the-key-to-scalable-portable-intelligence-from",
    "summary": "<p><i>Presented by Arm</i></p><hr /><p>A simpler software stack is the key to portable, scalable AI across cloud and edge.<b> </b></p><p>AI is now powering real-world applications, yet fragmented software stacks are holding it back. Developers routinely rebuild the same models for different hardware targets, losing time to glue code instead of shipping features. The good news is that a shift is underway. Unified toolchains and optimized libraries are making it possible to deploy models across platforms without compromising performance.</p><p>Yet one critical hurdle remains: software complexity. Disparate tools, hardware-specific optimizations, and layered tech stacks continue to bottleneck progress. To unlock the next wave of AI innovation, the industry must pivot decisively away from siloed development and toward streamlined, end-to-end platforms.</p><p>This transformation is already taking shape. Major cloud providers, edge platform vendors, and open-source communities are converging on unified toolchains that simplify development and accelerate deployment, from cloud to edge. In this article, we’ll explore why simplification is the key to scalable AI, what’s driving this momentum, and how next-gen platforms are turning that vision into real-world results.</p><h3><b>The bottleneck: fragmentation, complexity, and inefficiency</b></h3><p>The issue isn’t just hardware variety; it’s duplicated effort across frameworks and targets that slows time-to-value.</p><p><b>Diverse hardware targets</b>: GPUs, NPUs, CPU-only devices, mobile SoCs, and custom accelerators.</p><p><b>Tooling and framework fragmentation</b>: TensorFlow, PyTorch, ONNX, MediaPipe, and others.</p><p><b>Edge constraints</b>: Devices require real-time, energy-efficient performance with minimal overhead.</p><p>According to <a href=\"https://www.gartner.com/en/documents/3994810\">Gartner Research</a>, these mismatches create a key hurdle: over 60% of AI initiatives stall before production, driven by integration complexity and performance variability. </p><h3><b>What software simplification looks like</b></h3><p>Simplification is coalescing around five moves that cut re-engineering cost and risk:</p><p><b>Cross-platform abstraction layers</b> that minimize re-engineering when porting models.</p><p><b>Performance-tuned libraries</b> integrated into major ML frameworks.</p><p><b>Unified architectural designs</b> that scale from datacenter to mobile.</p><p><b>Open standards and runtimes</b> (e.g., ONNX, MLIR) reducing lock-in and improving compatibility.</p><p><b>Developer-first ecosystems</b> emphasizing speed, reproducibility, and scalability.</p><p>These shifts are making AI more accessible, especially for startups and academic teams that previously lacked the resources for bespoke optimization. Projects like Hugging Face’s Optimum and MLPerf benchmarks are also helping standardize and validate cross-hardware performance.</p><p><b>Ecosystem momentum and real-world signals</b> Simplification is no longer aspirational; it’s happening now. Across the industry, software considerations are influencing decisions at the IP and silicon design level, resulting in solutions that are production-ready from day one. Major ecosystem players are driving this shift by aligning hardware and software development efforts, delivering tighter integration across the stack.</p><p>A key catalyst is the rapid rise of edge inference, where AI models are deployed directly on devices rather than in the cloud. This has intensified demand for streamlined software stacks that support end-to-end optimization, from silicon to system to application. Companies like Arm are responding by enabling tighter coupling between their compute platforms and software toolchains, helping developers accelerate time-to-deployment without sacrificing performance or portability. The emergence of multi-modal and general-purpose foundation models (e.g., LLaMA, Gemini, Claude) has also added urgency. These models require flexible runtimes that can scale across cloud and edge environments. AI agents, which interact, adapt, and perform tasks autonomously, further drive the need for high-efficiency, cross-platform software.</p><p>MLPerf Inference v3.1 included over 13,500 performance results from 26 submitters, validating multi-platform benchmarking of AI workloads. Results spanned both data center and edge devices, demonstrating the diversity of optimized deployments now being tested and shared.</p><p>Taken together, these signals make clear that the market’s demand and incentives are aligning around a common set of priorities, including maximizing performance-per-watt, ensuring portability, minimizing latency, and delivering security and consistency at scale.</p><h3><b>What must happen for successful simplification</b></h3><p>To realize the promise of simplified AI platforms, several things must occur:</p><p><b>Strong hardware/software co-design</b>: hardware features that are exposed in software frameworks (e.g., matrix multipliers, accelerator instructions), and conversely, software that is designed to take advantage of underlying hardware.</p><p><b>Consistent, robust toolchains and libraries</b>: developers need reliable, well-documented libraries that work across devices. Performance portability is only useful if the tools are stable and well supported.</p><p><b>Open ecosystem</b>: hardware vendors, software framework maintainers, and model developers need to cooperate. Standards and shared projects help avoid re-inventing the wheel for every new device or use case.</p><p><b>Abstractions that don’t obscure performance</b>: while high-level abstraction helps developers, they must still allow tuning or visibility where needed. The right balance between abstraction and control is key.</p><p><b>Security, privacy, and trust built in</b>: especially as more compute shifts to devices (edge/mobile), issues like data protection, safe execution, model integrity, and privacy matter.</p><h3><b>Arm as one example of ecosystem-led simplification </b></h3><p>Simplifying AI at scale now hinges on system-wide design, where silicon, software, and developer tools evolve in lockstep. This approach enables AI workloads to run efficiently across diverse environments, from cloud inference clusters to battery-constrained edge devices. It also reduces the overhead of bespoke optimization, making it easier to bring new products to market faster. Arm (Nasdaq:Arm) is advancing this model with a platform-centric focus that pushes hardware-software optimizations up through the software stack. At <a href=\"https://newsroom.arm.com/blog/arm-computex-2025?utm_source=vb&amp;utm_medium=sponsored-content&amp;utm_content=longform_txt_na_sw-simplification&amp;utm_campaign=mk30_brand-paid_brand-tl_thirdparty_mediabuy_na\">COMPUTEX 2025</a>, Arm demonstrated how its latest Arm9 CPUs, combined with AI-specific ISA extensions and the Kleidi libraries, enable tighter integration with widely used frameworks like PyTorch, ExecuTorch, ONNX Runtime, and MediaPipe. This alignment reduces the need for custom kernels or hand-tuned operators, allowing developers to unlock hardware performance without abandoning familiar toolchains. </p><p>The real-world implications are significant. In the data center, Arm-based platforms are delivering improved performance-per-watt, critical for scaling AI workloads sustainably. On consumer devices, these optimizations enable ultra-responsive user experiences and background intelligence that’s always on, yet power efficient.</p><p>More broadly, the industry is coalescing around simplification as a design imperative, embedding AI support directly into hardware roadmaps, optimizing for software portability, and standardizing support for mainstream AI runtimes. Arm’s approach illustrates how deep integration across the compute stack can make scalable AI a practical reality.</p><h3><b>Market validation and momentum</b></h3><p>In 2025, <a href=\"https://newsroom.arm.com/blog/half-of-compute-shipped-to-top-hyperscalers-in-2025-will-be-arm-based?utm_source=vb&amp;utm_medium=sponsored-content&amp;utm_content=longform_txt_na_sw-simplification&amp;utm_campaign=mk30_brand-paid_brand-tl_thirdparty_mediabuy_na\">nearly half of the compute shipped to major hyperscalers will run on Arm-based architectures</a>, a milestone that underscores a significant shift in cloud infrastructure. As AI workloads become more resource-intensive, cloud providers are prioritizing architectures that deliver superior performance-per-watt and support seamless software portability. This evolution marks a strategic pivot toward energy-efficient, scalable infrastructure optimized for the performance and demands of modern AI.</p><p>At the edge, Arm-compatible inference engines are enabling real-time experiences, such as live translation and always-on voice assistants, on battery-powered devices. These advancements bring powerful AI capabilities directly to users, without sacrificing energy efficiency.</p><p>Developer momentum is accelerating as well. In a recent collaboration, GitHub and Arm introduced native Arm Linux and Windows runners for GitHub Actions, streamlining CI workflows for Arm-based platforms. These tools lower the barrier to entry for developers and enable more efficient, cross-platform development at scale. </p><h3><b>What comes next</b></h3><p>Simplification doesn’t mean removing complexity entirely; it means managing it in ways that empower innovation. As the AI stack stabilizes, winners will be those who deliver seamless performance across a fragmented landscape.</p><p>From a future-facing perspective, expect:</p><p><b>Benchmarks as guardrails:</b> MLPerf + OSS suites guide where to optimize next.</p><p><b>More upstream, fewer forks:</b> Hardware features land in mainstream tools, not custom branches.</p><p><b>Convergence of research + production:</b> Faster handoff from papers to product via shared runtimes.</p><h2><b>Conclusion</b></h2><p>AI’s next phase isn’t about exotic hardware; it’s also about software that travels well. When the same model lands efficiently on cloud, client, and edge, teams ship faster and spend less time rebuilding the stack.</p><p>Ecosystem-wide simplification, not brand-led slogans, will separate the winners. The practical playbook is clear: unify platforms, upstream optimizations, and measure with open benchmarks. <a href=\"https://www.arm.com/markets/artificial-intelligence/software?utm_source=vb&amp;utm_medium=sponsored-content&amp;utm_content=longform_txt_na_sw-simplification&amp;utm_campaign=mk30_brand-paid_brand-tl_thirdparty_mediabuy_na\">Explore how Arm AI software platforms</a> are enabling this future — efficiently, securely, and at scale.</p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p><p>\n</p>",
    "published": "Wed, 22 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:03.536983",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "Qwen's new Deep Research update lets you turn its reports into webpages, podcasts in seconds",
    "link": "https://venturebeat.com/ai/qwens-new-deep-research-update-lets-you-turn-its-reports-into-webpages",
    "summary": "<p>Chinese e-commerce giant Alibaba’s <a href=\"https://venturebeat.com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks\">famously prolific Qwen Team</a> of AI model researchers and engineers has introduced a major expansion to its Qwen Deep Research tool, which is available as an optional modality the user can activate on the web-based Qwen Chat (a competitor to ChatGPT).</p><p>The update lets users generate not only comprehensive research reports with well-organized citations, but also interactive web pages and multi-speaker podcasts — all within 1-2 clicks.</p><p>This functionality is part of a <b>proprietary release</b>, distinct from many of Qwen’s previous open-source model offerings. </p><p>While the feature relies on the open-source models <b>Qwen3-Coder</b>, <b>Qwen-Image</b>, and <b>Qwen3-TTS</b> to power its core capabilities, the end-to-end experience — including research execution, web deployment, and audio generation — is <b>hosted and operated by Qwen</b>. </p><p>This means users benefit from a managed, integrated workflow without needing to configure infrastructure. That said, developers with access to the open-source models could theoretically replicate similar functionality on private or commercial systems.</p><p>The update was announced via the team’s official<a href=\"https://x.com/Alibaba_Qwen/status/1980609551486624237\"> X account (@Alibaba_Qwen)</a> today, October 21, 2025, stating:</p><blockquote><p>“Qwen Deep Research just got a major upgrade. It now creates not only the report, but also a live webpage and a podcast — powered by Qwen3-Coder, Qwen-Image, and Qwen3-TTS. Your insights, now visual and audible.”</p></blockquote><h3><b>Multi-Format Research Output</b></h3><p>The core workflow begins with a user request inside the Qwen Chat interface. From there, Qwen collaborates by asking clarifying questions to shape the research scope, pulls data from the web and official sources, and analyzes or resolves any inconsistencies it finds — even generating custom code when needed.</p><p>A <a href=\"https://x.com/Alibaba_Qwen/status/1980609551486624237\">demo video posted by Qwen on X</a> walks through this process on Qwen Chat using the U.S. SaaS market as an example. </p><p>In it, Qwen retrieves data from multiple industry sources, identifies discrepancies in market size estimates (e.g., $206 billion vs. $253 billion), and highlights ambiguities in the U.S. share of global figures. The assistant comments on differences in scope between sources and calculates a compound annual growth rate (CAGR) of 19.8% from 2020 to 2023, providing contextual analysis to back up the raw numbers.</p><p>Once the research is complete, users can click on the &quot;eyeball&quot; icon below the output result (see screenshot), which will bring up a PDF-style report in the right hand pane.</p><p>Then, when viewing the report in the right-hand pane, the user can click the &quot;Create&quot; button in the upper-right hand corner and select from the following two options:</p><ol><li><p><b>&quot;Web Dev&quot; </b>which produces a <b>live, professional-grade web page</b>, automatically deployed and <b>hosted by Qwen</b>, using Qwen3-Coder for structure and Qwen-Image for visuals.</p></li><li><p>&quot;<b>Podcast</b>,&quot; which, as it states, produces an audio <b>podcast</b>, featuring dynamic, multi-speaker narration generated by Qwen3-TTS, also <b>hosted by Qwen</b> for easy sharing and playback.</p></li></ol><p>This enables users to quickly convert a single research project into multiple forms of content — written, visual, and audible — with minimal extra input.</p><p>The website includes inline graphics generated by Qwen Image, making it suitable for use in public presentations, classrooms, or publishing. </p><p>The podcast feature allows users to select between 17 different speaker names as the host and 7 as the co-host, though I wasn&#x27;t able to find a way to preview the voice outputs before selecting them. It appears designed for deep listening on the go. </p><p>There was no way to change the language output that I could see, so mine came out in English, like my reports and initial prompts, though the Qwen LLMs are multi-modal. The voices were slightly more robotic than other AI tools I&#x27;ve used.</p><p>Here&#x27;s an example of a web page I generated <a href=\"https://chat.qwen.ai/s/deploy/65743dcf-7e0e-455b-b430-5004c8f36841\">on commonalities in authoritarian regimes throughout history</a>, <a href=\"https://chat.qwen.ai/s/deploy/caf2033e-725b-43dc-b4d0-721063728774\">another one on UFO or UAP sightings</a>, and below this paragraph, a podcast on UFO or UAP sightings. </p><p>While the website is hosted via a public link, the podcast must be downloaded by the user and can&#x27;t be linked to publicly, from what I could tell in my brief usage so far.</p><p>Note the podcast is much different than the actual report — not just a straight read-through audio version of it, rather, a new format of two hosts discussing and bantering about the subject using the report as the jumping off point. </p><p>The web page versions of the report also include new graphics not found in the PDF report.</p><h3><b>Comparisons to Google&#x27;s NotebookLM</b></h3><p>While the new capabilities have been well received by many early users, comparisons to other research assistants have surfaced — particularly Google’s <b>NotebookLM</b>, which recently exited beta.</p><p>AI commentator and newsletter writer <a href=\"https://x.com/kimmonismus/status/1980612332767072444\">Chubby (@kimmonismus) noted on X</a>:</p><blockquote><p>“I am really grateful that Qwen provides regular updates. That’s great.</p></blockquote><blockquote><p>But the attempt to build a NotebookLM clone inside Qwen-3-max doesn’t sound very promising compared to Google’s version.”</p></blockquote><p>While NotebookLM is built around organizing and querying existing documents and web pages, Qwen Deep Research focuses more on <b>generating new research content from scratch</b>, aggregating sources from the open web, and presenting it across multiple modalities. </p><p>The comparison suggests that while the two tools overlap in general concept — AI-assisted research — they diverge in approach and target user experience.</p><h3><b>Availability</b></h3><p>Qwen Deep Research is now live and available through the <b>Qwen Chat app</b>. The feature can be accessed with <a href=\"https://chat.qwen.ai/?inputFeature=deep_research\">the following URL.</a></p><p>No pricing details have been provided for Qwen3-Max or the specific Deep Research capabilities as of this writing.</p><h3><b>What&#x27;s Next For Qwen Deep Research?</b></h3><p>By combining research guidance, data analysis, and multi-format content creation into a single tool, Qwen Deep Research aims to streamline the path from idea to publishable output. </p><p>The integration of code, visuals, and voice makes it especially attractive to content creators, educators, and independent analysts who want to scale their research into web- or podcast-friendly forms without switching platforms.</p><p>Still, comparisons to more specialized offerings like NotebookLM raise questions about how Qwen’s generalized approach stacks up on depth, precision, and refinement. Whether the strength of its multi-format execution outweighs those concerns may come down to user priorities — and whether they value single-click publishing over tight integration with existing notes and materials.</p><p>For now, Qwen is signaling that research doesn’t end with a document — it begins with one.</p><p>Let me know if you want this repackaged into something shorter or tailored to a particular audience — newsletter, press-style blog, internal team explainer, etc.</p>",
    "published": "Tue, 21 Oct 2025 18:32:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-22T09:02:03.537569",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "DeepSeek drops open-source model that compresses text 10x through images, defying conventions",
    "link": "https://venturebeat.com/ai/deepseek-drops-open-source-model-that-compresses-text-10x-through-images",
    "summary": "<p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a>, the Chinese artificial intelligence research company that has repeatedly challenged assumptions about <a href=\"https://www.reuters.com/technology/artificial-intelligence/big-tech-faces-heat-chinas-deepseek-sows-doubts-billion-dollar-spending-2025-01-27/\"><u>AI development costs</u></a>, has released a <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>new model</u></a> that fundamentally reimagines how large language models process information—and the implications extend far beyond its modest branding as an optical character recognition tool.</p><p>The company&#x27;s <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR model</u></a>, released Monday with full <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>open-source code</u></a> and <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>weights</u></a>, achieves what researchers describe as a paradigm inversion: compressing text through visual representation up to 10 times more efficiently than traditional text tokens. The finding challenges a core assumption in AI development and could pave the way for language models with dramatically expanded context windows, potentially reaching tens of millions of tokens.</p><p>&quot;We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping,&quot; the research team wrote in their <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>technical paper</u></a>. &quot;Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio &lt; 10×), the model can achieve decoding (OCR) precision of 97%.&quot;</p><p>The implications have resonated across the AI research community. <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Andrej Karpathy</u></a>, co-founder of OpenAI and former director of AI at Tesla, said in a post that the work raises fundamental questions about how AI systems should process information. &quot;Maybe it makes more sense that all inputs to LLMs should only ever be images,&quot; Karpathy wrote. &quot;Even if you happen to have pure text input, maybe you&#x27;d prefer to render it and then feed that in.&quot;</p><h2><b>How DeepSeek achieved 10x compression by treating text as images</b></h2><p>While <a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> marketed the release as an <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>OCR model</u></a> — a technology for converting images of text into digital characters — the research paper reveals more ambitious goals. The model demonstrates that visual representations can serve as a superior compression medium for textual information, inverting the conventional hierarchy where text tokens were considered more efficient than vision tokens.</p><p>&quot;Traditionally, vision LLM tokens almost seemed like an afterthought or &#x27;bolt on&#x27; to the LLM paradigm,&quot; wrote <a href=\"https://x.com/doodlestein/status/1980282222893535376\"><u>Jeffrey Emanuel</u></a>, an AI researcher, in a detailed analysis of the paper. &quot;And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens...But that gets inverted now from the ideas in this paper.&quot;</p><p>The model&#x27;s architecture consists of two primary components: <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepEncoder</u></a>, a novel 380-million-parameter vision encoder, and a 3-billion-parameter mixture-of-experts language decoder with 570 million activated parameters. DeepEncoder combines Meta&#x27;s <a href=\"https://segment-anything.com/\"><u>Segment Anything Model (SAM)</u></a> for local visual perception with <a href=\"https://openai.com/index/clip/\"><u>OpenAI&#x27;s CLIP model</u></a> for global visual understanding, connected through a 16x compression module.</p><p>To validate their compression claims, DeepSeek researchers tested the model on the <a href=\"https://github.com/ucaslcl/Fox\"><u>Fox benchmark</u></a>, a dataset of diverse document layouts. The results were striking: using just 100 vision tokens, the model achieved 97.3% accuracy on documents containing 700-800 text tokens — representing an effective compression ratio of 7.5x. Even at compression ratios approaching 20x, accuracy remained around 60%.</p><h2><b>The practical impact: Processing 200,000 pages per day on a single GPU</b></h2><p>The efficiency gains translate directly to production capabilities. According to the company, a single <a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><u>Nvidia A100-40G GPU</u></a> can process more than 200,000 pages per day using <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR</u></a>. Scaling to a cluster of 20 servers with eight GPUs each, throughput reaches 33 million pages daily — sufficient to rapidly construct training datasets for other AI models.</p><p>On <a href=\"https://github.com/opendatalab/OmniDocBench\"><u>OmniDocBench</u></a>, a comprehensive document parsing benchmark, <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>DeepSeek-OCR</u></a> outperformed GOT-OCR2.0 (which uses 256 tokens per page) while using only 100 vision tokens. More dramatically, it surpassed MinerU2.0 — which requires more than 6,000 tokens per page on average — while using fewer than 800 vision tokens.</p><p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> designed the model to support five distinct resolution modes, each optimized for different compression ratios and use cases. The &quot;Tiny&quot; mode operates at 512×512 resolution with just 64 vision tokens, while &quot;Gundam&quot; mode combines multiple resolutions dynamically for complex documents. &quot;Gundam mode consists of n×640×640 tiles (local views) and a 1024×1024 global view,&quot; the researchers wrote.</p><h2><b>Why this breakthrough could unlock 10 million token context windows</b></h2><p>The compression breakthrough has immediate implications for one of the most pressing challenges in AI development: expanding the context windows that determine how much information language models can actively consider. Current state-of-the-art models typically handle context windows measured in hundreds of thousands of tokens. DeepSeek&#x27;s approach suggests a path to windows ten times larger.</p><p>&quot;The potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting,&quot; <a href=\"https://x.com/doodlestein/status/1980282222893535376\"><u>Emanuel wrote</u></a>. &quot;You could basically cram all of a company&#x27;s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective.&quot;</p><p>The researchers explicitly frame their work in terms of context compression for language models. &quot;Through DeepSeek-OCR, we demonstrate that vision-text compression can achieve significant token reduction (7-20×) for different historical context stages, offering a promising direction for addressing long-context challenges in large language models,&quot; <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>they wrote</u></a>.</p><p><a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>The paper</u></a> includes a speculative but intriguing diagram illustrating how the approach could implement memory decay mechanisms similar to human cognition. Older conversation rounds could be progressively downsampled to lower resolutions, consuming fewer tokens while maintaining key information — a form of computational forgetting that mirrors biological memory.</p><h2><b>How visual processing could eliminate the &#x27;ugly&#x27; tokenizer problem</b></h2><p>Beyond compression, Karpathy highlighted how the approach challenges fundamental assumptions about how language models should process text. Traditional tokenizers—the systems that break text into units for processing—have long been criticized for their complexity and limitations.</p><p>&quot;I already ranted about how much I dislike the tokenizer,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Karpathy wrote</u></a>. &quot;Tokenizers are ugly, separate, not end-to-end stage. It &#x27;imports&#x27; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network.&quot;</p><p>Visual processing of text could eliminate these issues while enabling new capabilities. The approach naturally handles formatting information lost in pure text representations: bold text, colors, layout, embedded images. &quot;Input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful,&quot; Karpathy noted.</p><p>The implications resonate with human cognitive science. <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel drew a parallel to Hans Bethe</u></a>, the renowned physicist who memorized vast amounts of reference data: &quot;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&quot;</p><h2><b>The model&#x27;s training: 30 million PDF pages across 100 languages</b></h2><p>The model&#x27;s capabilities rest on an extensive training regimen using diverse data sources. DeepSeek collected 30 million PDF pages covering approximately 100 languages, with Chinese and English accounting for 25 million pages. The training data spans nine document types — academic papers, financial reports, textbooks, newspapers, handwritten notes, and others.</p><p>Beyond document OCR, the training incorporated what the researchers call &quot;OCR 2.0&quot; data: 10 million synthetic charts, 5 million chemical formulas, and 1 million geometric figures. The model also received 20% general vision data for tasks like image captioning and object detection, plus 10% text-only data to maintain language capabilities.</p><p>The training process employed pipeline parallelism across 160 <a href=\"https://www.nvidia.com/en-us/data-center/a100/\"><u>Nvidia A100-40G GPUs</u></a> (20 nodes with 8 GPUs each), with the vision encoder divided between two pipeline stages and the language model split across two others. &quot;For multimodal data, the training speed is 70B tokens/day,&quot; the researchers reported.</p><h2><b>Open source release accelerates research and raises competitive questions</b></h2><p>True to DeepSeek&#x27;s pattern of open development, the company released the complete model weights, training code, and inference scripts on <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><u>GitHub</u></a> and <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><u>Hugging Face</u></a>. The GitHub repository gained over 4,000 stars within 24 hours of release, according to Dataconomy.</p><p>The breakthrough raises questions about whether other AI labs have developed similar techniques but kept them proprietary. Emanuel speculated that Google&#x27;s Gemini models, which feature large context windows and strong OCR performance, might employ comparable approaches. &quot;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel wrote</u></a>.</p><p>Google&#x27;s <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\"><u>Gemini 2.5 Pro</u></a> offers a 1-million-token context window, with plans to expand to 2 million, though the company has not publicly detailed the technical approaches enabling this capability. OpenAI&#x27;s <a href=\"https://openai.com/index/introducing-gpt-5/\"><u>GPT-5</u></a> supports 400,000 tokens, while Anthropic&#x27;s <a href=\"https://www.anthropic.com/news/claude-sonnet-4-5\"><u>Claude 4.5</u></a> offers 200,000 tokens, with a 1-million-token window available in beta for eligible organizations.</p><h2><b>The unanswered question: Can AI reason over compressed visual tokens?</b></h2><p>While the compression results are impressive, researchers acknowledge important open questions. &quot;It&#x27;s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM,&quot; <a href=\"https://x.com/karpathy/status/1980397031542989305\"><u>Emanuel noted</u></a>. &quot;Can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality?&quot;</p><p>The <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepSeek paper</u></a> focuses primarily on the compression-decompression capability, measured through OCR accuracy, rather than downstream reasoning performance. This leaves open whether language models could reason effectively over large contexts represented primarily as compressed visual tokens.</p><p>The researchers acknowledge their work represents &quot;an initial exploration into the boundaries of vision-text compression.&quot; They note that &quot;OCR alone is insufficient to fully validate true context optical compression&quot; and plan future work including &quot;digital-optical text interleaved pretraining, needle-in-a-haystack testing, and other evaluations.&quot;</p><p><a href=\"https://www.deepseek.com/\"><u>DeepSeek</u></a> has established a pattern of achieving competitive results with dramatically lower computational resources than Western AI labs. The company&#x27;s earlier <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3\"><u>DeepSeek-V3 model</u></a> reportedly cost <a href=\"https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/\"><u>just $5.6 million to train</u></a>—though this figure represents only the final training run and excludes R&amp;D and infrastructure costs—compared to hundreds of millions for comparable models from OpenAI and Anthropic.</p><p>Industry analysts have questioned the $5.6 million figure, with some estimates placing the company&#x27;s total infrastructure and operational costs <a href=\"https://www.cnbc.com/2025/01/30/chinas-deepseek-has-some-big-ai-claims-not-all-experts-are-convinced-.html\"><u>closer to $1.3 billion</u></a>, though still lower than American competitors&#x27; spending.</p><h2><b>The bigger picture: Should language models process text as images?</b></h2><p><a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u>DeepSeek-OCR</u></a> poses a fundamental question for AI development: should language models process text as text, or as images of text? The research demonstrates that, at least for compression purposes, visual representation offers significant advantages. Whether this translates to effective reasoning over vast contexts remains to be determined.</p><p>&quot;From another perspective, optical contexts compression still offers substantial room for research and improvement, representing a promising new direction,&quot; the researchers concluded<a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><u> in their paper</u></a>.</p><p>For the AI industry, the work adds another dimension to the race for longer context windows — a competition that has intensified as language models are applied to increasingly complex tasks requiring vast amounts of information. The open-source release ensures the technique will be widely explored, tested, and potentially integrated into future AI systems.</p><p>As Karpathy framed the deeper implication: &quot;OCR is just one of many useful vision -&gt; text tasks. And text -&gt; text tasks can be made to be vision -&gt;text tasks. Not vice versa.&quot; In other words, the path forward for AI might not run through better tokenizers — it might bypass text tokens altogether.</p><p>\n</p>",
    "published": "Tue, 21 Oct 2025 18:30:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-22T09:02:03.537994",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "Google's new vibe coding AI Studio experience lets anyone build, deploy apps live in minutes",
    "link": "https://venturebeat.com/ai/googles-new-vibe-coding-ai-studio-experience-lets-anyone-build-deploy-apps",
    "summary": "<p>Google AI Studio has gotten a big vibe coding upgrade with a new interface, buttons, suggestions and community features that allow anyone with an idea for an app — even complete novices, laypeople, or non-developers like yours truly — to bring it into existence and deploy it live, on the web, for anyone to use, within <i>minutes</i>.</p><p>The updated Build tab is available now at <a href=\"http://ai.studio/build\">ai.studio/build</a>, and it’s free to start. </p><p>Users can experiment with building applications without needing to enter payment information upfront, though certain advanced features like Veo 3.1 and Cloud Run deployment require a paid API key.</p><p>The new features appear to me to make Google&#x27;s AI models and offerings even more competitive, perhaps preferred, for many general users to dedicated AI startup rivals like Anthropic&#x27;s Claude Code and OpenAI&#x27;s Codex, respectively, two &quot;vibe coding&quot; focused products that are beloved by developers — but seem to have a higher barrier to entry or may require more technical know-how.</p><h3><b>A Fresh Start: Redesigned Build Mode</b></h3><p>The updated Build tab serves as the entry point to vibe coding. It introduces a new layout and workflow where users can select from Google’s suite of AI models and features to power their applications. The default is Gemini 2.5 Pro, which is great for most cases.</p><p>Once selections are made, users simply describe what they want to build, and the system automatically assembles the necessary components using Gemini’s APIs.</p><p>This mode supports mixing capabilities like Nano Banana (a lightweight AI model), Veo (for video understanding), Imagine (for image generation), Flashlight (for performance-optimized inference), and Google Search.</p><p>Patrick Löber, Developer Relations at Google DeepMind, highlighted that the experience is meant to help users “supercharge your apps with AI” using a simple prompt-to-app pipeline.</p><p>In a video demo he posted on X and LinedIn, he showed how just a few clicks led to the automatic generation of a garden planning assistant app, complete with layouts, visuals, and a conversational interface.</p><div></div><h3><b>From Prompt to Production: Building and Editing in Real Time</b></h3><p>Once an app is generated, users land in a fully interactive editor. On the left, there’s a traditional code-assist interface where developers can chat with the AI model for help or suggestions. On the right, a code editor displays the full source of the app.</p><p>Each component—such as React entry points, API calls, or styling files—can be edited directly. Tooltips help users understand what each file does, which is especially useful for those less familiar with TypeScript or frontend frameworks.</p><p>Apps can be saved to GitHub, downloaded locally, or shared directly. Deployment is possible within the Studio environment or via Cloud Run if advanced scaling or hosting is needed.</p><h3><b>Inspiration on Demand: The ‘I’m Feeling Lucky’ Button</b></h3><p>One standout feature in this update is the “I’m Feeling Lucky” button. Designed for users who need a creative jumpstart, it generates randomized app concepts and configures the app setup accordingly. Each press yields a different idea, complete with suggested AI features and components.</p><p>Examples produced during demos include:</p><ul><li><p>An interactive map-based chatbot powered by Google Search and conversational AI.</p></li><li><p>A dream garden designer using image generation and advanced planning tools.</p></li><li><p>A trivia game app with an AI host whose personality users can define, integrating both Imagine and Flashlight with Gemini 2.5 Pro for conversation and reasoning.</p></li></ul><p>Logan Kilpatrick, Lead of Product for Google AI Studio and Gemini AI, noted in a demo video of his own that this feature encourages discovery and experimentation. </p><p>“You get some really, really cool, different experiences,” he said, emphasizing its role in helping users find novel ideas quickly.</p><div></div><h3><b>Hands-On Test: From Prompt to App in 65 Seconds</b></h3><p>To test the new workflow, I prompted Gemini with:</p><p><i>A randomized dice rolling web application where the user can select between common dice sizes (6 sides, 10 sides, etc) and then see an animated die rolling and choose the color of their die as well.</i></p><p><b>Within 65 seconds (just over a minute) AI Studio returned a fully working web app</b> featuring:</p><ul><li><p>Dice size selector (d4, d6, d8, d10, d12, d20)</p></li><li><p>Color customization options for the die</p></li><li><p>Animated rolling effect with randomized results</p></li><li><p>Clean, modern UI built with React, TypeScript, and Tailwind CSS</p></li></ul><p>The platform also generated a complete set of structured files, including App.tsx, constants.ts, and separate components for dice logic and controls. </p><p>After generation, it was easy to iterate: adding sound effects for each interaction (rolling, choosing a die, changing color) required only a single follow-up prompt to the built-in assistant. This was also suggested by Gemini, too, by the way. </p><p>From there, the app can be previewed live or exported using built-in controls to:</p><ul><li><p>Save to GitHub</p></li><li><p>Download the full codebase</p></li><li><p>Copy the project for remixing</p></li><li><p>Deploy via integrated tools</p></li></ul><p>My brief, hands-on test showed just how quickly even small utility apps can go from idea to interactive prototype—without leaving the browser or writing boilerplate code manually.</p><h3><b>AI-Suggested Enhancements and Feature Refinement</b></h3><p>In addition to code generation, Google AI Studio now offers context-aware feature suggestions. These recommendations, generated by Gemini’s Flashlight capability, analyze the current app and propose relevant improvements.</p><p>In one example, the system suggested implementing a feature that displays the history of previously generated images in an image studio tab. These iterative enhancements allow builders to expand app functionality over time without starting from scratch.</p><p>Kilpatrick emphasized that users can continue to refine their projects as they go, combining both automatic generation and manual adjustments. “You can go in and continue to edit and sort of refine the experience that you want iteratively,” he said.</p><h3><b>Free to Start, Flexible to Grow</b></h3><p>The new experience is available at no cost for users who want to experiment, prototype, or build lightweight apps. There’s no requirement to enter credit card information to begin using vibe coding.</p><p>However, more powerful capabilities — such as using models like Veo 3.1 or deploying through Cloud Run — do require switching to a paid API key.</p><p>This pricing structure is intended to lower the barrier to entry for experimentation while providing a clear path to scale when needed.</p><h3><b>Built for All Skill Levels</b></h3><p>One of the central goals of the vibe coding launch is to make AI app development accessible to more people. The system supports both high-level visual builders and low-level code editing, creating a workflow that works for developers across experience levels.</p><p>Kilpatrick mentioned that while he’s more familiar with Python than TypeScript, he still found the editor useful because of the helpful file descriptions and intuitive layout. </p><p>This focus on usability could make AI Studio a compelling option for developers exploring AI for the first time.</p><h3><b>More to Come: A Week of Launches</b></h3><p>The launch of vibe coding is the first in a series of announcements expected throughout the week. While specific future features haven’t been revealed yet, both Kilpatrick and Löber hinted that additional updates are on the way.</p><p>With this update, Google AI Studio positions itself as a flexible, user-friendly environment for building AI-powered applications—whether for fun, prototyping, or production deployment. The focus is clear: make the power of Gemini’s APIs accessible without unnecessary complexity.</p>",
    "published": "Tue, 21 Oct 2025 17:45:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:03.538294",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "New 'Markovian Thinking' technique unlocks a path to million-token AI reasoning",
    "link": "https://venturebeat.com/ai/new-markovian-thinking-technique-unlocks-a-path-to-million-token-ai",
    "summary": "<p>Researchers at Mila have proposed a new technique that makes large language models (LLMs) vastly more efficient when performing complex reasoning. Called <a href=\"https://arxiv.org/abs/2510.06557\"><u>Markovian Thinking</u></a>, the approach allows LLMs to engage in lengthy reasoning without incurring the prohibitive computational costs that currently limit such tasks.</p><p>The team’s implementation, an environment named Delethink, structures the reasoning chain into fixed-size chunks, breaking the scaling problem that plagues very long LLM responses. Initial estimates show that for a 1.5B parameter model, this method can cut the costs of training by more than two-thirds compared to standard approaches.</p><h2>The quadratic curse of long-chain reasoning</h2><p>For an LLM to solve a complex problem, it often needs to generate a long series of intermediate “thinking” tokens, often referred to as chain-of-thought (CoT). In recent years, researchers have found that using <a href=\"https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost\"><u>reinforcement learning</u></a> (RL) to train models to produce longer CoTs (sometimes referred to as LongCoT) has significantly improved their reasoning capabilities.</p><p>However, the standard method for this has a critical flaw: The AI&#x27;s &quot;state&quot; (the prompt plus all the reasoning tokens it has generated thus far in its processing) grows with every new reasoning token. For modern <a href=\"https://bdtechtalks.com/2022/05/02/what-is-the-transformer/\"><u>transformer-based models</u></a>, this means the computational cost explodes quadratically as the reasoning chain gets longer, making it prohibitively expensive to train models for very complex tasks.</p><p>Most current attempts to manage this cost focus on limiting how much thinking the model does, implicitly preferring shorter solutions or terminating the process early. While these methods offer some relief, the Mila researchers still operate within the LongCoT framework and are thus fundamentally bound by its quadratic nature.</p><p>Instead of trying to control the computational growth, Mila created an RL environment that avoids the quadratic problem altogether. As co-author Amirhossein Kazemnejad explained, the goal is to enable capabilities like multi-week reasoning and scientific discovery. &quot;That regime (and the RL needed to enable such capabilities) is not supported by the current LongCoT paradigm, because of quadratic compute cost,&quot; he said.</p><h2>Thinking in chunks with Delethink</h2><p>The researchers&#x27; solution is a paradigm they call the &quot;Markovian Thinker,&quot; where the model reasons while keeping the size of its reasoning context window constant. The core idea is to change the RL setup to separate &quot;how long the model thinks&quot; from &quot;how much context it must process.&quot; If done correctly, a Markovian Thinker turns the quadratic growth problem into linear compute and fixed memory requirements for LLM reasoning.</p><p>The researchers put this paradigm into practice through Delethink, which forces the model to reason in a sequence of fixed-size chunks, such as 8,000 tokens at a time. Within each chunk, the model reasons as it normally would, using the classic attention mechanism. But when it reaches the limit of the chunk, the environment resets the context, creating a new prompt that includes the original query plus a short &quot;carryover&quot; from the previous chunk. For example, the carryover could be the last few tokens of the previous chunk of CoT or a summary of the most important results.</p><p>This rearrangement of the problem forces the model to learn how to embed a summary of its progress, or a &quot;textual Markovian state,&quot; into this carryover to continue its reasoning in the next chunk. This addresses the common concern of whether the model can remember important details from earlier steps. </p><p>According to Kazemnejad, the model learns what to remember. &quot;With training... the model is forced to learn to carry forward the task-critical state,&quot; he explained. He added crucial clarification for practical use: The original input prompt is not modified, including the documents or contextual data added to it. “Our approach is aimed at the reasoning phase and does not modify the prompt,&quot; he said.</p><h2>Delethink in action</h2><p>To test their approach, the researchers trained R1-Distill-1.5B with Delethink on a dataset of competition-level math problems, then evaluated it against several benchmarks. The model was trained to reason for up to 24,000 tokens but with fixed 8,000-token chunks. </p><p>The researchers <!-- -->compared this to models trained with the standard LongCoT-RL method. Their findings indicate that the model trained with Delethink could reason up to 24,000 tokens, and matched or surpassed a LongCoT model trained with the same 24,000-token budget on math benchmarks. On other tasks like coding and PhD-level questions, Delethink also matched or slightly beat its LongCoT counterpart. “Overall, these results indicate that Delethink uses its thinking tokens as effectively as LongCoT-RL with reduced compute,” the researchers write.</p><p>The benefits become even more pronounced when scaling beyond the training budget. While models trained with LongCoT quickly plateaued at their training limits, the Delethink-trained model continued to improve its performance. For instance, some math problems were only solved after the model reasoned for up to 140,000 tokens, far beyond its 24,000-token training budget. This linear compute advantage is substantial for enterprise applications. The researchers estimate that training a model to an average thinking length of 96,000 tokens would require 27 H100-GPU-months with LongCoT, versus just 7 with Delethink.</p><p>This efficiency extends directly to inference, the primary operational cost for most enterprises. &quot;Models trained in Markovian Thinking use the same inference style (delethink-tracing) during test time, which provides the same advantages of linear compute and constant memory after training,&quot; said Kazemnejad. He offered a practical example: An AI agent could &quot;debug a large codebase and think for a long time... which of course reduces the cost significantly compared to the conventional LongCoT approach.&quot;</p><p>Interestingly, the researchers found that off-the-shelf reasoning models, even without any specific training, already exhibit some ability to think in a Markovian way. This finding has immediate practical implications for developers. &quot;In practice, this means that — without Delethink-RL— these models can already run a delethink-tracing wrapper and perform competitively with LongCoT on our benchmarked tasks,&quot; Kazemnejad said.</p><p>Their experiments with larger models such as <a href=\"https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b\"><u>GPT-OSS 120B</u></a> showed robust performance with Delethink across a range of complex tasks. This latent ability provides a strong starting point for RL training, helping explain why the method is so effective. “Together, these results suggest that Delethink is compatible and scales with state-of-the-art models,” the researchers conclude.</p><p>The success of Markovian Thinking shows it may be possible for &quot;next-generation reasoning models to think for millions of tokens,&quot; the researchers note. This opens the door to fundamentally new AI capabilities, moving beyond current constraints. </p><p>&quot;Markovian Thinking... opens the path for models that can &#x27;think&#x27; for very long horizons, which we view as a necessary step toward eventual scientific discovery,&quot; Kazemnejad said. &quot;Our approach removes a key bottleneck and can allow training for much longer horizon tasks, which enables next-gen capabilities.&quot;</p>",
    "published": "Tue, 21 Oct 2025 05:12:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:03.538552",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "The unexpected benefits of AI PCs: why creativity could be the new productivity",
    "link": "https://venturebeat.com/ai/the-unexpected-benefits-of-ai-pcs-why-creativity-could-be-the-new",
    "summary": "<p><i>Presented by HP</i></p><hr /><p>Creativity is quickly becoming the new measure of productivity. While AI is often framed as a tool for efficiency and automation, new research from <a href=\"https://mitsloan.mit.edu/\">MIT Sloan School of Management</a> shows that generative AI enhances human creativity — when employees have the right tools and skills to use it effectively. </p><p>That’s where <a href=\"https://www.hp.com/us-en/ai-solutions/next-gen-ai-pcs.html?jumpid=af_us_oc_mk_ot_cm019000_aw_ot_venturebeat\">AI PCs</a> come in. These next-generation laptops combine local AI processing with powerful Neural Processing Units (NPUs), delivering the speed and security that knowledge workers expect while also unlocking new creative possibilities. By handling AI tasks directly on the device, AI PCs minimize latency, protect sensitive data, and lower energy consumption.</p><p>Teams are already proving the impact. Marketing teams are using AI PCs to generate campaign assets in hours instead of weeks. Engineers are shortening design and prototyping cycles. Sales reps are creating personalized proposals onsite, even without cloud access. In each case, AI PCs are not just accelerating workflows — they’re sparking fresh ideas, faster iteration, and more engaged teams.</p><p>The payoff is clear: creativity that translates into measurable business outcomes, from faster time-to-market and stronger compliance to deeper customer engagement. Still, adoption is uneven, and the benefits aren’t yet reaching the wider workforce.</p><h3>Early creative benefits, but a divide remains</h3><p>New Morning Consult and HP research shows nearly half of IT decision makers (45%) already use AI PCs for creative assistance, with almost a third (29%) using them for tasks like image generation and editing. That’s not just about efficiency — it’s about bringing imagination into everyday workflows.</p><p>According to <a href=\"https://www.hp.com/us-en/solutions/future-of-work.html?jumpid=ma_globalnav_sol_mkt_fow\">HP’s 2025 Work Relationship Index</a>, fulfillment is the single biggest driver of a healthy work relationship, outranking even leadership. Give employees tools that let them create, not just execute tasks, and you unlock productivity, satisfaction, retention, and optimism. The same instinct that drives workers to build outside the office is the one companies can harness inside it.</p><p>The challenge is that among broader knowledge workers, adoption is still low, just 29% for creative assistance and just 19% for image generation. This creative divide means the full potential of AI PCs hasn’t reached the wider workforce. For CIOs, the opportunity isn’t just deploying faster machines — it’s fostering a workplace culture where creativity drives measurable business value. </p><h3>Creative benefits of AI PCs</h3><p>So when you put AI PCs in front of the employees who embrace the possibilities, what does that look like in practice? Early adopters are already seeing AI PCs reshape how creative work gets done. </p><p>Teams dream up fresh ideas, faster. AI PCs can spark new perspectives and out-of-the-box solutions, enhancing human creativity rather than replacing it. With dedicated NPUs handling AI workloads, employees stay in flow without interruptions. Battery life is extended, latency drops, and performance improves — allowing teams to focus on ideas, not wait times.</p><p>On-device AI is opening new creative mediums, from visual design to video production to music editing, and videos, photos, and presentations that can be generated, edited, and refined in real time. </p><p>Plus, AI workloads like summarization, transcription, and code generation run instantly without relying on cloud APIs. That means employees can work productively in low-bandwidth or disconnected environments, removing downtime risks, especially for mobile workforces and global deployments.</p><p>And across the organization, AI PCs mean real-world, measurable business outcomes. </p><p><b>Marketing: </b>AI PCs enable creative teams to generate ad variations, social content, and campaign assets in minutes instead of days, reducing dependence on external agencies. And that leads to faster campaign launches, reduced external vendor spend, and increased pipeline velocity.</p><p><b>Product and engineering: </b>Designers/engineers can prototype in CAD, generate 3D mockups, or run simulations locally with on-device AI accelerators, shortening feedback loops. That means reduced iteration cycles, faster prototyping, and faster time-to-market.</p><p><b>Sales/customer engagement: </b>Reps can use AI PCs to generate real-time proposals, personalized presentations, or analyze contracts offline at client sites, even without cloud connection. This generates faster deal cycles, higher client engagement, and a shorter sales turnaround.</p><h3>From efficiency to fulfillment</h3><p>AI PCs are more than just a performance upgrade. They’re reshaping how people approach and experience work. By giving employees tools that spark creativity as well as productivity, organizations can unlock faster innovation, deeper engagement, and stronger retention. </p><p>For CIOs, the opportunity goes beyond efficiency gains. The true value of AI PCs won’t be measured in speed or specs, but in how they open new possibilities for creation, collaboration, and competition — helping teams not just work faster, but work more creatively and productively.</p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>",
    "published": "Tue, 21 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:03.538857",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "OpenAI announces ChatGPT Atlas, an AI-enabled web browser to challenge Google Chrome",
    "link": "https://venturebeat.com/ai/openai-releases-chatgpt-atlas-an-ai-enabled-web-browser-to-challenge-google",
    "summary": "<p><a href=\"https://openai.com/\"><u>OpenAI</u></a> is entering the browser world with the launch of ChatGPT Atlas, an AI-enabled browser. </p><p>Atlas, now available globally, can be accessed through Apple’s macOS, with support for Windows, iOS and Android coming soon. The announcement comes several months after rumors in July that OpenAI would release a web browser that would challenge the dominance of <a href=\"https://www.google.com/\"><u>Google</u></a>’s Chrome. </p><p>In a <a href=\"https://www.youtube.com/watch?v=8UWKxJbjriY\"><u>livestream</u></a>, CEO Sam Altman said he hopes Atlas will help bring about a new way of interacting with and using the web, one where people chat with the browser rather than typing a URL. </p><p>“We think AI represents a rare once-in-a-decade opportunity to rethink what a browser can be about and how to use one, and how to most productively and pleasantly use the web,” Altman said. “Tabs were great, but we haven’t seen a lot of innovation since then, so we got very excited to really rethink what this could be.” </p><div></div><p>Atlas is meant to offer users a more seamless way to browse the web and ask chat agents questions. It invites users to either search for information via a prompt or question, or just type a URL. </p><p>Part of Atlas’s value proposition is the ability to call on <a href=\"https://venturebeat.com/ai/openai-unveils-chatgpt-agent-that-gives-chatgpt-its-own-computer-to-autonomously-use-your-email-and-web-apps-download-and-create-files-for-you\"><u>agents to do tasks</u></a> directly in the browser. However, agents will only be available to ChatGPT Business, Plus and Pro users for now. </p><p>Users can download Atlas from its dedicated <a href=\"https://chatgpt.com/atlas/get-started/\"><u>site</u></a>, but must log in to their ChatGPT account to begin using it.   </p><h2>Chatting with a browser about your memories</h2><p>Atlas differentiates itself from browsers like Chrome or <a href=\"https://www.apple.com/\"><u>Apple</u></a>’s Safari with its chat feature. The home page essentially is ChatGPT, with a prompt box and several suggested questions. During the livestream, OpenAI said that the more people use Atlas, the more personalized the suggestions will be. </p><p>The chat box “follows” the user, meaning people can chat with ChatGPT on any website. The model will read what’s on the browser and answer any questions users might have. </p><p>When you first open Atlas, it prompts you to import data from other browsers you may be using. When I set up mine, it only asked me for Chrome or Safari, the two browsers I mainly use. Importing browser data creates a memory base for Atlas that ChatGPT will reference. So far, Atlas’s memory is hit or miss. I connected my Chrome history, and when I asked about a recent travel destination search I did (and have been searching for every day for a month), Atlas claimed I had never searched for that information.</p><p>The in-browser chat also reduces the copy-pasting that users often resort to when, say, writing an email. People can open their Gmail, then ask ChatGPT in the browser to help tidy up the message. Of course, Gmail or any other Google Workspace product already offers <a href=\"https://venturebeat.com/ai/google-adds-more-ai-tools-to-its-workspace-productivity-apps\"><u>Gemini-powered capabilities</u></a>, such as email rewriting. </p><p>OpenAI CEO of Applications, Fidji Simo, said in a <a href=\"https://fidjisimo.substack.com/p/launching-our-new-browser-chatgpt\"><u>blog post</u></a> that users can toggle browser memory on or off and control what it can see.</p><h2>Agent mode on the browser</h2><p>In the past few months, OpenAI has shored up its agent infrastructure in the expectation that individuals and enterprises will rely more and more on agents. </p><p>Agents on Atlas can use the browser if needed to accomplish a task. For example, you could be looking at a recipe and ask chat to build a grocery list. The agent can then begin shopping on your preferred grocery site. OpenAI has already added a <a href=\"https://venturebeat.com/ai/openai-debuts-new-chatgpt-buy-button-and-open-source-agentic-commerce\"><u>buy button to ChatGPT</u></a> and proposed an agentic commerce protocol, which could be helpful for Atlas. However, during the demo, OpenAI staff opted not to let the agent proceed to purchase products. </p><p>Having the agent directly in the browser moves a step beyond point A, where the browser uses an agent in Chrome. Ideally, it already knows what you were looking at and has the information it needs to access and execute on the browser.</p><h2>A new browser war</h2><p>With more people using AI models and chat platforms for <a href=\"https://venturebeat.com/ai/no-more-links-no-more-scrolling-the-browser-is-becoming-an-ai-agent\"><u>web searches</u></a>, launching an AI-enabled browser has become another battleground for model providers. Of course, as Chrome has become more popular, it has slowly added AI capabilities thanks to Google&#x27;s Gemini models. Google has also been experimenting with other AI-powered search capabilities, such as <a href=\"https://venturebeat.com/ai/google-adds-ai-image-generation-to-search-but-theres-a-catch\"><u>generative image search</u></a>. But, companies like <a href=\"https://www.perplexity.ai/\"><u>Perplexity</u></a>, with its <a href=\"https://www.perplexity.ai/comet\"><u>Comet browser</u></a>, is hoping to take on Chrome. <a href=\"https://www.opera.com/\"><u>Opera</u></a>, long a Chrome competitor, also repositioned itself as an AI-powered browser by embedding AI features into its platform. </p><p>For some, Atlas represents a fresh new way to use a web browser. </p><div></div><p>However, many pointed out that Atlas does not exactly reinvent the wheel, as it shares some features with Comet. </p><div></div><div></div><p>What is interesting about Atlas is how familiar it is. It looks just like ChatGPT, but it also has tabs like Chrome. </p><p>OpenAI emphasized that this is the first version of Atlas, implying that this may not be its final form. What is for sure is that Atlas is OpenAI’s first volley in the AI browser wars. </p>",
    "published": "Tue, 21 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-22T09:02:03.539099",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.8
  },
  {
    "title": "AI’s financial blind spot: Why long-term success depends on cost transparency",
    "link": "https://venturebeat.com/ai/ais-financial-blind-spot-why-long-term-success-depends-on-cost-transparency",
    "summary": "<p><i>Presented by Apptio, an IBM company</i></p><hr /><p>When a technology with revolutionary potential comes on the scene, it’s easy for companies to let enthusiasm outpace fiscal discipline. Bean counting can seem short-sighted in the face of exciting opportunities for business transformation and competitive dominance. But money is always an object. And when the tech is AI, those beans can add up fast.</p><p>AI’s value is becoming evident in areas like operational efficiency, worker productivity, and customer satisfaction. However, this comes at a cost. The key to long-term success is understanding the relationship between the two — so you can ensure that the potential of AI translates into real, positive impact for your business. </p><h3><b>The AI acceleration paradox</b></h3><p>While AI is helping to transform business operations, its own financial footprint often remains obscure. If you can’t connect costs to impact, how can you be sure your AI investments will drive meaningful ROI? This uncertainty makes it no surprise that in the 2025 Gartner® <a href=\"https://www.gartner.com/en/articles/hype-cycle-for-artificial-intelligence\">Hype Cycle™ for Artificial Intelligence</a>, GenAI has moved into the “Trough of Disillusionment” . </p><p>Effective strategic planning depends on clarity. In its absence, decision-making falls back on guesswork and gut instinct. And there’s a lot riding on these decisions. According to Apptio research, 68% of technology leaders surveyed expect to increase their AI budgets, and 39% believe AI will be their departments’ biggest driver of future budget growth. </p><p>But bigger budgets don’t guarantee better outcomes. Gartner® also reveals that “despite an average spend of $1.9 million on GenAI initiatives in 2024, fewer than 30% of AI leaders say their CEOs are satisfied with the return on investment.” If there’s no clear link between cost and outcome, organizations risk scaling investments without scaling the value they’re meant to create.</p><p>To move forward with well-founded confidence, business leaders in finance, IT, and tech must collaborate to gain visibility into AI’s financial blind spot.</p><h3><b>The hidden financial risks of AI</b></h3><p>The runaway costs of AI can give IT leaders flashbacks to the early days of public cloud. When it’s easy for DevOps teams and business units to procure their own resources on an OpEx basis, costs and inefficiencies can quickly spiral. In fact, AI projects are avid consumers of cloud infrastructure — while incurring additional costs for data platforms and engineering resources. And that’s on top of the tokens used for each query. The decentralized nature of these costs makes them particularly difficult to attribute to business outcomes. </p><p>As with the cloud, the ease of AI procurement quickly leads to AI sprawl. And finite budgets mean that every dollar spent represents an unconscious tradeoff with other needs. People worry that AI will take their job. But it’s just as likely that AI will take their department’s budget. </p><p>Meanwhile, according to Gartner®, “Over 40% of agentic AI projects will be canceled by end of 2027, due to escalating costs, unclear business value or inadequate rish controls”. But are those the right projects to cancel? Lacking a way to connect investment to impact, how can business leaders know whether those rising costs are justified by proportionally greater ROI? ? </p><p>Without transparency into AI costs, companies risk overspending, under-delivering, and missing out on better opportunities to drive value. </p><h3><b>Why traditional financial planning can&#x27;t handle AI</b></h3><p>As we learned with cloud, we see that traditional static budget models are poorly suited for dynamic workloads and rapidly scaling resources. The key to cloud cost management has been tagging and telemetry, which help companies attribute each dollar of cloud spend to specific business outcomes. AI cost management will require similar practices. But the scope of the challenge goes much further. On top of costs for storage, compute, and data transfer, each AI project brings its own set of requirements — from prompt optimization and model routing to data preparation, regulatory compliance, security, and personnel.</p><p>This complex mix of ever-shifting factors makes it understandable that finance and business teams lack granular visibility into AI-related spend — and IT teams struggle to reconcile usage with business outcomes. But it’s impossible to precisely and accurately track ROI without these connections.</p><h3><b>The strategic value of cost transparency</b></h3><p>Cost transparency empowers smarter decisions — from resource allocation to talent deployment. </p><p>Connecting specific AI resources with the projects that they support helps technology decision-makers ensure that the most high-value projects are given what they need to succeed. Setting the right priorities is especially critical when top talent is in short supply. If your highly compensated engineers and data scientists are spread across too many interesting but unessential pilots, it’ll be hard to staff the next strategic — and perhaps pressing — pivot.</p><p>FinOps best practices apply equally to AI. Cost insights can surface opportunities to optimize infrastructure and address waste whether by right-sizing performance and latency to match workload requirements, or by selecting a smaller, more cost-effective model instead of defaulting to the latest large language model (LLM). As work proceeds, tracking can flag rising costs so leaders can pivot quickly in more-promising directions as needed. A project that makes sense at X cost might not be worthwhile at 2X cost. </p><p>Companies that adopt a structured, transparent, and well-governed approach to AI costs are more likely to spend the right money in the right ways and see optimal ROI from their investment. </p><h3><b>TBM: An enterprise framework for AI cost management</b></h3><p>Transparency and control over AI costs depend on three practices:</p><p><b>IT financial management (ITFM):</b> Managing IT costs and investments in alignment with business priorities</p><p><b>FinOps:</b> Optimizing cloud costs and ROI through financial accountability and operational efficiency </p><p><b>Strategic portfolio management (SPM):</b> Prioritizing and managing projects to better ensure they deliver maximum value for the business</p><p>Collectively, these three disciplines make up Technology Business Management (TBM) — a structured framework that helps technology, business, and finance leaders connect technology investments to business outcomes for better financial transparency and decision-making. </p><p>Most companies are already on the road to TBM, whether they realize it or not. They may have adopted some form of FinOps or cloud cost management. Or they might be developing strong financial expertise for IT. Or they may rely on Enterprise Agile Planning or Strategic Portfolio Management project management to deliver initiatives more successfully. AI can draw on — and impact — all of these areas. By unifying them under one umbrella with a common model and vocabulary, TBM brings essential clarity to AI costs and the business impact they enable.</p><p>AI success depends on value — not just velocity. The cost transparency that TBM provides offers a road map that can help business and IT leaders make the right investments, deliver them cost-effectively, scale them responsibly, and turn AI from a costly mistake into a measurable business asset and strategic driver. </p><p><i>Sources : Gartner® Press Release, Gartner® Predicts Over 40% of Agentic AI Projects Will Be Canceled by End of 2027, June 25, 2025 </i><a href=\"https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027\"><i>https://www.Gartner®.com/en/newsroom/press-releases/2025-06-25-Gartner®-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027</i></a><i> </i></p><p><i>GARTNER® is a registered trademark and service mark of Gartner®, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved.</i></p><hr /><p><i>Ajay Patel is General Manager, Apptio and IT Automation at IBM.</i></p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=\"mailto:sales@venturebeat.com\"><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>",
    "published": "Tue, 21 Oct 2025 04:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-22T09:02:03.539397",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.6400000000000001
  },
  {
    "title": "Claude Code comes to web and mobile, letting devs launch parallel jobs on Anthropic’s managed infra",
    "link": "https://venturebeat.com/ai/claude-code-comes-to-web-and-mobile-letting-devs-launch-parallel-jobs-on",
    "summary": "<p>Vibe coding <a href=\"https://venturebeat.com/ai/vibe-coding-is-dead-agentic-swarm-coding-is-the-new-enterprise-moat\"><u>is evolving</u></a> and with it are the leading AI-powered coding services and tools, including <a href=\"https://www.anthropic.com/\"><u>Anthropic</u></a>’s Claude Code. </p><p>As of today, the service will be available via the web and, in preview, on the Claude iOS app, giving developers access to additional asynchronous capabilities. Previously, it was available through the terminal on developers&#x27; PCs with support for Git, Docker, Kubernetes, npm, pip, AWS CLI, etc., and as an extension for Microsoft&#x27;s open source <a href=\"https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code\">VS Code editor</a> and other JetBrains-powered integrated development environments (IDEs) via <a href=\"https://blog.jetbrains.com/ai/2025/09/introducing-claude-agent-in-jetbrains-ides/\">Claude Agent</a>.   </p><p>“Claude Code on the web lets you kick off coding sessions without opening your terminal,” Anthropic said in a <a href=\"https://www.anthropic.com/news/claude-code-on-the-web\"><u>blog post</u></a>. “Connect your GitHub repositories, describe what you need, and Claude handles the implementation. Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it’s working through tasks.”</p><p>This allows users to run coding projects asynchronously, a trend that many enterprises are looking for. </p><p>The web version of Claude Code, currently in research preview, will be available to Pro and Max users. However, web Claude Code will be subject to the same rate limits as other versions. Anthropic <a href=\"https://venturebeat.com/ai/anthropic-throttles-claude-rate-limits-devs-call-foul\"><u>throttled rate limits</u></a> to Claude and Claude Code after the unexpected popularity of the coding tool in July, which enabled some users to run Claude Code overnight. </p><p>Anthropic is now ensuring Claude Code comes closer to matching the availability of rival OpenAI&#x27;s Codex AI coding platform, powered by a variant of GPT-5, which launches on mobile and the web back in <a href=\"https://x.com/OpenAI/status/1967636903165038708\">mid September 2025.</a></p><h3><b>Parallel usage</b></h3><p>Anthropic said running Claude Code in the cloud means teams can “now run multiple tasks in parallel across different repositories from a single interface and ship faster with automatic PR creation and clear change summaries.”</p><p>One of the big draws of coding agents is giving developers the ability to run multiple coding projects, such as bugfixes, at the same time. <a href=\"https://www.google.com/\"><u>Google</u></a>’s <a href=\"https://venturebeat.com/ai/googles-jules-aims-to-out-code-codex-in-battle-for-the-ai-developer-stack\"><u>two coding agents</u></a>, Jules and Code Assist, both offer asynchronous code generation and checks. <a href=\"https://venturebeat.com/programming-development/openai-launches-research-preview-of-codex-ai-software-engineering-agent-for-developers-with-parallel-tasking\"><u>Codex</u></a> from <a href=\"https://openai.com/\"><u>OpenAI</u></a> also lets people work in parallel. </p><p>Anthropic said bringing Claude Code to the web won’t disrupt workflows, but noted running tasks in the cloud work best for tasks such as answering questions around projects and how repositories are mapped, bugfixes and for routine, well-defined tasks, and backend changes to verify any adjustments. </p><p>While most developers will likely prefer to use Claude Code on a desktop, Anthropic said the mobile version could encourage more users to “explore coding with Claude on the go.”</p><h3><b>Isolated environments </b></h3><p>Anthropic insisted that Claude Code tasks on the cloud will have the same level of security as the earlier version. It runs on an “isolated sandbox environment with network and filesystem restrictions.” </p><p>Interactions go through a secure proxy service, which the company said ensures the model only accesses authorized repositories.</p><p>Enterprise users can customize which domains Claude Code can connect to. </p><p>Claude Code is powered by Claude Sonnet 4.5, which Anthropic claims is the best coding model around. The company recently made Claude Haiku 4.5, a smaller version of Claude that also has strong coding capabilities, <a href=\"https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take\"><u>available to all Claude subscribers</u></a>, including free users. </p>",
    "published": "Mon, 20 Oct 2025 18:15:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:03.539609",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "Adobe Foundry wants to rebuild Firefly for your brand — not just tweak it",
    "link": "https://venturebeat.com/ai/adobe-foundry-wants-to-rebuild-firefly-for-your-brand-not-just-tweak-it",
    "summary": "<p>Hoping to attract more enterprise teams to its ecosystem, <a href=\"https://www.adobe.com/\"><u>Adobe</u></a> launched a new model customization service called Adobe AI Foundry, which would create bespoke versions of its flagship AI model, Firefly.</p><p>Adobe AI Foundry will work with enterprise customers to rearchitect and retrain <a href=\"https://venturebeat.com/ai/adobe-firefly-ai-video-generator-debuts-the-most-ip-safe-ai-tool-yet\"><u>Firefly models</u></a> specific to the client. AI Foundry version models are different from custom Firefly models in that Foundry models understand multiple concepts compared to custom models with only a single concept. These models will <a href=\"https://venturebeat.com/ai/adobe-previews-firefly-video-ai-model-offering-high-quality-generations\"><u>also be multimodal</u></a>, offering a wider use case than custom Firefly models, which can only ingest and respond with images. </p><p>Adobe AI Foundry models, with Firefly at its base, will know a company’s brand tone, image and video style, products and services and all its IP. The models will generate content based on this information for any use case the company wants. </p><p>Hannah Elsakr, vice president, GenAI New Business Ventures at Adobe, told VentureBeat that the idea to set up AI Foundry came because enterprise customers wanted more sophisticated custom versions of Firefly. But with how complex the needs of enterprises are, Adobe will be doing the rearchitecting rather than handing the reins over to customers. </p><p>“We will retrain our own Firefly commercially safe models with the enterprise IP. We keep that IP separate. We never take that back into the base model, and the enterprise itself owns that output,” Elsakr said. </p><p>Adobe will deploy the Foundry version of Firefly through its API solution, Firefly Services. </p><p>Elsakr likened AI Foundry to an advisory service, since Adobe will have teams working directly with enterprise customers to retrain the model. </p><h3><b>Deep tuning</b></h3><p>Elsakr refers to Foundry as a deep tuning method because it goes further than simply fine-tuning a model.</p><p>“The way we think about it, maybe more layman&#x27;s terms, is that we&#x27;re surgically reopening the Firefly-based models,” Elsakr said. “So you get the benefit of all the world&#x27;s knowledge from our image model or a video model. We&#x27;re going back in time and are bringing in the IP from the enterprise, like a brand. It could be footage from a shot style, whatever they have a license to contribute. We then retrain. We call this continuous pre-training, where we overweigh the model to dial some things differently. So we&#x27;re literally retraining our base model, and that&#x27;s why we call it deep tuning instead of fine-tuning.”</p><p>Part of the training pipeline involves Adobe’s embedded teams working with the company to identify the data they would need. Then the data is securely transferred and ingested before being tagged. It is fed to the base model, and then Adobe begins a pre-training model run. </p><p>Elsakr maintains the Foundry versions of Firefly will not be small or distilled models. Often, the additional data from companies expands the parameters of Firefly.</p><p>Two early customers of Adobe AI Foundry are Home Depot and Walt Disney Imagineering, the research and development arm of Disney for its theme parks. </p><p>“We are always exploring innovative ways to enhance our customer experience and streamline our creative workflows. Adobe’s AI Foundry represents an exciting step forward in embracing cutting-edge technologies to deepen customer engagement and deliver impactful content across our digital channels,” said Molly Battin, senior vice president and chief marketing officer at The Home Depot.</p><h3><b>More customization</b></h3><p>Enterprises often turn to <a href=\"https://venturebeat.com/ai/fine-tuning-vs-in-context-learning-new-research-guides-better-llm-customization-for-real-world-tasks\"><u>fine-tuning and model customization</u></a> to bring large language models with their vast external knowledge closer to their company’s needs. Fine-tuning also enables enterprise users to utilize models only in the context of their organization’s data, so the model doesn’t respond with text wholly unrelated to the business.</p><p>Most organizations, however, do the fine-tuning themselves. They connect to the model’s API and begin retraining it to answer based on their ground truth or their preferences. Several methods for fine-tuning exist, including some that can be done <a href=\"https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models\"><u>with just a prompt</u></a>. Other model providers also try to make it easier for their customers to fine-tune models, such as <a href=\"https://openai.com/\"><u>OpenAI</u></a> with its <a href=\"https://venturebeat.com/ai/you-can-now-fine-tune-your-enterprises-own-version-of-openais-o4-mini-reasoning-model-with-reinforcement-learning\"><u>o4-mini reasoning model</u></a>. </p><p>Elsakr said she expects some companies will have three versions of Firefly: the Foundry version for most projects, a custom Firefly for specific single-concept use cases, and the base Firefly because some teams want a model less encumbered by corporate knowledge. </p>",
    "published": "Mon, 20 Oct 2025 13:00:00 GMT",
    "source_name": "venture_beat",
    "source_url": "https://venturebeat.com/feed/",
    "category": "tech_news",
    "weight": 0.8,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:03.539912",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.48
  },
  {
    "title": "How accounting firms are using AI agents to reclaim time and trust",
    "link": "https://www.artificialintelligence-news.com/news/finance-ai-reclaiming-time-trust-with-openai-chatgpt/",
    "summary": "<p>For CFOs and CIOs under pressure to modernise finance operations, automation – as seen in several generations of RPA (robotic process automation) – isn’t enough. It&#8217;s apparent that transparency and explainability matter just as much. Accounting firms and finance functions inside organisations are now turning to AI systems that reason, not just compute. One of [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/finance-ai-reclaiming-time-trust-with-openai-chatgpt/\">How accounting firms are using AI agents to reclaim time and trust</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",
    "published": "Tue, 21 Oct 2025 11:41:15 +0000",
    "source_name": "ai_news",
    "source_url": "https://artificialintelligence-news.com/feed/",
    "category": "ai_specific",
    "weight": 0.95,
    "ai_relevance_score": 1.0,
    "timestamp": "2025-10-22T09:02:06.200441",
    "ready_for_mytribal": true,
    "parsed_date": null,
    "days_old": 999,
    "priority_score": 0.95
  },
  {
    "title": "Over 800 public figures, including \"AI godfathers\" and Steve Wozniak, sign open letter to ban superintelligent AI",
    "link": "https://www.reddit.com/r/artificial/comments/1od6amj/over_800_public_figures_including_ai_godfathers/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1od6amj/over_800_public_figures_including_ai_godfathers/\"> <img alt=\"Over 800 public figures, including &quot;AI godfathers&quot; and Steve Wozniak, sign open letter to ban superintelligent AI\" src=\"https://external-preview.redd.it/07D2hn7hRL2hViTPFEnkPkwj9M_TULsESfe9hWfen2U.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f81b239ff3d379a68a4cea0e118bd53be05c978\" title=\"Over 800 public figures, including &quot;AI godfathers&quot; and Steve Wozniak, sign open letter to ban superintelligent AI\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MetaKnowing\"> /u/MetaKnowing </a> <br /> <span><a href=\"https://www.techspot.com/news/109960-over-800-public-figures-including-ai-godfathers-steve.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1od6amj/over_800_public_figures_including_ai_godfathers/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-22T12:01:07+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-22T09:02:08.873639",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-22T12:01:07+00:00",
    "days_old": 0,
    "priority_score": 0.8399999999999999
  },
  {
    "title": "The war between bosses and employees over AI is getting ugly",
    "link": "https://www.reddit.com/r/artificial/comments/1oci6nu/the_war_between_bosses_and_employees_over_ai_is/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1oci6nu/the_war_between_bosses_and_employees_over_ai_is/\"> <img alt=\"The war between bosses and employees over AI is getting ugly\" src=\"https://external-preview.redd.it/1OKwA2zvhk6_CX5_LSuRJKz9gPgsml0mLSwc8JfPApo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d51cd07d43f7ebbc3b6667ef579d587fed206ac4\" title=\"The war between bosses and employees over AI is getting ugly\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thisisinsider\"> /u/thisisinsider </a> <br /> <span><a href=\"https://www.businessinsider.com/inside-ai-divide-roiling-video-game-giant-electronic-arts-2025-10?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=BusinessInsider-post-artificial\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1oci6nu/the_war_between_bosses_and_employees_over_ai_is/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-21T16:50:14+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:08.873762",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-21T16:50:14+00:00",
    "days_old": 0,
    "priority_score": 0.63
  },
  {
    "title": "AI photo “enhancement” — or how to turn a selfie into a Pixar character",
    "link": "https://www.reddit.com/r/artificial/comments/1od14id/ai_photo_enhancement_or_how_to_turn_a_selfie_into/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1od14id/ai_photo_enhancement_or_how_to_turn_a_selfie_into/\"> <img alt=\"AI photo “enhancement” — or how to turn a selfie into a Pixar character\" src=\"https://external-preview.redd.it/MaUQvHLPKqfSy3Gore3A3InlxAZRML9uTJQoNLUdKbA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e68195ceaf959f8319a2aea9638f5864cc0477c\" title=\"AI photo “enhancement” — or how to turn a selfie into a Pixar character\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>AI photo tools keep promising “realistic improvements,” but somehow every face ends up glowing like it’s been polished by a 3D renderer. At this rate, your phone won’t just enhance your photo — it’ll enhance your entire identity. Are we improving pictures or deleting reality?</p> <h1>ai #photoediting #techhumor #upgradingai</h1> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thinkhamza\"> /u/thinkhamza </a> <br /> <span><a href=\"https://v.redd.it/0ofq1oac1mwf1\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1od14id/ai_photo_enhancement_or_how_to_turn_a_selfie_into/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-22T06:49:53+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:08.873874",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-22T06:49:53+00:00",
    "days_old": 0,
    "priority_score": 0.63
  },
  {
    "title": "Boris Johnson admits writing books using ChatGPT. Former prime minister said ChatGPT was “frankly fantastic” and AI would help society “because we’re all simple.”",
    "link": "https://www.reddit.com/r/artificial/comments/1ocnd6j/boris_johnson_admits_writing_books_using_chatgpt/",
    "summary": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1ocnd6j/boris_johnson_admits_writing_books_using_chatgpt/\"> <img alt=\"Boris Johnson admits writing books using ChatGPT. Former prime minister said ChatGPT was “frankly fantastic” and AI would help society “because we’re all simple.”\" src=\"https://external-preview.redd.it/ZuFi1MhosUo8fsH4zqF8Ajuiqfmc-j6nbW41hurujCs.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f473a91bb6346ed943cf0ed892325a31d620ac17\" title=\"Boris Johnson admits writing books using ChatGPT. Former prime minister said ChatGPT was “frankly fantastic” and AI would help society “because we’re all simple.”\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/esporx\"> /u/esporx </a> <br /> <span><a href=\"https://www.politico.eu/article/uk-boris-johnson-admits-writing-books-using-chatgpt-ai/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1ocnd6j/boris_johnson_admits_writing_books_using_chatgpt/\">[comments]</a></span> </td></tr></table>",
    "published": "2025-10-21T20:02:07+00:00",
    "source_name": "reddit_ai",
    "source_url": "https://www.reddit.com/r/artificial/hot.rss?limit=10",
    "category": "community_discussion",
    "weight": 0.7,
    "ai_relevance_score": 0.8,
    "timestamp": "2025-10-22T09:02:08.873962",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-21T20:02:07+00:00",
    "days_old": 0,
    "priority_score": 0.8399999999999999
  },
  {
    "title": "People Who Say They’re Experiencing AI Psychosis Beg the FTC for Help",
    "link": "https://www.wired.com/story/ftc-complaints-chatgpt-ai-psychosis/",
    "summary": "The Federal Trade Commission received 200 complaints mentioning ChatGPT between November 2022 and August 2025. Several attributed delusions, paranoia, and spiritual crises to the chatbot.",
    "published": "Wed, 22 Oct 2025 10:30:00 +0000",
    "source_name": "wired_ai",
    "source_url": "https://www.wired.com/feed/rss",
    "category": "tech_news",
    "weight": 0.85,
    "ai_relevance_score": 0.6,
    "timestamp": "2025-10-22T09:02:11.632153",
    "ready_for_mytribal": true,
    "parsed_date": "2025-10-22T10:30:00+00:00",
    "days_old": 0,
    "priority_score": 0.765
  }
]